{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with Tensorflow to build the same predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rebuild the same model as the PyTorch one and see if they perform any different\n",
    "2. Try more layers, different parameters, etc to learn a bit more\n",
    "3. Stick in a GridSearchCV pipeline to see how it does with getting a better version\n",
    "4. Save out the models as before to use in an application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial code setup comes from the [Tensorflow Tutorial](https://www.tensorflow.org/tutorials/keras/classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATEAAAD4CAYAAACE9dGgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAdDklEQVR4nO3dfXBc5ZXn8e+RLMm2LL9hYww4MRCTYJLFZB0gMJUhYSZAKjWGSUhBzTJODTVmd2EnTPEHhJ2tsDXFFpUNsKnJwI4JbJwqCOsJMDAMFV4cEkIyvBjj4LclNtjBxsavYBvbsqXus3/01dCydM+9UrfUfc3vQ3WpdU8/fR+3pMO9zz33eczdEREpqpZGd0BEpBZKYiJSaEpiIlJoSmIiUmhKYiJSaGNGc2ft1uFj6RzNXYp8pHRzgCN+2Gp5j4u/2Om795RyvfbV1w8/5e6X1LK/WtWUxMzsEuD7QCvwQ3e/PXr9WDo51y6qZZciEnjJl9X8Hrv3lHj5qY/lem3rzPXTat5hjYZ9OmlmrcDfA5cCc4GrzGxuvTomIo3hQDnnf1nMbJaZPWdm68xsjZl9K9l+q5m9Y2Yrk8dXqtp828w2mNkbZnZx1j5qORI7B9jg7m8lO34IWACsreE9RaTBHKfH851O5tAL3OjuK8ysC3jVzJ5JYne5+/eqX5wcCF0JnAmcCDxrZqe7p3eoloH9k4DNVd9vSbb1Y2aLzGy5mS3v4XANuxOR0VKvIzF33+buK5Ln+4F1DJInqiwAHnL3w+6+EdhA5YApVS1JbLDBwwH3MLn7Ynef7+7z2+ioYXciMhocp+T5HsC0voOU5LEo7X3NbDZwNvBSsul6M3vdzO43synJtlwHR9VqSWJbgFlV358MbK3h/USkSZTxXA9gV99BSvJYPNj7mdkE4GHgBnffB9wDnAbMA7YBd/S9dJDm4Q3etSSxV4A5ZnaKmbVTOY99vIb3E5Em4EAJz/XIw8zaqCSwB9z9EQB33+7uJXcvA/fy4SnjkA+Ohp3E3L0XuB54isp57lJ3XzPc9xOR5jGEI7GQmRlwH7DO3e+s2j6z6mWXA6uT548DV5pZh5mdAswBXo72UVOdmLs/CTxZy3uISHNxoKd+U3RdAFwNrDKzlcm2W6iUZM1LdrcJuBbA3deY2VIqVQ69wHXRlUkY5Yp9EWl+PoRTxcz3cn+Bwce5Ug9+3P024La8+1ASE5H+HEoFmitVSUxE+qlU7BeHkpiIHMUoDXoG2JyUxESkn8rAvpKYiBRUpU5MSUxECqysIzERKSodiYlIoTlGqUAz1yuJicgAOp0UkcJyjCPe2uhu5KYkJiL9VIpddTopIgWmgX1pHpbxy1jjbAWtx00N4+9dfHpqbOKDL9a076x/m41pS415z5Ha9l2rrJ9LpH4zTKS8vVFyHYmJSIGVdSQmIkVVGdgvTmooTk9FZFRoYF9ECq+kOjERKSpV7ItI4ZV1dVJEiqpyA7iSmDQJa41vH/He3jDeMm9uGF937YS4/aH0WNuBcHV6xhyKJ0lue3p5GK+pFiyrBi3jc8XiJFBL32xM8Gcb/zhzcYwe3XYkIkXljopdRaTITMWuIlJcjo7ERKTgNLAvIoXlmCZFFJHiqizZVpzUUJyeisgo0eK50kTCmiKy68Q2Xzw5jP/Z538Vxn+989TU2O87Tgjb+rgwzJg/+nwYP/3ud1JjvZvejt88Y86urM8tS+uUKenBUilsW9q3Lz1Yh6nGnI9Qxb6ZbQL2AyWg193n16NTItJYH7UjsS+6+646vI+INAF3++gciYnIsacysP/Rue3IgafNzIF/cPfFR7/AzBYBiwDGMr7G3YnIyCvWHPu19vQCd/8scClwnZl94egXuPtid5/v7vPb6KhxdyIy0ioD+5brkcXMZpnZc2a2zszWmNm3ku1TzewZM1uffJ1S1ebbZrbBzN4ws4uz9lFTEnP3rcnXHcCjQDwtgYgUQomWXI8ceoEb3f0M4DwqBztzgZuBZe4+B1iWfE8SuxI4E7gEuNvMwnPbYScxM+s0s66+58CXgdXDfT8RaQ59Ffv1OBJz923uviJ5vh9YB5wELACWJC9bAlyWPF8APOTuh919I7CBjIOjWsbEZgCPWmXepTHAg+7+sxreT0ZAubu7pvZHzv4gjH99Ujyn19iWntTYL1vi+cLe+fmsMF76d3Hffn9nV2qs/Nr5YdvjVse1WhNf2xbGd33hpDC+89+nF3TNyFiOc8qzb6bGbE99rtUNYaGQaWZW/UuweLCxcQAzmw2cDbwEzHD3bVBJdGZ2fPKyk4DqT2BLsi3VsP/F7v4WcNZw24tIc3KHnnLuJLYrT32omU0AHgZucPd9lj7p5GCBsIRXJRYi0k/ldLJ+VyfNrI1KAnvA3R9JNm83s5nJUdhMYEeyfQtQfQh+MrA1ev/iXEcVkVFTSu6fzHpkscoh133AOne/syr0OLAweb4QeKxq+5Vm1mFmpwBzgJejfehITET66SuxqJMLgKuBVWa2Mtl2C3A7sNTMrgHeBq4AcPc1ZrYUWEvlyuZ17h4OUCqJichR6nc66e4vMPg4F8BFKW1uA27Luw8lMREZQHPsy+iKlhfLmFLmg2+cF8b/fO4vwvibPdPD+Mnte1JjV5z4atiW/xDHf/DGH4bxA29NSo21dMafy7vnxUci7yyI/93eE0/VM2VF+p9ey8LtYdt9R9KnNyotq/2umMrVyY/OvZMicozR9NQiUng6nRSRwqrz1ckRpyQmIgNoUkQRKSx3o1dJTESKTKeTIlJYGhOToYvqvEbYeTeFt6XxxQlra3r/k4IJCA54e9j2/VJnGP/O3H8J4ztPT5+KJ2tx2B+uj6fq+SCoQQNo7Y1/puf9xWupsa9NfSVs+92HP5Maa/EDYdu8lMREpLBUJyYihac6MREpLHfozT8pYsMpiYnIADqdFJHC0piYiBSeK4mJSJFpYF+GJmPOr5G0/oPjw/juiRPC+Lu9k8P4ca3py6p1tRwK285u2xXGd5bS68AAWtvSl4Q74vF8Wf/9zH8O491ntIXxNouXfDt/bPraF1es/fOwbSdvhfFauWtMTEQKzSjp6qSIFJnGxESksHTvpIgUmzd0mHbIlMREZABdnRSRwnIN7ItI0el0Ugpjekd6HRfAWOsJ4+0Wr6+4tWdKamz9oU+GbX+3L65hu2TGmjDeE9SCtQbznEF2ndeJbe+F8W6P68iiT/WCGXEd2MowWh9FujqZecxoZveb2Q4zW121baqZPWNm65Ov6b+pIlIo7pUklufRDPKc+P4IuOSobTcDy9x9DrAs+V5EjhFlt1yPZpCZxNz9eeDotegXAEuS50uAy+rcLxFpIPd8j2Yw3DGxGe6+DcDdt5lZ6uCFmS0CFgGMZfwwdycio8UxygW6OjniPXX3xe4+393nt9Ex0rsTkTrwnI9mMNwktt3MZgIkX3fUr0si0lDH4MD+YB4HFibPFwKP1ac7ItIUCnQoljkmZmY/AS4EppnZFuA7wO3AUjO7BngbuGIkO3nMy1h30lrjua+8N71Wq3VKXP3yh5NXhfGdpYlh/P1SPM45ufVgamx/79iw7Z5D8Xt/qmNbGF9xcHZqbHp7XOcV9Rtg05FpYXxOx7th/LvbL0qNzRp79HW0/nov+kJqzF/617BtXs1ylJVHZhJz96tSQuk/BREpLAfK5fokMTO7H/gqsMPdP51suxX4S2Bn8rJb3P3JJPZt4BqgBPyVuz+VtY/iXIIQkdHhgFu+R7YfMbDOFOAud5+XPPoS2FzgSuDMpM3dZhafhqAkJiKDqFedWEqdaZoFwEPuftjdNwIbgHOyGimJichA+Qf2p5nZ8qrHopx7uN7MXk9ua+wbuD0J2Fz1mi3JtpBuABeRowypfGKXu88f4g7uAf6WShr8W+AO4C9g0EnMMo/3dCQmIgONYImFu29395K7l4F7+fCUcQswq+qlJwPpy0IldCTWDDIGF2xM/GOKSiw2X3NG2PZL4+OlyX7THR/NTx+zP4xH0+HM7Ngbtu2a0R3Gs8o7po5Jn2Zof2lc2HZ8y+EwnvXv/mx7vNzcXz/72dRY16d3h20ntgXHHvW4qOjgdbo6ORgzm9l32yJwOdA3Q87jwINmdidwIjAHeDnr/ZTERGQQdSuxGKzO9EIzm0flWG4TcC2Au68xs6XAWqAXuM7d44ndUBITkcHUqRo/pc70vuD1twG3DWUfSmIiMlCT3FKUh5KYiPTXV+xaEEpiIjJAs0x4mIeSmIgMNIJXJ+tNSUxEBjAdiclQWFt7GC93x/VSkWmrjoTxXaV4abHJLfGUNO0ZS5sdCerEzp+6MWy7M6OWa8WhU8J4V+uh1Nj0lrjOa1ZbXKu1qntWGH/ywCfC+DVffTY19pPFfxy2bf/Zb1Jj5vHPK5cmmissDyUxETlK7hkqmoKSmIgMpCMxESm0cqM7kJ+SmIj0pzoxESk6XZ0UkWIrUBLTfGIiUmjFOhILljazMXG9k7Vm5OuWOF7uDuaXKmfOFhLynriWqxbf/4cfhPHNvZPD+Ls9cTxrabNSMKXLi4cmhW3HtvSE8elj9oXxfeW4ziyyvxwvJxfNkwbZfb/puPWpsUf2/lHYdjTodFJEisvRbUciUnA6EhORItPppIgUm5KYiBSakpiIFJW5TidFpOh0dXJ4allfMavWyuOynYY6tOCcML75srgO7c/OTl+a793errDtawdnh/FJwZxcAJ0Z6zN2e3r93tYjU1JjkF1rFa0rCXB8UEdW8rgu8J2euG9ZsurntvQGa2L+STzX2eQfD6tLQ1KkI7HMin0zu9/MdpjZ6qptt5rZO2a2Mnl8ZWS7KSKjagRXAK+3PLcd/Qi4ZJDtd7n7vOTxZH27JSIN4x+Oi2U9mkFmEnP354E9o9AXEWkWx9iRWJrrzez15HQzdQDBzBaZ2XIzW95DPH4iIs3ByvkezWC4Sewe4DRgHrANuCPthe6+2N3nu/v8NjqGuTsRkcENK4m5+3Z3L7l7GbgXiC+viUixHOunk2Y2s+rby4HVaa8VkYIp2MB+Zp2Ymf0EuBCYZmZbgO8AF5rZPCq5eBNwbT06E9WB1WrMzBPCeM8pM8L4njPGp8YOnhAXBs77yrow/s0Z/yeM7yxNDONtlv65be45Lmx79vhNYfzne+eG8V1jJoTxqM7s/M70ObUA3i+nf+YAJ455L4zftOHrqbEZ4+NarB9+PL7g3uPxgNAbPfHQyd5y+nxkfzX3ubDto0wP43XRJAkqj8wk5u5XDbL5vhHoi4g0i2MpiYnIR4vRPFce81ASE5H+mmi8Kw8tFCIiA9Xp6mTKbYtTzewZM1uffJ1SFfu2mW0wszfM7OI8XVUSE5GB6ldi8SMG3rZ4M7DM3ecAy5LvMbO5wJXAmUmbu80sXpEFJTERGUS9SixSbltcACxJni8BLqva/pC7H3b3jcAGctSgNtWY2OFLPxfGj/+vb6XG5k3cEradO+6FMN5djpd8i6aFWXvopLDtwXJ7GF9/JC7/2Nsblxq0BqOwO47EU/HcsTFeHmzZOf87jP/N1sHmBvhQy7j03/Tdpbg842sT4iXZIP6ZXfux51Njp7bvCNs+cWBmGN+aMVXPjLa9YXx2287U2J92/S5sewyUWMxw920A7r7NzI5Ptp8EvFj1ui3JtlBTJTERaQI+pKuT08xsedX3i9198TD3PFjBZWY6VRITkYHyH4ntcvf5Q3z37WY2MzkKmwn0HRZvAWZVve5kYGvWm2lMTEQGGOHbjh4HFibPFwKPVW2/0sw6zOwUYA6QPm1xQkdiIjJQncbEUm5bvB1YambXAG8DVwC4+xozWwqsBXqB69w9npsdJTEROVodZ6hIuW0R4KKU198G3DaUfSiJiUg/RrEq9pXERGQAJbE0Fi/Ldu7/eCVsflHXmtTYQY+nPsmqA8uq+4lMGhMvz3W4J/6Yd/TEU+1kOb3j3dTY5RNXhm2f/8G5YfwPuv9LGH/zS/E0QssOpRdc7+yN/91XbvxSGF/x9qwwft7sjamxz3S9E7bNqs3rau0O49H0SAAHyum/ry92x/Vzo0JJTEQKTUlMRAqrYLNYKImJyEBKYiJSZJoUUUQKTaeTIlJcTbQcWx5KYiIykJLY4HqO72Tr1elznN066e/C9g/uOS81Nmvs0fOu9ffx9l1h/Kxxvw/jka6WuGbokxPjmqEnDpwcxn/x/qfC+My291Njvzp4Wtj2oVv/Zxj/5l/fGMY//+R/DOP7ZqfPMdDbGf+lTDxrdxj/m7P/JYy3W/ptd++X4jqwqR0Hwvjk1rg2MEtU19jVkr7MHUDrJz+RGrNN8bx5eahiX0QKz8rFyWJKYiLSn8bERKTodDopIsWmJCYiRaYjMREpNiUxESmsoa121HCjmsRaemD89vRP54l988L2p45LX6tvV0+8vuJTH3wmjJ887r0wPqk1vXbnE8F8XgAruyeH8Z/tPDOMnzguXn9xe8+k1Njuns6w7cFgXiuA++66M4zfsT1et/LyqStSY2e1x3Vg75fjdWzWZqzXub88NjXW7fH8cnsz6si6gt8HgB6P/7RaPf3vYHJLXIO27zPHpcZK22v/ky5anVjmakdmNsvMnjOzdWa2xsy+lWyfambPmNn65OvwZxUUkebinu/RBPIs2dYL3OjuZwDnAdeZ2VzgZmCZu88BliXfi8gxYISXbKurzCTm7tvcfUXyfD+wjsrS4guAJcnLlgCXjVQnRWQU+RAeTWBIJ9BmNhs4G3gJmOHu26CS6Mzs+JQ2i4BFAO2dOuMUKYIiDeznXgHczCYADwM3uHs80lzF3Re7+3x3nz+mIx5kFpHmYOV8j2aQK4mZWRuVBPaAuz+SbN5uZjOT+Exgx8h0UURGlVOogf3M00kzM+A+YJ27V19vfxxYSGVJ8oXAY1nv1XqkTNfmw6nxslvY/ue70qekmTF2f9h2XtfmMP7Gwfhy/apDJ6bGVoz5WNh2XGtPGJ/UHk/l0zkm/TMDmNaW/m8/pSP+f0s0XQ3AK93xv+0/Tf9FGH+7N30I4Z8PnB62XXsw/TMHmJKxVN6qfentD/a2h20Pl+I/je7euGRnUkf8M/3c1PSpn95gZth251nB9Ea/Dpvm1iyD9nnkGRO7ALgaWGVmfYsY3kIleS01s2uAt4ErRqaLIjLqjqUk5u4vUKl/G8xF9e2OiDRa0YpddduRiPTnrkkRRaTgipPDlMREZCCdTopIcTmg00kRKbTi5LBRTmIfHKLll6+lhv/x6QvC5v9twT+mxn6ZsazZE+/GdT37jsRT0kwfn76E18SgTgtgalu8/NekjHqnsRYv+fZeb/qdEIdb4ilnSqkXnivePZw+zQ/Ar8tzwnhPuTU1djiIQXZ93Z4j08L4ieP2psb296ZP0wOwaf/UML5r74Qw3j0+/tN6oZS+lN4lJ6wJ247bkf4za4l/VXLT6aSIFFo9r06a2SZgP1ACet19vplNBf4vMBvYBHzD3eNJ/VLkvndSRD4iRmYWiy+6+zx3n598X7epvJTERKSfSrGr53rUoG5TeSmJichA5ZwPmGZmy6seiwZ5NweeNrNXq+L9pvICBp3KKw+NiYnIAEM4ytpVdYqY5gJ335rMOfiMmf2/2nrXn47ERKS/Oo+JufvW5OsO4FHgHOo4lZeSmIgcpXLvZJ5HFjPrNLOuvufAl4HVfDiVF+ScyitNU51OnnrTv4bxu1//enrb//xG2PbSE1aH8RX74nmz3g7qhn4bzDUG0NYST4E5vu1IGB+bUS/V3po+J1hLxv8uyxl1Yp2tcd+y5jqb2pFeI9fVGs+51VLj1KGtwb/95b2zw7Yzxse1f5+YuCuM93p8fPD5SW+mxu7feH7Ydsbf/SY1tsnjmsTc6jfh4Qzg0cq0hIwBHnT3n5nZK9RpKq+mSmIi0gTquHiuu78FnDXI9t3UaSovJTERGahJpp7OQ0lMRAYqTg5TEhORgazcJEsZ5aAkJiL9OX2FrIWgJCYi/Rg131I0qpTERGQgJbFASzCHVDleA3HSAy+mxnY/EO/2p1+7OIyfe8srYfyrs3+bGvtU+/awbVvGsfnYjOvZnS1xLVd38AuXVc38wqFZYbyU8Q4/f++MMP5+z7jU2PaDE8O2bUH9Wx7ROqaHeuN51vYeiucba22J/8i7fxHPdbZxbfr8d5OejH8XR4WSmIgUlsbERKTodHVSRArMdTopIgXmKImJSMEV52xSSUxEBlKdmIgU27GUxMxsFvBj4AQqB5mL3f37ZnYr8JfAzuSlt7j7k5l7zKgFGymdD78Uxlc/HLdfzSmpMfvcn4RtD52QXisF0LE7npNr/8fj9hPfTJ9DquVwvBBh+bfrwni2D2pouy+MxrOo1aY9Iz695j38ruZ3aBh3KBXnfDLPkVgvcKO7r0hmaHzVzJ5JYne5+/dGrnsi0hDH0pFYshJJ36ok+81sHXDSSHdMRBqoQElsSHPsm9ls4Gyg79zsejN73czuN7MpKW0W9S3n1EN82iQiTcCBsud7NIHcSczMJgAPAze4+z7gHuA0YB6VI7U7Bmvn7ovdfb67z2+jow5dFpGR5eDlfI8mkOvqpJm1UUlgD7j7IwDuvr0qfi/wxIj0UERGl1Oogf3MIzGrLFNyH7DO3e+s2j6z6mWXU1mGSUSOBe75Hk0gz5HYBcDVwCozW5lsuwW4yszmUcnbm4BrR6SHBeCvrArj8aQu2Samr9CVqTj/P5Wm0iQJKo88VydfgEEXJ8yuCRORAmqeo6w8VLEvIv05oKl4RKTQdCQmIsV17N12JCIfJQ7eJDVgeSiJichATVKNn4eSmIgMpDExESksd12dFJGC05GYiBSX46XGTF46HEpiItJf31Q8BaEkJiIDFajEYkiTIorIsc8BL3uuRx5mdomZvWFmG8zs5nr3V0lMRPrz+k2KaGatwN8DlwJzqcx+M7ee3dXppIgMUMeB/XOADe7+FoCZPQQsANbWawejmsT2896uZ/2nv6/aNA3YNZp9GIJm7Vuz9gvUt+GqZ98+Xusb7Oe9p571n07L+fKxZra86vvF7r646vuTgM1V328Bzq21j9VGNYm5e7/l/MxsubvPH80+5NWsfWvWfoH6NlzN1jd3v6SObzfYXIR1vfSpMTERGUlbgFlV358MbK3nDpTERGQkvQLMMbNTzKwduBJ4vJ47aPTA/uLslzRMs/atWfsF6ttwNXPfauLuvWZ2PfAU0Arc7+5r6rkP8wLdIyUicjSdTopIoSmJiUihNSSJjfRtCLUws01mtsrMVh5V/9KIvtxvZjvMbHXVtqlm9oyZrU++Tmmivt1qZu8kn91KM/tKg/o2y8yeM7N1ZrbGzL6VbG/oZxf0qyk+t6Ia9TGx5DaE3wF/TOXy6yvAVe5etwreWpjZJmC+uze8MNLMvgB8APzY3T+dbPsusMfdb0/+BzDF3W9qkr7dCnzg7t8b7f4c1beZwEx3X2FmXcCrwGXAN2ngZxf06xs0wedWVI04Evu32xDc/QjQdxuCHMXdnwf2HLV5AbAkeb6Eyh/BqEvpW1Nw923uviJ5vh9YR6VyvKGfXdAvqUEjkthgtyE00w/SgafN7FUzW9Tozgxihrtvg8ofBXB8g/tztOvN7PXkdLMhp7rVzGw2cDbwEk302R3VL2iyz61IGpHERvw2hBpd4O6fpXLX/XXJaZPkcw9wGjAP2Abc0cjOmNkE4GHgBnff18i+VBukX031uRVNI5LYiN+GUAt335p83QE8SuX0t5lsT8ZW+sZYdjS4P//G3be7e8krixbeSwM/OzNro5IoHnD3R5LNDf/sButXM31uRdSIJDbityEMl5l1JgOumFkn8GVgddxq1D0OLEyeLwQea2Bf+ulLEInLadBnZ2YG3Aesc/c7q0IN/ezS+tUsn1tRNaRiP7mE/L/48DaE20a9E4Mws1OpHH1B5ZasBxvZNzP7CXAhlalatgPfAf4JWAp8DHgbuMLdR32APaVvF1I5JXJgE3Bt3xjUKPftD4BfAauAvpn7bqEy/tSwzy7o11U0wedWVLrtSEQKTRX7IlJoSmIiUmhKYiJSaEpiIlJoSmIiUmhKYiJSaEpiIlJo/x/RvxJh5ClQ5gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data - in this case it's scaling each of the pixel values from 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example model is below - I'll be expanding on it later in the notebook (turning it into a CNN, experimenting with different layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.4972 - accuracy: 0.8270\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3763 - accuracy: 0.8646\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.3379 - accuracy: 0.8774\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.3134 - accuracy: 0.8856\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2953 - accuracy: 0.8917\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2794 - accuracy: 0.8972\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2671 - accuracy: 0.9001\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2571 - accuracy: 0.9047\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2475 - accuracy: 0.9066\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.2391 - accuracy: 0.9098\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x217868dea58>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.3686 - accuracy: 0.8762\n",
      "\n",
      "Test accuracy: 0.8762000203132629\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "# for overfitting try a little L@ regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([model, \n",
    "                                         tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = probability_model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(i, predictions_array, true_label, img):\n",
    "  predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "  plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "  if predicted_label == true_label:\n",
    "    color = 'blue'\n",
    "  else:\n",
    "    color = 'red'\n",
    "\n",
    "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "  predictions_array, true_label = predictions_array, true_label[i]\n",
    "  plt.grid(False)\n",
    "  plt.xticks(range(10))\n",
    "  plt.yticks([])\n",
    "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "  plt.ylim([0, 1])\n",
    "  predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "  thisplot[predicted_label].set_color('red')\n",
    "  thisplot[true_label].set_color('blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i, predictions[i], test_labels, test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i, predictions[i],  test_labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the first X test images, their predicted labels, and the true labels.\n",
    "# Color correct predictions in blue and incorrect predictions in red.\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions[i], test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(1, 28, 28)\n",
      "[[2.3624027e-06 1.1932366e-13 9.9958640e-01 5.9092257e-14 4.0738180e-04\n",
      "  3.4496109e-13 3.8244493e-06 5.4478804e-23 1.1616879e-11 8.6670207e-15]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grab an image from the test dataset.\n",
    "img = test_images[1]\n",
    "\n",
    "print(img.shape)\n",
    "# Add the image to a batch where it's the only member.\n",
    "img = (np.expand_dims(img,0))\n",
    "\n",
    "print(img.shape)\n",
    "predictions_single = probability_model.predict(img)\n",
    "\n",
    "print(predictions_single)\n",
    "plot_value_array(1, predictions_single[0], test_labels)\n",
    "_ = plt.xticks(range(10), class_names, rotation=45)\n",
    "np.argmax(predictions_single[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
    "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 12)          1812      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 12)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                610       \n",
      "=================================================================\n",
      "Total params: 32,998\n",
      "Trainable params: 32,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Mimic the format of the structure of the PyTorch model\n",
    "batch_size = 100\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(filters=6, kernel_size=5, activation='relu', input_shape=(28, 28,1), data_format='channels_last'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(keras.layers.Conv2D(filters=12, kernel_size=5, activation='relu'))# input_shape=(6,12, 12)))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(120, activation='relu'))#, input_shape=(12*4*4,)))\n",
    "model.add(keras.layers.Dense(60, activation='relu'))\n",
    "model.add(keras.layers.Dense(10))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out because I had to restart the kernel and didn't want to rerun this\n",
    "# model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(train_images,\n",
    "#          train_labels,\n",
    "#          batch_size=500,\n",
    "#          epochs=100,\n",
    "#          shuffle=True,\n",
    "#          validation_data=(test_images, test_labels))\n",
    "# # It's overfit, but I'll include overfitting on the below one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out because I had to restart the kernel and didn't want to rerun this\n",
    "# model.save('saved_models/tf-mimic-3.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the testing is done, lets recreate the model in a way we can use it with scikit-learn and GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper function for the Keras CNN that will allow easier experimentation with GridSearchCV\n",
    "def build_classifier(kernel_size=5, # kernel size of the Conv layers\n",
    "                     max_pool_size=2, #Size of the MaxPool layers\n",
    "                     pool_strides=2, # Stride of the MaxPool layers\n",
    "                     filters=(6,12), #Filter size\n",
    "                     dense_size=(120,60), #Size of the dense layers\n",
    "                     learning_rate=0.001, # Learnig rate for Adam\n",
    "                     dropout_d=0.0 # dropout rate in the dense layers\n",
    "                    ): \n",
    "    \n",
    "    # Starting model\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Convolutional / pooling layers\n",
    "    model.add(keras.layers.Conv2D(filters=filters[0], kernel_size=kernel_size, activation='relu', input_shape=(28, 28,1), data_format='channels_last'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=max_pool_size, strides=pool_strides))\n",
    "    model.add(keras.layers.Conv2D(filters=[1], kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=max_pool_size, strides=pool_strides))\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(dropout_d))\n",
    "    model.add(keras.layers.Dense(dense_size[0], activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_d))\n",
    "    model.add(keras.layers.Dense(dense_size[1], activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_d))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(10))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "             metrics='accuracy'\n",
    "                 )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_callback = keras.callbacks.EarlyStopping(\n",
    "  monitor='val_loss',\n",
    "  min_delta=0.0001,\n",
    "  patience=3,\n",
    "  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"kernel_size\": [3,5],\n",
    "    \"max_pool_size\": [2,3],\n",
    "    \"pool_strides\": [1,2],\n",
    "    \"filters\": [(6,12),(4,8)],\n",
    "    \"dense_size\": [(120,60), (100,50)],\n",
    "    \"learning_rate\": [0.005,0.001, 0.0005],\n",
    "    \"dropout_d\": [0.0, 0.2],\n",
    "    \"batch_size\": [100, 250, 500],\n",
    "    \"epochs\": [100]\n",
    "#     \"shuffle\": [True],\n",
    "#     \"callbacks\": [[earlystop_callback]],\n",
    "#     \"validation_data\": [(test_images, test_labels)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this with the default args, and those that should be overwritten with the param_grid will be (I think)\n",
    "keras_model = KerasClassifier(\n",
    "    build_fn=build_classifier,\n",
    "    \n",
    "    # The param_grid args to the build function\n",
    "    kernel_size=5, # kernel size of the Conv layers\n",
    "    max_pool_size=2, #Size of the MaxPool layers\n",
    "    pool_strides=2, # Stride of the MaxPool layers\n",
    "    filters=(6,12), #Filter size\n",
    "    dense_size=(120,60), #Size of the dense layers\n",
    "    learning_rate=0.001, # Learnig rate for Adam\n",
    "    dropout_d=0.0, # dropout rate in the dense layers\n",
    "    \n",
    "    # The param_grid args to the fit function\n",
    "    epochs = 50, # This should really be kept under control by the early stopping\n",
    "    batch_size=100,\n",
    "    \n",
    "    # Other single args to the fit function\n",
    "    shuffle=True, # Shuffle the traiing data\n",
    "    callbacks=[earlystop_callback], # Utilize early stopping as defined above\n",
    "    validation_data=(test_images, test_labels) # Use test data as validation data\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator = keras_model,\n",
    "    param_grid = param_grid,\n",
    "    # Scoring will just be the default from the Keras model, aka Categorical cross entropy + accuracy\n",
    "    cv = 3,\n",
    "    n_jobs = -1,\n",
    "    verbose = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.fit(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.2'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue Summary:\n",
    "There's a [known](https://github.com/keras-team/keras/issues/13586) [bug](https://github.com/scikit-learn/scikit-learn/issues/15722) That for me for whatever reason isn't fixed by reverting the sklearn version. Instead, what I'll be doing is running a knockoff of a gridsearch, without CV (but with teh validation set) of teh parameter sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_callback = keras.callbacks.EarlyStopping(\n",
    "  monitor='val_loss',\n",
    "  min_delta=0.0001,\n",
    "  patience=3,\n",
    "  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The below cell is definitely duplicating the train/test images and labels, so just pass those in separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "param_grid = {\n",
    "    \"kernel_size\": [3,5],\n",
    "    \"max_pool_size\": [2],\n",
    "    \"pool_strides\": [1,2],\n",
    "    \"filters\": [(6,12)],\n",
    "    \"dense_size\": [(120,60)],\n",
    "    \"learning_rate\": [0.001, 0.0005],\n",
    "    \"dropout_d\": [0.0, 0.2],\n",
    "    \"batch_size\": [100, 250, 500],\n",
    "    \"epochs\": [100],\n",
    "    \"shuffle\": [True]\n",
    "}\n",
    "single_params = {\n",
    "    # Since none of these change, just pass them in independently\n",
    "    \"callbacks\": [[earlystop_callback]],\n",
    "    \"test_images\": test_images,\n",
    "    \"test_labels\": test_labels,\n",
    "    \"train_images\": train_images,\n",
    "    \"train_labels\": train_labels\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "for idx, d in enumerate(permutations_dicts):\n",
    "    name = \"model-\"+str(idx)\n",
    "    d[\"model_name\"] = name\n",
    "\n",
    "len(permutations_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the way pool works, have to define this in a separate function then import it\n",
    "def build_and_fit_model(\n",
    "                        # Positional, previously created objects\n",
    "                        model_name=\"nonegiven\",\n",
    "                        train_images=None,\n",
    "                        train_labels=None,\n",
    "                        test_images=None,\n",
    "                        test_labels=None,\n",
    "                        callbacks=None,\n",
    "                        kernel_size=5, # kernel size of the Conv layers\n",
    "                        max_pool_size=2, #Size of the MaxPool layers\n",
    "                        pool_strides=2, # Stride of the MaxPool layers\n",
    "                        filters=(6,12), #Filter size\n",
    "                        dense_size=(120,60), #Size of the dense layers\n",
    "                        learning_rate=0.001, # Learnig rate for Adam\n",
    "                        dropout_d=0.0, # dropout rate in the dense layers\n",
    "                        batch_size=500,\n",
    "                        shuffle=True,\n",
    "                        epochs=100\n",
    "                    ): \n",
    "    \n",
    "    # Starting model\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Convolutional / pooling layers\n",
    "    model.add(keras.layers.Conv2D(filters=filters[0], kernel_size=kernel_size, activation='relu', input_shape=(28, 28,1), data_format='channels_last'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=max_pool_size, strides=pool_strides))\n",
    "    model.add(keras.layers.Conv2D(filters=filters[1], kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=max_pool_size, strides=pool_strides))\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(dropout_d))\n",
    "    model.add(keras.layers.Dense(dense_size[0], activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_d))\n",
    "    model.add(keras.layers.Dense(dense_size[1], activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_d))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(10))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "             metrics='accuracy'\n",
    "                 )\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(train_images,\n",
    "         train_labels,\n",
    "         batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         shuffle=shuffle,\n",
    "         callbacks=callbacks,\n",
    "         validation_data=(test_images, test_labels))\n",
    "    \n",
    "    #save the model and the parameters of the model to files\n",
    "    path=\"./saved_models/fake-gridsearch-saves/\" + model_name\n",
    "    model.save(f'{path}.h5')\n",
    "    evals = model.evaluate(test_images, test_labels)\n",
    "    \n",
    "    with open('./saved_models/fake-gridsearch-saves/results-tf.txt', 'a') as f:\n",
    "        f.write(f\"{model_name}: {evals}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from build_and_fit_model import build_and_fit_model\n",
    "# build_and_fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = permutations_dicts[0:2]\n",
    "full_params = [{**x, **single_params} for x in permutations_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_and_fit_model(**test_params, **single_params)\n",
    "# Works great on a single one! now to get it done with multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "600/600 [==============================] - 32s 53ms/step - loss: 0.4705 - accuracy: 0.8303 - val_loss: 0.3685 - val_accuracy: 0.8678\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 32s 53ms/step - loss: 0.3180 - accuracy: 0.8843 - val_loss: 0.3335 - val_accuracy: 0.8789\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 32s 53ms/step - loss: 0.2715 - accuracy: 0.9004 - val_loss: 0.2899 - val_accuracy: 0.8922\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2427 - accuracy: 0.9100 - val_loss: 0.2833 - val_accuracy: 0.8974\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 32s 53ms/step - loss: 0.2151 - accuracy: 0.9197 - val_loss: 0.2917 - val_accuracy: 0.8940\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1946 - accuracy: 0.9276 - val_loss: 0.2667 - val_accuracy: 0.9019\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1749 - accuracy: 0.9339 - val_loss: 0.2706 - val_accuracy: 0.9070\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1582 - accuracy: 0.9404 - val_loss: 0.2785 - val_accuracy: 0.9050\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1423 - accuracy: 0.9467 - val_loss: 0.2786 - val_accuracy: 0.9062\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2667 - accuracy: 0.9019\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 29s 120ms/step - loss: 0.5183 - accuracy: 0.8157 - val_loss: 0.3943 - val_accuracy: 0.8617\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 28s 119ms/step - loss: 0.3222 - accuracy: 0.8849 - val_loss: 0.3459 - val_accuracy: 0.8777\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 29s 120ms/step - loss: 0.2800 - accuracy: 0.8977 - val_loss: 0.3044 - val_accuracy: 0.8878\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 29s 122ms/step - loss: 0.2506 - accuracy: 0.9085 - val_loss: 0.2965 - val_accuracy: 0.8918\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 28s 117ms/step - loss: 0.2310 - accuracy: 0.9136 - val_loss: 0.2690 - val_accuracy: 0.9018\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.2123 - accuracy: 0.9212 - val_loss: 0.2753 - val_accuracy: 0.9028\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.1972 - accuracy: 0.9274 - val_loss: 0.2764 - val_accuracy: 0.9049\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.1851 - accuracy: 0.9305 - val_loss: 0.2670 - val_accuracy: 0.9047\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.1648 - accuracy: 0.9392 - val_loss: 0.2741 - val_accuracy: 0.9011\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.1537 - accuracy: 0.9424 - val_loss: 0.2855 - val_accuracy: 0.9041\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.1407 - accuracy: 0.9478 - val_loss: 0.2814 - val_accuracy: 0.9025\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2670 - accuracy: 0.9047\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 219ms/step - loss: 0.6167 - accuracy: 0.7810 - val_loss: 0.4342 - val_accuracy: 0.8465\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.3818 - accuracy: 0.8648 - val_loss: 0.3729 - val_accuracy: 0.8685\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.3286 - accuracy: 0.8835 - val_loss: 0.3349 - val_accuracy: 0.8815\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.2970 - accuracy: 0.8941 - val_loss: 0.3181 - val_accuracy: 0.8868\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2725 - accuracy: 0.9017 - val_loss: 0.3034 - val_accuracy: 0.8914\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.2551 - accuracy: 0.9076 - val_loss: 0.3025 - val_accuracy: 0.8934\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.2378 - accuracy: 0.9140 - val_loss: 0.2793 - val_accuracy: 0.9001\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.2286 - accuracy: 0.9157 - val_loss: 0.2833 - val_accuracy: 0.8944\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2137 - accuracy: 0.9217 - val_loss: 0.2743 - val_accuracy: 0.9003\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2005 - accuracy: 0.9262 - val_loss: 0.2636 - val_accuracy: 0.9051\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.1944 - accuracy: 0.9288 - val_loss: 0.2616 - val_accuracy: 0.9095\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.1802 - accuracy: 0.9344 - val_loss: 0.2585 - val_accuracy: 0.9055\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.1772 - accuracy: 0.9343 - val_loss: 0.2641 - val_accuracy: 0.9059\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.1647 - accuracy: 0.9401 - val_loss: 0.2628 - val_accuracy: 0.9091\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.1587 - accuracy: 0.9416 - val_loss: 0.2570 - val_accuracy: 0.9109\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.1468 - accuracy: 0.9460 - val_loss: 0.2643 - val_accuracy: 0.9083\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.1401 - accuracy: 0.9486 - val_loss: 0.2587 - val_accuracy: 0.9111\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.1383 - accuracy: 0.9484 - val_loss: 0.2664 - val_accuracy: 0.9104\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2570 - accuracy: 0.9109\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.5969 - accuracy: 0.7853 - val_loss: 0.3864 - val_accuracy: 0.8604\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.3955 - accuracy: 0.8585 - val_loss: 0.3494 - val_accuracy: 0.8739\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.3459 - accuracy: 0.8766 - val_loss: 0.3118 - val_accuracy: 0.8848\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.3176 - accuracy: 0.8855 - val_loss: 0.3096 - val_accuracy: 0.8852\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 32s 53ms/step - loss: 0.2922 - accuracy: 0.8929 - val_loss: 0.2919 - val_accuracy: 0.8922\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2711 - accuracy: 0.9005 - val_loss: 0.2959 - val_accuracy: 0.8911\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2579 - accuracy: 0.9056 - val_loss: 0.2693 - val_accuracy: 0.9009\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2433 - accuracy: 0.9108 - val_loss: 0.2583 - val_accuracy: 0.9094\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2311 - accuracy: 0.9144 - val_loss: 0.2586 - val_accuracy: 0.9062\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2225 - accuracy: 0.9182 - val_loss: 0.2634 - val_accuracy: 0.9049\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2127 - accuracy: 0.9206 - val_loss: 0.2529 - val_accuracy: 0.9095\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2027 - accuracy: 0.9254 - val_loss: 0.2591 - val_accuracy: 0.9104\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.1963 - accuracy: 0.9275 - val_loss: 0.2604 - val_accuracy: 0.9081\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.1873 - accuracy: 0.9309 - val_loss: 0.2581 - val_accuracy: 0.9116\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2529 - accuracy: 0.9095\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.6974 - accuracy: 0.7431 - val_loss: 0.4266 - val_accuracy: 0.8475\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.4364 - accuracy: 0.8433 - val_loss: 0.3723 - val_accuracy: 0.8681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3827 - accuracy: 0.8631 - val_loss: 0.3341 - val_accuracy: 0.8800\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3480 - accuracy: 0.8749 - val_loss: 0.3202 - val_accuracy: 0.8831\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3243 - accuracy: 0.8827 - val_loss: 0.2975 - val_accuracy: 0.8931\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.3073 - accuracy: 0.8882 - val_loss: 0.2944 - val_accuracy: 0.8944\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2944 - accuracy: 0.8925 - val_loss: 0.2884 - val_accuracy: 0.8946\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2769 - accuracy: 0.8992 - val_loss: 0.2928 - val_accuracy: 0.8944\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2670 - accuracy: 0.9014 - val_loss: 0.2666 - val_accuracy: 0.9017\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2533 - accuracy: 0.9059 - val_loss: 0.2725 - val_accuracy: 0.9003\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2448 - accuracy: 0.9110 - val_loss: 0.2644 - val_accuracy: 0.9043\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2377 - accuracy: 0.9131 - val_loss: 0.2616 - val_accuracy: 0.9068\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2261 - accuracy: 0.9162 - val_loss: 0.2576 - val_accuracy: 0.9098\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.2198 - accuracy: 0.9185 - val_loss: 0.2652 - val_accuracy: 0.9050\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2170 - accuracy: 0.9204 - val_loss: 0.2578 - val_accuracy: 0.9060\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2058 - accuracy: 0.9229 - val_loss: 0.2546 - val_accuracy: 0.9102\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1994 - accuracy: 0.9258 - val_loss: 0.2502 - val_accuracy: 0.9132\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1946 - accuracy: 0.9280 - val_loss: 0.2479 - val_accuracy: 0.9127\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1862 - accuracy: 0.9302 - val_loss: 0.2649 - val_accuracy: 0.9100\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1805 - accuracy: 0.9321 - val_loss: 0.2670 - val_accuracy: 0.9102\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1771 - accuracy: 0.9334 - val_loss: 0.2641 - val_accuracy: 0.9106\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2479 - accuracy: 0.9127\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.7100 - accuracy: 0.7543 - val_loss: 0.4339 - val_accuracy: 0.8435\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.4286 - accuracy: 0.8478 - val_loss: 0.3663 - val_accuracy: 0.8671\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.3653 - accuracy: 0.8705 - val_loss: 0.3219 - val_accuracy: 0.8838\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.3369 - accuracy: 0.8773 - val_loss: 0.3034 - val_accuracy: 0.8898\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3098 - accuracy: 0.8878 - val_loss: 0.2976 - val_accuracy: 0.8915\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2891 - accuracy: 0.8940 - val_loss: 0.2829 - val_accuracy: 0.8970\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2770 - accuracy: 0.8989 - val_loss: 0.2738 - val_accuracy: 0.9001\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2649 - accuracy: 0.9036 - val_loss: 0.2939 - val_accuracy: 0.8900\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2520 - accuracy: 0.9077 - val_loss: 0.2696 - val_accuracy: 0.9012\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2405 - accuracy: 0.9104 - val_loss: 0.2569 - val_accuracy: 0.9057\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2296 - accuracy: 0.9161 - val_loss: 0.2602 - val_accuracy: 0.9047\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2286 - accuracy: 0.9149 - val_loss: 0.2648 - val_accuracy: 0.9028\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2154 - accuracy: 0.9204 - val_loss: 0.2587 - val_accuracy: 0.9070\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2569 - accuracy: 0.9057\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.5134 - accuracy: 0.8219 - val_loss: 0.4031 - val_accuracy: 0.8567\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.3547 - accuracy: 0.8739 - val_loss: 0.3589 - val_accuracy: 0.8695\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.3079 - accuracy: 0.8897 - val_loss: 0.3078 - val_accuracy: 0.8877\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2774 - accuracy: 0.8993 - val_loss: 0.2962 - val_accuracy: 0.8900\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2537 - accuracy: 0.9083 - val_loss: 0.2931 - val_accuracy: 0.8918\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2346 - accuracy: 0.9147 - val_loss: 0.2865 - val_accuracy: 0.8933\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2164 - accuracy: 0.9206 - val_loss: 0.2635 - val_accuracy: 0.9064\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2016 - accuracy: 0.9262 - val_loss: 0.2602 - val_accuracy: 0.9057\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.1841 - accuracy: 0.9323 - val_loss: 0.2515 - val_accuracy: 0.9101\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.1714 - accuracy: 0.9366 - val_loss: 0.2484 - val_accuracy: 0.9115\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.1567 - accuracy: 0.9422 - val_loss: 0.2488 - val_accuracy: 0.9157\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.1438 - accuracy: 0.9478 - val_loss: 0.2487 - val_accuracy: 0.9153\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.1326 - accuracy: 0.9518 - val_loss: 0.2659 - val_accuracy: 0.9102\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2484 - accuracy: 0.9115\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.6115 - accuracy: 0.7831 - val_loss: 0.4781 - val_accuracy: 0.8202\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 27s 115ms/step - loss: 0.3975 - accuracy: 0.8576 - val_loss: 0.4027 - val_accuracy: 0.8558\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3484 - accuracy: 0.8771 - val_loss: 0.3538 - val_accuracy: 0.8744\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3167 - accuracy: 0.8868 - val_loss: 0.3468 - val_accuracy: 0.8736\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2955 - accuracy: 0.8938 - val_loss: 0.3239 - val_accuracy: 0.8810\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2769 - accuracy: 0.9008 - val_loss: 0.3130 - val_accuracy: 0.8881\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2610 - accuracy: 0.9045 - val_loss: 0.2964 - val_accuracy: 0.8913\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.2501 - accuracy: 0.9097 - val_loss: 0.3010 - val_accuracy: 0.8902\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 29s 120ms/step - loss: 0.2351 - accuracy: 0.9139 - val_loss: 0.2855 - val_accuracy: 0.8972\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 28s 116ms/step - loss: 0.2226 - accuracy: 0.9194 - val_loss: 0.2792 - val_accuracy: 0.8975\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2128 - accuracy: 0.9219 - val_loss: 0.2721 - val_accuracy: 0.9011\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2047 - accuracy: 0.9253 - val_loss: 0.2680 - val_accuracy: 0.9041\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1957 - accuracy: 0.9279 - val_loss: 0.2626 - val_accuracy: 0.9048\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.1888 - accuracy: 0.9302 - val_loss: 0.2614 - val_accuracy: 0.9070\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1808 - accuracy: 0.9334 - val_loss: 0.2637 - val_accuracy: 0.9044\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1706 - accuracy: 0.9374 - val_loss: 0.2617 - val_accuracy: 0.9069\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.1650 - accuracy: 0.9392 - val_loss: 0.2769 - val_accuracy: 0.9018\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2614 - accuracy: 0.9070\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.6828 - accuracy: 0.7568 - val_loss: 0.4949 - val_accuracy: 0.8241\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.4499 - accuracy: 0.8376 - val_loss: 0.4473 - val_accuracy: 0.8415\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.4012 - accuracy: 0.8565 - val_loss: 0.4116 - val_accuracy: 0.8534\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3745 - accuracy: 0.8665 - val_loss: 0.3999 - val_accuracy: 0.8573\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3451 - accuracy: 0.8769 - val_loss: 0.3586 - val_accuracy: 0.8735\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3260 - accuracy: 0.8833 - val_loss: 0.3642 - val_accuracy: 0.8711\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3142 - accuracy: 0.8878 - val_loss: 0.3588 - val_accuracy: 0.8670\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2982 - accuracy: 0.8932 - val_loss: 0.3294 - val_accuracy: 0.8837\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2850 - accuracy: 0.8986 - val_loss: 0.3169 - val_accuracy: 0.8874\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2744 - accuracy: 0.9018 - val_loss: 0.3152 - val_accuracy: 0.8884\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2618 - accuracy: 0.9068 - val_loss: 0.3063 - val_accuracy: 0.8896\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2549 - accuracy: 0.9078 - val_loss: 0.2990 - val_accuracy: 0.8901\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2463 - accuracy: 0.9123 - val_loss: 0.2984 - val_accuracy: 0.8916\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2351 - accuracy: 0.9149 - val_loss: 0.2867 - val_accuracy: 0.8971\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2276 - accuracy: 0.9186 - val_loss: 0.2917 - val_accuracy: 0.8975\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2207 - accuracy: 0.9210 - val_loss: 0.2931 - val_accuracy: 0.8963\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2134 - accuracy: 0.9222 - val_loss: 0.2919 - val_accuracy: 0.8989\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2867 - accuracy: 0.8971\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.6525 - accuracy: 0.7703 - val_loss: 0.4222 - val_accuracy: 0.8504\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.4228 - accuracy: 0.8504 - val_loss: 0.3484 - val_accuracy: 0.8737\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.3688 - accuracy: 0.8686 - val_loss: 0.3281 - val_accuracy: 0.8803\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.3326 - accuracy: 0.8802 - val_loss: 0.3088 - val_accuracy: 0.8885\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.3100 - accuracy: 0.8871 - val_loss: 0.3014 - val_accuracy: 0.8895\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2894 - accuracy: 0.8948 - val_loss: 0.2814 - val_accuracy: 0.8946\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2723 - accuracy: 0.9017 - val_loss: 0.2681 - val_accuracy: 0.9012\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2578 - accuracy: 0.9056 - val_loss: 0.2615 - val_accuracy: 0.9021\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2467 - accuracy: 0.9108 - val_loss: 0.2709 - val_accuracy: 0.9016\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2334 - accuracy: 0.9145 - val_loss: 0.2582 - val_accuracy: 0.9049\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.2262 - accuracy: 0.9166 - val_loss: 0.2658 - val_accuracy: 0.9011\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2159 - accuracy: 0.9209 - val_loss: 0.2549 - val_accuracy: 0.9059\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2075 - accuracy: 0.9246 - val_loss: 0.2608 - val_accuracy: 0.9075\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.1984 - accuracy: 0.9275 - val_loss: 0.2489 - val_accuracy: 0.9089\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.1917 - accuracy: 0.9294 - val_loss: 0.2482 - val_accuracy: 0.9117\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.1819 - accuracy: 0.9323 - val_loss: 0.2458 - val_accuracy: 0.9115\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.1786 - accuracy: 0.9337 - val_loss: 0.2527 - val_accuracy: 0.9112\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.1701 - accuracy: 0.9374 - val_loss: 0.2468 - val_accuracy: 0.9152\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.1661 - accuracy: 0.9381 - val_loss: 0.2520 - val_accuracy: 0.9134\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2458 - accuracy: 0.9115\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.7355 - accuracy: 0.7433 - val_loss: 0.4395 - val_accuracy: 0.8481\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.4531 - accuracy: 0.8407 - val_loss: 0.3771 - val_accuracy: 0.8627\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3875 - accuracy: 0.8625 - val_loss: 0.3380 - val_accuracy: 0.8775\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3538 - accuracy: 0.8731 - val_loss: 0.3144 - val_accuracy: 0.8876\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3244 - accuracy: 0.8854 - val_loss: 0.3059 - val_accuracy: 0.8864\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3072 - accuracy: 0.8903 - val_loss: 0.2931 - val_accuracy: 0.8938\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.2913 - accuracy: 0.8948 - val_loss: 0.2816 - val_accuracy: 0.8973\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.2759 - accuracy: 0.9007 - val_loss: 0.2791 - val_accuracy: 0.9002\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.2635 - accuracy: 0.9039 - val_loss: 0.2661 - val_accuracy: 0.9024\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2508 - accuracy: 0.9089 - val_loss: 0.2634 - val_accuracy: 0.9062\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2420 - accuracy: 0.9108 - val_loss: 0.2640 - val_accuracy: 0.9025\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2323 - accuracy: 0.9158 - val_loss: 0.2585 - val_accuracy: 0.9056\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2246 - accuracy: 0.9175 - val_loss: 0.2561 - val_accuracy: 0.9076\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2185 - accuracy: 0.9189 - val_loss: 0.2512 - val_accuracy: 0.9081\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 27s 115ms/step - loss: 0.2101 - accuracy: 0.9231 - val_loss: 0.2457 - val_accuracy: 0.9112\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2023 - accuracy: 0.9255 - val_loss: 0.2555 - val_accuracy: 0.9122\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.1979 - accuracy: 0.9267 - val_loss: 0.2430 - val_accuracy: 0.9117\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.1907 - accuracy: 0.9305 - val_loss: 0.2439 - val_accuracy: 0.9140\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 27s 115ms/step - loss: 0.1825 - accuracy: 0.9319 - val_loss: 0.2441 - val_accuracy: 0.9144\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.1774 - accuracy: 0.9351 - val_loss: 0.2516 - val_accuracy: 0.9108\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2430 - accuracy: 0.9117\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.8862 - accuracy: 0.6891 - val_loss: 0.5080 - val_accuracy: 0.8178\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.5133 - accuracy: 0.8186 - val_loss: 0.4126 - val_accuracy: 0.8521\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.4354 - accuracy: 0.8459 - val_loss: 0.3799 - val_accuracy: 0.8659\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3977 - accuracy: 0.8599 - val_loss: 0.3523 - val_accuracy: 0.8750\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3697 - accuracy: 0.8701 - val_loss: 0.3348 - val_accuracy: 0.8801\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3467 - accuracy: 0.8769 - val_loss: 0.3235 - val_accuracy: 0.8834\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3334 - accuracy: 0.8814 - val_loss: 0.3132 - val_accuracy: 0.8856\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3161 - accuracy: 0.8864 - val_loss: 0.3037 - val_accuracy: 0.8901\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.3047 - accuracy: 0.8904 - val_loss: 0.2925 - val_accuracy: 0.8929\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2918 - accuracy: 0.8951 - val_loss: 0.2896 - val_accuracy: 0.8946\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2838 - accuracy: 0.8983 - val_loss: 0.2843 - val_accuracy: 0.8968\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2744 - accuracy: 0.9008 - val_loss: 0.2774 - val_accuracy: 0.8977\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2692 - accuracy: 0.9015 - val_loss: 0.2749 - val_accuracy: 0.8992\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2592 - accuracy: 0.9061 - val_loss: 0.2706 - val_accuracy: 0.9004\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2528 - accuracy: 0.9064 - val_loss: 0.2677 - val_accuracy: 0.9029\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2476 - accuracy: 0.9090 - val_loss: 0.2652 - val_accuracy: 0.9032\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2401 - accuracy: 0.9127 - val_loss: 0.2616 - val_accuracy: 0.9042\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2358 - accuracy: 0.9143 - val_loss: 0.2664 - val_accuracy: 0.9018\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2330 - accuracy: 0.9154 - val_loss: 0.2494 - val_accuracy: 0.9065\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2246 - accuracy: 0.9178 - val_loss: 0.2536 - val_accuracy: 0.9040\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 26s 219ms/step - loss: 0.2194 - accuracy: 0.9203 - val_loss: 0.2503 - val_accuracy: 0.9090\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2147 - accuracy: 0.9218 - val_loss: 0.2484 - val_accuracy: 0.9084\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2090 - accuracy: 0.9236 - val_loss: 0.2527 - val_accuracy: 0.9091\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2029 - accuracy: 0.9257 - val_loss: 0.2513 - val_accuracy: 0.9097\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2026 - accuracy: 0.9271 - val_loss: 0.2451 - val_accuracy: 0.9093\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.1970 - accuracy: 0.9286 - val_loss: 0.2515 - val_accuracy: 0.9109\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.1923 - accuracy: 0.9287 - val_loss: 0.2454 - val_accuracy: 0.9108\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.1880 - accuracy: 0.9307 - val_loss: 0.2466 - val_accuracy: 0.9131\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2451 - accuracy: 0.9093\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.6117 - accuracy: 0.7794 - val_loss: 0.4486 - val_accuracy: 0.8410\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3962 - accuracy: 0.8582 - val_loss: 0.3828 - val_accuracy: 0.8655\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3521 - accuracy: 0.8724 - val_loss: 0.3483 - val_accuracy: 0.8776\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3239 - accuracy: 0.8820 - val_loss: 0.3402 - val_accuracy: 0.8804\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3022 - accuracy: 0.8901 - val_loss: 0.3278 - val_accuracy: 0.8821\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2876 - accuracy: 0.8944 - val_loss: 0.3267 - val_accuracy: 0.8805\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2736 - accuracy: 0.8996 - val_loss: 0.3051 - val_accuracy: 0.8911\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2633 - accuracy: 0.9030 - val_loss: 0.2976 - val_accuracy: 0.8942\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2534 - accuracy: 0.9057 - val_loss: 0.2962 - val_accuracy: 0.8929\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2439 - accuracy: 0.9099 - val_loss: 0.2995 - val_accuracy: 0.8944\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2346 - accuracy: 0.9124 - val_loss: 0.2969 - val_accuracy: 0.8973\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2258 - accuracy: 0.9162 - val_loss: 0.2986 - val_accuracy: 0.8931\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2962 - accuracy: 0.8929\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.7637 - accuracy: 0.7232 - val_loss: 0.5589 - val_accuracy: 0.7911\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4895 - accuracy: 0.8201 - val_loss: 0.4707 - val_accuracy: 0.8273\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4373 - accuracy: 0.8397 - val_loss: 0.4356 - val_accuracy: 0.8371\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4045 - accuracy: 0.8524 - val_loss: 0.4536 - val_accuracy: 0.8311\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3820 - accuracy: 0.8613 - val_loss: 0.3931 - val_accuracy: 0.8598\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 9s 37ms/step - loss: 0.3601 - accuracy: 0.8695 - val_loss: 0.3840 - val_accuracy: 0.8614\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3452 - accuracy: 0.8740 - val_loss: 0.3740 - val_accuracy: 0.8653\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3356 - accuracy: 0.8779 - val_loss: 0.3636 - val_accuracy: 0.8699\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3225 - accuracy: 0.8814 - val_loss: 0.3514 - val_accuracy: 0.8721\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3143 - accuracy: 0.8845 - val_loss: 0.3375 - val_accuracy: 0.8793\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3029 - accuracy: 0.8892 - val_loss: 0.3338 - val_accuracy: 0.8769\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2960 - accuracy: 0.8914 - val_loss: 0.3278 - val_accuracy: 0.8821\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2889 - accuracy: 0.8936 - val_loss: 0.3345 - val_accuracy: 0.8789\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2826 - accuracy: 0.8947 - val_loss: 0.3276 - val_accuracy: 0.8806\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2745 - accuracy: 0.8993 - val_loss: 0.3391 - val_accuracy: 0.8765\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2695 - accuracy: 0.8996 - val_loss: 0.3189 - val_accuracy: 0.8838\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2605 - accuracy: 0.9026 - val_loss: 0.3223 - val_accuracy: 0.8852\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2574 - accuracy: 0.9046 - val_loss: 0.3134 - val_accuracy: 0.8862\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2511 - accuracy: 0.9065 - val_loss: 0.3168 - val_accuracy: 0.8829\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 9s 35ms/step - loss: 0.2449 - accuracy: 0.9079 - val_loss: 0.3065 - val_accuracy: 0.8914\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2406 - accuracy: 0.9096 - val_loss: 0.3142 - val_accuracy: 0.8867\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2330 - accuracy: 0.9127 - val_loss: 0.2996 - val_accuracy: 0.8897\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2302 - accuracy: 0.9128 - val_loss: 0.3100 - val_accuracy: 0.8842\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2237 - accuracy: 0.9158 - val_loss: 0.3047 - val_accuracy: 0.8885\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2192 - accuracy: 0.9174 - val_loss: 0.3018 - val_accuracy: 0.8904\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2996 - accuracy: 0.8897\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.9984 - accuracy: 0.6434 - val_loss: 0.6314 - val_accuracy: 0.7501\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.5669 - accuracy: 0.7835 - val_loss: 0.5435 - val_accuracy: 0.7954\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.5026 - accuracy: 0.8132 - val_loss: 0.4889 - val_accuracy: 0.8240\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4540 - accuracy: 0.8368 - val_loss: 0.4632 - val_accuracy: 0.8321\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4229 - accuracy: 0.8492 - val_loss: 0.4547 - val_accuracy: 0.8344\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4027 - accuracy: 0.8551 - val_loss: 0.4187 - val_accuracy: 0.8510\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3849 - accuracy: 0.8636 - val_loss: 0.4067 - val_accuracy: 0.8552\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3760 - accuracy: 0.8643 - val_loss: 0.3949 - val_accuracy: 0.8590\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 9s 71ms/step - loss: 0.3563 - accuracy: 0.8730 - val_loss: 0.3763 - val_accuracy: 0.8659\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3496 - accuracy: 0.8747 - val_loss: 0.3775 - val_accuracy: 0.8651\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.3406 - accuracy: 0.8770 - val_loss: 0.3618 - val_accuracy: 0.8708\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3290 - accuracy: 0.8810 - val_loss: 0.3503 - val_accuracy: 0.8733\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3219 - accuracy: 0.8836 - val_loss: 0.3529 - val_accuracy: 0.8723\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3146 - accuracy: 0.8861 - val_loss: 0.3388 - val_accuracy: 0.8793\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3110 - accuracy: 0.8875 - val_loss: 0.3505 - val_accuracy: 0.8733\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3011 - accuracy: 0.8895 - val_loss: 0.3317 - val_accuracy: 0.8828\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.2963 - accuracy: 0.8924 - val_loss: 0.3330 - val_accuracy: 0.8801\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2929 - accuracy: 0.8933 - val_loss: 0.3274 - val_accuracy: 0.8839\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2862 - accuracy: 0.8968 - val_loss: 0.3257 - val_accuracy: 0.8838\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2858 - accuracy: 0.8954 - val_loss: 0.3194 - val_accuracy: 0.8871\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.2796 - accuracy: 0.8981 - val_loss: 0.3212 - val_accuracy: 0.8878\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 9s 71ms/step - loss: 0.2742 - accuracy: 0.9001 - val_loss: 0.3295 - val_accuracy: 0.8796\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2690 - accuracy: 0.9027 - val_loss: 0.3350 - val_accuracy: 0.8820\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3194 - accuracy: 0.8871\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.8202 - accuracy: 0.6921 - val_loss: 0.5233 - val_accuracy: 0.7947\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.5453 - accuracy: 0.7958 - val_loss: 0.4516 - val_accuracy: 0.8320\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.4843 - accuracy: 0.8195 - val_loss: 0.4188 - val_accuracy: 0.8415\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.4496 - accuracy: 0.8336 - val_loss: 0.3859 - val_accuracy: 0.8551\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.4209 - accuracy: 0.8457 - val_loss: 0.3627 - val_accuracy: 0.8661\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.4010 - accuracy: 0.8516 - val_loss: 0.3524 - val_accuracy: 0.8683\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3860 - accuracy: 0.8561 - val_loss: 0.3402 - val_accuracy: 0.8716\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3710 - accuracy: 0.8634 - val_loss: 0.3336 - val_accuracy: 0.8779\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3622 - accuracy: 0.8656 - val_loss: 0.3311 - val_accuracy: 0.8750\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3545 - accuracy: 0.8688 - val_loss: 0.3155 - val_accuracy: 0.8818\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3437 - accuracy: 0.8738 - val_loss: 0.3164 - val_accuracy: 0.8798\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3363 - accuracy: 0.8752 - val_loss: 0.3022 - val_accuracy: 0.8873\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3285 - accuracy: 0.8785 - val_loss: 0.3084 - val_accuracy: 0.8859\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3252 - accuracy: 0.8785 - val_loss: 0.2982 - val_accuracy: 0.8914\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3183 - accuracy: 0.8810 - val_loss: 0.3004 - val_accuracy: 0.8889\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3133 - accuracy: 0.8833 - val_loss: 0.2910 - val_accuracy: 0.8925\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3087 - accuracy: 0.8849 - val_loss: 0.2950 - val_accuracy: 0.8912\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3061 - accuracy: 0.8857 - val_loss: 0.2899 - val_accuracy: 0.8937\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3001 - accuracy: 0.8875 - val_loss: 0.2864 - val_accuracy: 0.8957\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2967 - accuracy: 0.8893 - val_loss: 0.2892 - val_accuracy: 0.8912\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2946 - accuracy: 0.8897 - val_loss: 0.2882 - val_accuracy: 0.8914\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2903 - accuracy: 0.8918 - val_loss: 0.2843 - val_accuracy: 0.8943\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2845 - accuracy: 0.8944 - val_loss: 0.2746 - val_accuracy: 0.8983\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2865 - accuracy: 0.8931 - val_loss: 0.2821 - val_accuracy: 0.8958\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2834 - accuracy: 0.8937 - val_loss: 0.2764 - val_accuracy: 0.8966\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2804 - accuracy: 0.8957 - val_loss: 0.2770 - val_accuracy: 0.8972\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2746 - accuracy: 0.8983\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.9579 - accuracy: 0.6464 - val_loss: 0.5684 - val_accuracy: 0.7827\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.6049 - accuracy: 0.7746 - val_loss: 0.5019 - val_accuracy: 0.8167\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.5442 - accuracy: 0.7991 - val_loss: 0.4648 - val_accuracy: 0.8274\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.5017 - accuracy: 0.8147 - val_loss: 0.4298 - val_accuracy: 0.8426\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4773 - accuracy: 0.8245 - val_loss: 0.4117 - val_accuracy: 0.8441\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4515 - accuracy: 0.8329 - val_loss: 0.3933 - val_accuracy: 0.8527\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4368 - accuracy: 0.8389 - val_loss: 0.3790 - val_accuracy: 0.8598\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4208 - accuracy: 0.8459 - val_loss: 0.3759 - val_accuracy: 0.8618\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4074 - accuracy: 0.8502 - val_loss: 0.3699 - val_accuracy: 0.8642\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3940 - accuracy: 0.8563 - val_loss: 0.3484 - val_accuracy: 0.8706\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3832 - accuracy: 0.8587 - val_loss: 0.3548 - val_accuracy: 0.8681\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3765 - accuracy: 0.8606 - val_loss: 0.3429 - val_accuracy: 0.8749\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3709 - accuracy: 0.8629 - val_loss: 0.3331 - val_accuracy: 0.8770\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3654 - accuracy: 0.8652 - val_loss: 0.3320 - val_accuracy: 0.8793\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3542 - accuracy: 0.8697 - val_loss: 0.3222 - val_accuracy: 0.8820\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3511 - accuracy: 0.8693 - val_loss: 0.3335 - val_accuracy: 0.8767\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3430 - accuracy: 0.8722 - val_loss: 0.3145 - val_accuracy: 0.8838\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3409 - accuracy: 0.8745 - val_loss: 0.3201 - val_accuracy: 0.8835\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3364 - accuracy: 0.8747 - val_loss: 0.3145 - val_accuracy: 0.8835\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3323 - accuracy: 0.8768 - val_loss: 0.3083 - val_accuracy: 0.8888\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3226 - accuracy: 0.8799 - val_loss: 0.3052 - val_accuracy: 0.8870\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3263 - accuracy: 0.8787 - val_loss: 0.3036 - val_accuracy: 0.8901\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3193 - accuracy: 0.8813 - val_loss: 0.3020 - val_accuracy: 0.8897\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3166 - accuracy: 0.8823 - val_loss: 0.2973 - val_accuracy: 0.8899\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3148 - accuracy: 0.8836 - val_loss: 0.3013 - val_accuracy: 0.8877\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3097 - accuracy: 0.8850 - val_loss: 0.2975 - val_accuracy: 0.8906\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3061 - accuracy: 0.8848 - val_loss: 0.2939 - val_accuracy: 0.8935\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3034 - accuracy: 0.8860 - val_loss: 0.2928 - val_accuracy: 0.8935\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3016 - accuracy: 0.8886 - val_loss: 0.2921 - val_accuracy: 0.8925\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2982 - accuracy: 0.8883 - val_loss: 0.2977 - val_accuracy: 0.8902\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2950 - accuracy: 0.8900 - val_loss: 0.2893 - val_accuracy: 0.8943\n",
      "Epoch 32/100\n",
      "240/240 [==============================] - 9s 35ms/step - loss: 0.2932 - accuracy: 0.8907 - val_loss: 0.2923 - val_accuracy: 0.8933\n",
      "Epoch 33/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2919 - accuracy: 0.8914 - val_loss: 0.2927 - val_accuracy: 0.8917\n",
      "Epoch 34/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2892 - accuracy: 0.8923 - val_loss: 0.2886 - val_accuracy: 0.8933\n",
      "Epoch 35/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2876 - accuracy: 0.8931 - val_loss: 0.2839 - val_accuracy: 0.8964\n",
      "Epoch 36/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2849 - accuracy: 0.8921 - val_loss: 0.2835 - val_accuracy: 0.8963\n",
      "Epoch 37/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2822 - accuracy: 0.8940 - val_loss: 0.2821 - val_accuracy: 0.8957\n",
      "Epoch 38/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2819 - accuracy: 0.8940 - val_loss: 0.2841 - val_accuracy: 0.8955\n",
      "Epoch 39/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2810 - accuracy: 0.8950 - val_loss: 0.2855 - val_accuracy: 0.8969\n",
      "Epoch 40/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2780 - accuracy: 0.8952 - val_loss: 0.2808 - val_accuracy: 0.8952\n",
      "Epoch 41/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2770 - accuracy: 0.8963 - val_loss: 0.2769 - val_accuracy: 0.8999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2770 - accuracy: 0.8970 - val_loss: 0.2812 - val_accuracy: 0.8953\n",
      "Epoch 43/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2708 - accuracy: 0.8984 - val_loss: 0.2801 - val_accuracy: 0.8984\n",
      "Epoch 44/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2706 - accuracy: 0.8979 - val_loss: 0.2747 - val_accuracy: 0.8988\n",
      "Epoch 45/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2702 - accuracy: 0.8989 - val_loss: 0.2762 - val_accuracy: 0.8999\n",
      "Epoch 46/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2694 - accuracy: 0.8984 - val_loss: 0.2738 - val_accuracy: 0.9015\n",
      "Epoch 47/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2683 - accuracy: 0.8998 - val_loss: 0.2797 - val_accuracy: 0.8987\n",
      "Epoch 48/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2665 - accuracy: 0.8990 - val_loss: 0.2763 - val_accuracy: 0.8993\n",
      "Epoch 49/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2644 - accuracy: 0.9000 - val_loss: 0.2743 - val_accuracy: 0.9019\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2738 - accuracy: 0.9015\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 1.1451 - accuracy: 0.5895 - val_loss: 0.6218 - val_accuracy: 0.7635\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.6385 - accuracy: 0.7599 - val_loss: 0.5256 - val_accuracy: 0.8000\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.5605 - accuracy: 0.7900 - val_loss: 0.4799 - val_accuracy: 0.8181\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.5231 - accuracy: 0.8067 - val_loss: 0.4500 - val_accuracy: 0.8311\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4892 - accuracy: 0.8185 - val_loss: 0.4190 - val_accuracy: 0.8425\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4638 - accuracy: 0.8280 - val_loss: 0.4049 - val_accuracy: 0.8482\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4466 - accuracy: 0.8351 - val_loss: 0.3894 - val_accuracy: 0.8550\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4259 - accuracy: 0.8421 - val_loss: 0.3728 - val_accuracy: 0.8604\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4127 - accuracy: 0.8490 - val_loss: 0.3648 - val_accuracy: 0.8615\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3992 - accuracy: 0.8530 - val_loss: 0.3545 - val_accuracy: 0.8714\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3890 - accuracy: 0.8568 - val_loss: 0.3444 - val_accuracy: 0.8721\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3835 - accuracy: 0.8595 - val_loss: 0.3350 - val_accuracy: 0.8775\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3684 - accuracy: 0.8640 - val_loss: 0.3321 - val_accuracy: 0.8790\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3608 - accuracy: 0.8666 - val_loss: 0.3236 - val_accuracy: 0.8801\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3558 - accuracy: 0.8694 - val_loss: 0.3243 - val_accuracy: 0.8815\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3522 - accuracy: 0.8706 - val_loss: 0.3175 - val_accuracy: 0.8826\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3454 - accuracy: 0.8740 - val_loss: 0.3135 - val_accuracy: 0.8858\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3426 - accuracy: 0.8734 - val_loss: 0.3088 - val_accuracy: 0.8876\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 9s 73ms/step - loss: 0.3346 - accuracy: 0.8759 - val_loss: 0.3061 - val_accuracy: 0.8878\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 9s 73ms/step - loss: 0.3333 - accuracy: 0.8771 - val_loss: 0.3038 - val_accuracy: 0.8895\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 9s 73ms/step - loss: 0.3263 - accuracy: 0.8793 - val_loss: 0.3019 - val_accuracy: 0.8904\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 9s 76ms/step - loss: 0.3265 - accuracy: 0.8779 - val_loss: 0.2976 - val_accuracy: 0.8899\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 9s 71ms/step - loss: 0.3232 - accuracy: 0.8801 - val_loss: 0.2947 - val_accuracy: 0.8941\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3178 - accuracy: 0.8827 - val_loss: 0.2940 - val_accuracy: 0.8928\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3161 - accuracy: 0.8829 - val_loss: 0.2943 - val_accuracy: 0.8907\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3129 - accuracy: 0.8842 - val_loss: 0.2903 - val_accuracy: 0.8940\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3106 - accuracy: 0.8861 - val_loss: 0.2885 - val_accuracy: 0.8936\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3056 - accuracy: 0.8862 - val_loss: 0.2845 - val_accuracy: 0.8968\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3030 - accuracy: 0.8888 - val_loss: 0.2850 - val_accuracy: 0.8951\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3016 - accuracy: 0.8891 - val_loss: 0.2861 - val_accuracy: 0.8951\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2999 - accuracy: 0.8904 - val_loss: 0.2828 - val_accuracy: 0.8954\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.2956 - accuracy: 0.8909 - val_loss: 0.2823 - val_accuracy: 0.8973\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2946 - accuracy: 0.8899 - val_loss: 0.2785 - val_accuracy: 0.8975\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.2900 - accuracy: 0.8924 - val_loss: 0.2799 - val_accuracy: 0.8961\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.2904 - accuracy: 0.8914 - val_loss: 0.2770 - val_accuracy: 0.8974\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2878 - accuracy: 0.8926 - val_loss: 0.2775 - val_accuracy: 0.8963\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2838 - accuracy: 0.8954 - val_loss: 0.2763 - val_accuracy: 0.8978\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 8s 68ms/step - loss: 0.2852 - accuracy: 0.8939 - val_loss: 0.2724 - val_accuracy: 0.8993\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2827 - accuracy: 0.8944 - val_loss: 0.2733 - val_accuracy: 0.9004\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2771 - accuracy: 0.8963 - val_loss: 0.2737 - val_accuracy: 0.8984\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.2761 - accuracy: 0.8978 - val_loss: 0.2747 - val_accuracy: 0.8973\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2724 - accuracy: 0.8993\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.7129 - accuracy: 0.7566 - val_loss: 0.4994 - val_accuracy: 0.8220\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.4415 - accuracy: 0.8416 - val_loss: 0.4342 - val_accuracy: 0.8455\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3955 - accuracy: 0.8600 - val_loss: 0.3973 - val_accuracy: 0.8594\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3688 - accuracy: 0.8684 - val_loss: 0.4029 - val_accuracy: 0.8537\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3501 - accuracy: 0.8747 - val_loss: 0.3732 - val_accuracy: 0.8700\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3351 - accuracy: 0.8800 - val_loss: 0.3580 - val_accuracy: 0.8719\n",
      "Epoch 7/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3234 - accuracy: 0.8832 - val_loss: 0.3620 - val_accuracy: 0.8673\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3100 - accuracy: 0.8882 - val_loss: 0.3467 - val_accuracy: 0.8761\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3010 - accuracy: 0.8914 - val_loss: 0.3399 - val_accuracy: 0.8784\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2914 - accuracy: 0.8935 - val_loss: 0.3239 - val_accuracy: 0.8848\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2829 - accuracy: 0.8976 - val_loss: 0.3256 - val_accuracy: 0.8829\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2751 - accuracy: 0.9003 - val_loss: 0.3143 - val_accuracy: 0.8889\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2669 - accuracy: 0.9029 - val_loss: 0.3131 - val_accuracy: 0.8865\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2599 - accuracy: 0.9061 - val_loss: 0.3224 - val_accuracy: 0.8841\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2554 - accuracy: 0.9080 - val_loss: 0.3003 - val_accuracy: 0.8944\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2484 - accuracy: 0.9094 - val_loss: 0.3021 - val_accuracy: 0.8948\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2424 - accuracy: 0.9109 - val_loss: 0.3196 - val_accuracy: 0.8884\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2350 - accuracy: 0.9135 - val_loss: 0.3096 - val_accuracy: 0.8878\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3003 - accuracy: 0.8944\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.9354 - accuracy: 0.6951 - val_loss: 0.6239 - val_accuracy: 0.7681\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.5599 - accuracy: 0.7934 - val_loss: 0.5336 - val_accuracy: 0.8028\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4975 - accuracy: 0.8170 - val_loss: 0.4896 - val_accuracy: 0.8207\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4633 - accuracy: 0.8326 - val_loss: 0.4975 - val_accuracy: 0.8189\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4335 - accuracy: 0.8438 - val_loss: 0.4493 - val_accuracy: 0.8348\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4155 - accuracy: 0.8509 - val_loss: 0.4302 - val_accuracy: 0.8462\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3961 - accuracy: 0.8584 - val_loss: 0.4131 - val_accuracy: 0.8502\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3811 - accuracy: 0.8644 - val_loss: 0.4042 - val_accuracy: 0.8523\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3684 - accuracy: 0.8694 - val_loss: 0.3840 - val_accuracy: 0.8615\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3593 - accuracy: 0.8718 - val_loss: 0.3767 - val_accuracy: 0.8614\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3461 - accuracy: 0.8762 - val_loss: 0.3647 - val_accuracy: 0.8686\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3398 - accuracy: 0.8770 - val_loss: 0.3691 - val_accuracy: 0.8672\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3325 - accuracy: 0.8796 - val_loss: 0.3609 - val_accuracy: 0.8694\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3238 - accuracy: 0.8837 - val_loss: 0.3524 - val_accuracy: 0.8709\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3172 - accuracy: 0.8856 - val_loss: 0.3577 - val_accuracy: 0.8687\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3155 - accuracy: 0.8859 - val_loss: 0.3411 - val_accuracy: 0.8762\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3057 - accuracy: 0.8897 - val_loss: 0.3365 - val_accuracy: 0.8777\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3024 - accuracy: 0.8907 - val_loss: 0.3424 - val_accuracy: 0.8730\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2961 - accuracy: 0.8923 - val_loss: 0.3376 - val_accuracy: 0.8725\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2907 - accuracy: 0.8943 - val_loss: 0.3408 - val_accuracy: 0.8748\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3365 - accuracy: 0.8777\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 1.1719 - accuracy: 0.6383 - val_loss: 0.7181 - val_accuracy: 0.7417\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.6307 - accuracy: 0.7689 - val_loss: 0.5972 - val_accuracy: 0.7785\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.5330 - accuracy: 0.8048 - val_loss: 0.5144 - val_accuracy: 0.8128\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4849 - accuracy: 0.8227 - val_loss: 0.4858 - val_accuracy: 0.8228\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.4606 - accuracy: 0.8323 - val_loss: 0.4636 - val_accuracy: 0.8286\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4454 - accuracy: 0.8391 - val_loss: 0.4557 - val_accuracy: 0.8351\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4280 - accuracy: 0.8450 - val_loss: 0.4647 - val_accuracy: 0.8301\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4164 - accuracy: 0.8490 - val_loss: 0.4279 - val_accuracy: 0.8435\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4049 - accuracy: 0.8525 - val_loss: 0.4396 - val_accuracy: 0.8393\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3978 - accuracy: 0.8555 - val_loss: 0.4184 - val_accuracy: 0.8487\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 9s 71ms/step - loss: 0.3853 - accuracy: 0.8601 - val_loss: 0.4123 - val_accuracy: 0.8495\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 9s 72ms/step - loss: 0.3772 - accuracy: 0.8644 - val_loss: 0.4072 - val_accuracy: 0.8508\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3701 - accuracy: 0.8655 - val_loss: 0.3936 - val_accuracy: 0.8598\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3614 - accuracy: 0.8693 - val_loss: 0.3948 - val_accuracy: 0.8572\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3561 - accuracy: 0.8713 - val_loss: 0.3849 - val_accuracy: 0.8603\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3508 - accuracy: 0.8728 - val_loss: 0.3802 - val_accuracy: 0.8632\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3471 - accuracy: 0.8742 - val_loss: 0.3766 - val_accuracy: 0.8613\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3395 - accuracy: 0.8776 - val_loss: 0.3687 - val_accuracy: 0.8656\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.3323 - accuracy: 0.8792 - val_loss: 0.3665 - val_accuracy: 0.8656\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3331 - accuracy: 0.8801 - val_loss: 0.3715 - val_accuracy: 0.8640\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3256 - accuracy: 0.8817 - val_loss: 0.3621 - val_accuracy: 0.8679\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3214 - accuracy: 0.8838 - val_loss: 0.3610 - val_accuracy: 0.8676\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3186 - accuracy: 0.8845 - val_loss: 0.3608 - val_accuracy: 0.8685\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3142 - accuracy: 0.8863 - val_loss: 0.3520 - val_accuracy: 0.8711\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3121 - accuracy: 0.8877 - val_loss: 0.3509 - val_accuracy: 0.8727\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3078 - accuracy: 0.8895 - val_loss: 0.3478 - val_accuracy: 0.8731\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3055 - accuracy: 0.8891 - val_loss: 0.3474 - val_accuracy: 0.8737\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3008 - accuracy: 0.8907 - val_loss: 0.3465 - val_accuracy: 0.8721\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2976 - accuracy: 0.8921 - val_loss: 0.3426 - val_accuracy: 0.8737\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.2934 - accuracy: 0.8932 - val_loss: 0.3394 - val_accuracy: 0.8768\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2916 - accuracy: 0.8954 - val_loss: 0.3358 - val_accuracy: 0.8766\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2899 - accuracy: 0.8938 - val_loss: 0.3348 - val_accuracy: 0.8785\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2848 - accuracy: 0.8965 - val_loss: 0.3341 - val_accuracy: 0.8768\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 9s 72ms/step - loss: 0.2841 - accuracy: 0.8966 - val_loss: 0.3328 - val_accuracy: 0.8787\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 9s 71ms/step - loss: 0.2819 - accuracy: 0.8970 - val_loss: 0.3331 - val_accuracy: 0.8797\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.2782 - accuracy: 0.8999 - val_loss: 0.3314 - val_accuracy: 0.8809\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 9s 71ms/step - loss: 0.2794 - accuracy: 0.8983 - val_loss: 0.3268 - val_accuracy: 0.8819\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2721 - accuracy: 0.9015 - val_loss: 0.3276 - val_accuracy: 0.8809\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 9s 74ms/step - loss: 0.2702 - accuracy: 0.9019 - val_loss: 0.3306 - val_accuracy: 0.8794\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 9s 74ms/step - loss: 0.2703 - accuracy: 0.9015 - val_loss: 0.3315 - val_accuracy: 0.8813\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3268 - accuracy: 0.8819\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.9570 - accuracy: 0.6506 - val_loss: 0.5640 - val_accuracy: 0.7847\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.6001 - accuracy: 0.7745 - val_loss: 0.4890 - val_accuracy: 0.8199\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.5293 - accuracy: 0.8037 - val_loss: 0.4493 - val_accuracy: 0.8302\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.4891 - accuracy: 0.8193 - val_loss: 0.4130 - val_accuracy: 0.8459\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.4608 - accuracy: 0.8315 - val_loss: 0.4015 - val_accuracy: 0.8524\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.4370 - accuracy: 0.8395 - val_loss: 0.3834 - val_accuracy: 0.8578\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.4212 - accuracy: 0.8444 - val_loss: 0.3643 - val_accuracy: 0.8662\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.4083 - accuracy: 0.8498 - val_loss: 0.3541 - val_accuracy: 0.8690\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3936 - accuracy: 0.8543 - val_loss: 0.3414 - val_accuracy: 0.8767\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3856 - accuracy: 0.8573 - val_loss: 0.3388 - val_accuracy: 0.8746\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3742 - accuracy: 0.8608 - val_loss: 0.3275 - val_accuracy: 0.8795\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3657 - accuracy: 0.8649 - val_loss: 0.3243 - val_accuracy: 0.8797\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3568 - accuracy: 0.8692 - val_loss: 0.3208 - val_accuracy: 0.8813\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3486 - accuracy: 0.8700 - val_loss: 0.3165 - val_accuracy: 0.8827\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3417 - accuracy: 0.8742 - val_loss: 0.3106 - val_accuracy: 0.8852\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3358 - accuracy: 0.8752 - val_loss: 0.3141 - val_accuracy: 0.8834\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3346 - accuracy: 0.8765 - val_loss: 0.3001 - val_accuracy: 0.8869\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3262 - accuracy: 0.8789 - val_loss: 0.2988 - val_accuracy: 0.8900\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3238 - accuracy: 0.8801 - val_loss: 0.2974 - val_accuracy: 0.8897\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3206 - accuracy: 0.8813 - val_loss: 0.2913 - val_accuracy: 0.8915\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3157 - accuracy: 0.8826 - val_loss: 0.2929 - val_accuracy: 0.8883\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3138 - accuracy: 0.8840 - val_loss: 0.2906 - val_accuracy: 0.8932\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3093 - accuracy: 0.8856 - val_loss: 0.2831 - val_accuracy: 0.8958\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3064 - accuracy: 0.8869 - val_loss: 0.2809 - val_accuracy: 0.8955\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3035 - accuracy: 0.8890 - val_loss: 0.2919 - val_accuracy: 0.8928\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3006 - accuracy: 0.8891 - val_loss: 0.2793 - val_accuracy: 0.8971\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2952 - accuracy: 0.8910 - val_loss: 0.2826 - val_accuracy: 0.8949\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2969 - accuracy: 0.8902 - val_loss: 0.2770 - val_accuracy: 0.8984\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2966 - accuracy: 0.8904 - val_loss: 0.2727 - val_accuracy: 0.8989\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2896 - accuracy: 0.8931 - val_loss: 0.2779 - val_accuracy: 0.8972\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2880 - accuracy: 0.8924 - val_loss: 0.2723 - val_accuracy: 0.8996\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2832 - accuracy: 0.8968 - val_loss: 0.2772 - val_accuracy: 0.9002\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2814 - accuracy: 0.8950 - val_loss: 0.2723 - val_accuracy: 0.9010\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2807 - accuracy: 0.8959 - val_loss: 0.2707 - val_accuracy: 0.9003\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2773 - accuracy: 0.8970 - val_loss: 0.2684 - val_accuracy: 0.9011\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2790 - accuracy: 0.8954 - val_loss: 0.2689 - val_accuracy: 0.9001\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2731 - accuracy: 0.8983 - val_loss: 0.2689 - val_accuracy: 0.9001\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2717 - accuracy: 0.8982 - val_loss: 0.2654 - val_accuracy: 0.9029\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2711 - accuracy: 0.8999 - val_loss: 0.2674 - val_accuracy: 0.9019\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2694 - accuracy: 0.8994 - val_loss: 0.2595 - val_accuracy: 0.9055\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2687 - accuracy: 0.8991 - val_loss: 0.2641 - val_accuracy: 0.9039\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2678 - accuracy: 0.9004 - val_loss: 0.2637 - val_accuracy: 0.9024\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2686 - accuracy: 0.9002 - val_loss: 0.2658 - val_accuracy: 0.9013\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2595 - accuracy: 0.9055\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 1.1971 - accuracy: 0.5580 - val_loss: 0.6507 - val_accuracy: 0.7500\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.6852 - accuracy: 0.7456 - val_loss: 0.5564 - val_accuracy: 0.7860\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.6013 - accuracy: 0.7754 - val_loss: 0.5149 - val_accuracy: 0.8022\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.5549 - accuracy: 0.7948 - val_loss: 0.4759 - val_accuracy: 0.8198\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.5255 - accuracy: 0.8063 - val_loss: 0.4502 - val_accuracy: 0.8320\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4985 - accuracy: 0.8148 - val_loss: 0.4305 - val_accuracy: 0.8393\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.4783 - accuracy: 0.8236 - val_loss: 0.4221 - val_accuracy: 0.8445\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4596 - accuracy: 0.8307 - val_loss: 0.4074 - val_accuracy: 0.8494\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.4444 - accuracy: 0.8377 - val_loss: 0.3901 - val_accuracy: 0.8561\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4352 - accuracy: 0.8410 - val_loss: 0.3838 - val_accuracy: 0.8551\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4249 - accuracy: 0.8433 - val_loss: 0.3756 - val_accuracy: 0.8609\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4137 - accuracy: 0.8470 - val_loss: 0.3687 - val_accuracy: 0.8639\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.4048 - accuracy: 0.8503 - val_loss: 0.3632 - val_accuracy: 0.8644\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3990 - accuracy: 0.8529 - val_loss: 0.3589 - val_accuracy: 0.8647\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3903 - accuracy: 0.8556 - val_loss: 0.3503 - val_accuracy: 0.8699\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3854 - accuracy: 0.8570 - val_loss: 0.3465 - val_accuracy: 0.8709\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3800 - accuracy: 0.8587 - val_loss: 0.3402 - val_accuracy: 0.8744\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3730 - accuracy: 0.8622 - val_loss: 0.3378 - val_accuracy: 0.8733\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3713 - accuracy: 0.8630 - val_loss: 0.3333 - val_accuracy: 0.8763\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3646 - accuracy: 0.8650 - val_loss: 0.3290 - val_accuracy: 0.8787\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3584 - accuracy: 0.8664 - val_loss: 0.3273 - val_accuracy: 0.8806\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3551 - accuracy: 0.8690 - val_loss: 0.3228 - val_accuracy: 0.8799\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3520 - accuracy: 0.8691 - val_loss: 0.3212 - val_accuracy: 0.8826\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3464 - accuracy: 0.8713 - val_loss: 0.3191 - val_accuracy: 0.8831\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3437 - accuracy: 0.8722 - val_loss: 0.3154 - val_accuracy: 0.8842\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3409 - accuracy: 0.8738 - val_loss: 0.3118 - val_accuracy: 0.8849\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3381 - accuracy: 0.8738 - val_loss: 0.3093 - val_accuracy: 0.8863\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3316 - accuracy: 0.8757 - val_loss: 0.3054 - val_accuracy: 0.8870\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3305 - accuracy: 0.8792 - val_loss: 0.3025 - val_accuracy: 0.8903\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3284 - accuracy: 0.8790 - val_loss: 0.2998 - val_accuracy: 0.8892\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3234 - accuracy: 0.8803 - val_loss: 0.3005 - val_accuracy: 0.8885\n",
      "Epoch 32/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3213 - accuracy: 0.8802 - val_loss: 0.2987 - val_accuracy: 0.8914\n",
      "Epoch 33/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3199 - accuracy: 0.8814 - val_loss: 0.2944 - val_accuracy: 0.8907\n",
      "Epoch 34/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3132 - accuracy: 0.8832 - val_loss: 0.2944 - val_accuracy: 0.8912\n",
      "Epoch 35/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3117 - accuracy: 0.8850 - val_loss: 0.2942 - val_accuracy: 0.8920\n",
      "Epoch 36/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3102 - accuracy: 0.8857 - val_loss: 0.2921 - val_accuracy: 0.8943\n",
      "Epoch 37/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3104 - accuracy: 0.8845 - val_loss: 0.2913 - val_accuracy: 0.8923\n",
      "Epoch 38/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3070 - accuracy: 0.8860 - val_loss: 0.2903 - val_accuracy: 0.8940\n",
      "Epoch 39/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3043 - accuracy: 0.8874 - val_loss: 0.2915 - val_accuracy: 0.8943\n",
      "Epoch 40/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2997 - accuracy: 0.8894 - val_loss: 0.2865 - val_accuracy: 0.8949\n",
      "Epoch 41/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3009 - accuracy: 0.8892 - val_loss: 0.2829 - val_accuracy: 0.8959\n",
      "Epoch 42/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3004 - accuracy: 0.8887 - val_loss: 0.2842 - val_accuracy: 0.8975\n",
      "Epoch 43/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2962 - accuracy: 0.8903 - val_loss: 0.2821 - val_accuracy: 0.8975\n",
      "Epoch 44/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2939 - accuracy: 0.8908 - val_loss: 0.2800 - val_accuracy: 0.8988\n",
      "Epoch 45/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2917 - accuracy: 0.8906 - val_loss: 0.2810 - val_accuracy: 0.8989\n",
      "Epoch 46/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2929 - accuracy: 0.8916 - val_loss: 0.2774 - val_accuracy: 0.8997\n",
      "Epoch 47/100\n",
      "240/240 [==============================] - 9s 35ms/step - loss: 0.2868 - accuracy: 0.8933 - val_loss: 0.2776 - val_accuracy: 0.8984\n",
      "Epoch 48/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2868 - accuracy: 0.8939 - val_loss: 0.2768 - val_accuracy: 0.8990\n",
      "Epoch 49/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2820 - accuracy: 0.8977 - val_loss: 0.2790 - val_accuracy: 0.8989\n",
      "Epoch 50/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2842 - accuracy: 0.8945 - val_loss: 0.2793 - val_accuracy: 0.8971\n",
      "Epoch 51/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2822 - accuracy: 0.8957 - val_loss: 0.2750 - val_accuracy: 0.8997\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2809 - accuracy: 0.8947 - val_loss: 0.2760 - val_accuracy: 0.8995\n",
      "Epoch 53/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2825 - accuracy: 0.8960 - val_loss: 0.2744 - val_accuracy: 0.8994\n",
      "Epoch 54/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2831 - accuracy: 0.8952 - val_loss: 0.2743 - val_accuracy: 0.9021\n",
      "Epoch 55/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2770 - accuracy: 0.8976 - val_loss: 0.2720 - val_accuracy: 0.9012\n",
      "Epoch 56/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2770 - accuracy: 0.8970 - val_loss: 0.2693 - val_accuracy: 0.9034\n",
      "Epoch 57/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2740 - accuracy: 0.8979 - val_loss: 0.2734 - val_accuracy: 0.9009\n",
      "Epoch 58/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2726 - accuracy: 0.8996 - val_loss: 0.2713 - val_accuracy: 0.9020\n",
      "Epoch 59/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2717 - accuracy: 0.8990 - val_loss: 0.2706 - val_accuracy: 0.9019\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2693 - accuracy: 0.9034\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 1.4345 - accuracy: 0.4807 - val_loss: 0.7498 - val_accuracy: 0.7295\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.7827 - accuracy: 0.7123 - val_loss: 0.6115 - val_accuracy: 0.7684\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.6764 - accuracy: 0.7509 - val_loss: 0.5688 - val_accuracy: 0.7812\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.6240 - accuracy: 0.7678 - val_loss: 0.5314 - val_accuracy: 0.7955\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.5873 - accuracy: 0.7834 - val_loss: 0.5024 - val_accuracy: 0.8104\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.5551 - accuracy: 0.7944 - val_loss: 0.4861 - val_accuracy: 0.8193\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.5380 - accuracy: 0.8008 - val_loss: 0.4672 - val_accuracy: 0.8259\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.5205 - accuracy: 0.8072 - val_loss: 0.4510 - val_accuracy: 0.8328\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.5056 - accuracy: 0.8139 - val_loss: 0.4422 - val_accuracy: 0.8342\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4936 - accuracy: 0.8187 - val_loss: 0.4314 - val_accuracy: 0.8413\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4785 - accuracy: 0.8230 - val_loss: 0.4165 - val_accuracy: 0.8462\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4703 - accuracy: 0.8270 - val_loss: 0.4089 - val_accuracy: 0.8504\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4619 - accuracy: 0.8293 - val_loss: 0.4039 - val_accuracy: 0.8531\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4544 - accuracy: 0.8330 - val_loss: 0.3986 - val_accuracy: 0.8541\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4457 - accuracy: 0.8357 - val_loss: 0.3914 - val_accuracy: 0.8567\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4385 - accuracy: 0.8378 - val_loss: 0.3883 - val_accuracy: 0.8571\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4261 - accuracy: 0.8432 - val_loss: 0.3810 - val_accuracy: 0.8594\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4246 - accuracy: 0.8441 - val_loss: 0.3744 - val_accuracy: 0.8638\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4218 - accuracy: 0.8429 - val_loss: 0.3699 - val_accuracy: 0.8647\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4131 - accuracy: 0.8489 - val_loss: 0.3677 - val_accuracy: 0.8651\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4090 - accuracy: 0.8489 - val_loss: 0.3636 - val_accuracy: 0.8637\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4009 - accuracy: 0.8528 - val_loss: 0.3591 - val_accuracy: 0.8681\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4017 - accuracy: 0.8519 - val_loss: 0.3555 - val_accuracy: 0.8681\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3933 - accuracy: 0.8543 - val_loss: 0.3522 - val_accuracy: 0.8713\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3913 - accuracy: 0.8554 - val_loss: 0.3493 - val_accuracy: 0.8704\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3861 - accuracy: 0.8580 - val_loss: 0.3450 - val_accuracy: 0.8733\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3809 - accuracy: 0.8600 - val_loss: 0.3441 - val_accuracy: 0.8733\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3775 - accuracy: 0.8609 - val_loss: 0.3411 - val_accuracy: 0.8745\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3757 - accuracy: 0.8622 - val_loss: 0.3369 - val_accuracy: 0.8763\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3695 - accuracy: 0.8637 - val_loss: 0.3348 - val_accuracy: 0.8760\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3683 - accuracy: 0.8630 - val_loss: 0.3329 - val_accuracy: 0.8778\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3655 - accuracy: 0.8654 - val_loss: 0.3300 - val_accuracy: 0.8786\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3607 - accuracy: 0.8657 - val_loss: 0.3280 - val_accuracy: 0.8794\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3586 - accuracy: 0.8675 - val_loss: 0.3278 - val_accuracy: 0.8778\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.3548 - accuracy: 0.8679 - val_loss: 0.3212 - val_accuracy: 0.8816\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3512 - accuracy: 0.8705 - val_loss: 0.3219 - val_accuracy: 0.8797\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3490 - accuracy: 0.8702 - val_loss: 0.3180 - val_accuracy: 0.8826\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3479 - accuracy: 0.8708 - val_loss: 0.3192 - val_accuracy: 0.8818\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3451 - accuracy: 0.8717 - val_loss: 0.3172 - val_accuracy: 0.8817\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3444 - accuracy: 0.8725 - val_loss: 0.3118 - val_accuracy: 0.8844\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3381 - accuracy: 0.8761 - val_loss: 0.3108 - val_accuracy: 0.8855\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.3364 - accuracy: 0.8753 - val_loss: 0.3082 - val_accuracy: 0.8867\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3360 - accuracy: 0.8753 - val_loss: 0.3072 - val_accuracy: 0.8887\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3315 - accuracy: 0.8764 - val_loss: 0.3047 - val_accuracy: 0.8885\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3302 - accuracy: 0.8773 - val_loss: 0.3060 - val_accuracy: 0.8858\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3296 - accuracy: 0.8772 - val_loss: 0.3047 - val_accuracy: 0.8871\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3272 - accuracy: 0.8793 - val_loss: 0.3002 - val_accuracy: 0.8897\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3251 - accuracy: 0.8794 - val_loss: 0.3008 - val_accuracy: 0.8871\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3226 - accuracy: 0.8792 - val_loss: 0.3002 - val_accuracy: 0.8871\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3192 - accuracy: 0.8820 - val_loss: 0.2986 - val_accuracy: 0.8888\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3207 - accuracy: 0.8813 - val_loss: 0.2985 - val_accuracy: 0.8899\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3170 - accuracy: 0.8823 - val_loss: 0.2953 - val_accuracy: 0.8904\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3177 - accuracy: 0.8821 - val_loss: 0.2940 - val_accuracy: 0.8912\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3147 - accuracy: 0.8840 - val_loss: 0.2953 - val_accuracy: 0.8893\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3122 - accuracy: 0.8837 - val_loss: 0.2950 - val_accuracy: 0.8907\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3099 - accuracy: 0.8853 - val_loss: 0.2913 - val_accuracy: 0.8908\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3088 - accuracy: 0.8856 - val_loss: 0.2907 - val_accuracy: 0.8921\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3101 - accuracy: 0.8848 - val_loss: 0.2882 - val_accuracy: 0.8917\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3054 - accuracy: 0.8869 - val_loss: 0.2918 - val_accuracy: 0.8919\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3043 - accuracy: 0.8866 - val_loss: 0.2864 - val_accuracy: 0.8945\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3033 - accuracy: 0.8878 - val_loss: 0.2840 - val_accuracy: 0.8946\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3019 - accuracy: 0.8892 - val_loss: 0.2861 - val_accuracy: 0.8937\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3007 - accuracy: 0.8881 - val_loss: 0.2871 - val_accuracy: 0.8914\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3003 - accuracy: 0.8889 - val_loss: 0.2849 - val_accuracy: 0.8949\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2840 - accuracy: 0.8946\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.4914 - accuracy: 0.8233 - val_loss: 0.3723 - val_accuracy: 0.8658\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.3360 - accuracy: 0.8774 - val_loss: 0.3280 - val_accuracy: 0.8780\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2867 - accuracy: 0.8955 - val_loss: 0.3077 - val_accuracy: 0.8861\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.2595 - accuracy: 0.9041 - val_loss: 0.2901 - val_accuracy: 0.8927\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2339 - accuracy: 0.9119 - val_loss: 0.2818 - val_accuracy: 0.8960\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2153 - accuracy: 0.9198 - val_loss: 0.2704 - val_accuracy: 0.9022\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1962 - accuracy: 0.9268 - val_loss: 0.2712 - val_accuracy: 0.9025\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.1796 - accuracy: 0.9327 - val_loss: 0.2936 - val_accuracy: 0.8971\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 33s 56ms/step - loss: 0.1633 - accuracy: 0.9387 - val_loss: 0.2842 - val_accuracy: 0.9052\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2704 - accuracy: 0.9022\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.6176 - accuracy: 0.7783 - val_loss: 0.4841 - val_accuracy: 0.8141\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.4117 - accuracy: 0.8523 - val_loss: 0.3861 - val_accuracy: 0.8611\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3603 - accuracy: 0.8705 - val_loss: 0.3645 - val_accuracy: 0.8669\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3262 - accuracy: 0.8813 - val_loss: 0.3405 - val_accuracy: 0.8796\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 28s 117ms/step - loss: 0.3008 - accuracy: 0.8898 - val_loss: 0.3102 - val_accuracy: 0.8888\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.2785 - accuracy: 0.8964 - val_loss: 0.3187 - val_accuracy: 0.8849\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2631 - accuracy: 0.9036 - val_loss: 0.3030 - val_accuracy: 0.8932\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2468 - accuracy: 0.9086 - val_loss: 0.2942 - val_accuracy: 0.8953\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2351 - accuracy: 0.9127 - val_loss: 0.3088 - val_accuracy: 0.8895\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2233 - accuracy: 0.9176 - val_loss: 0.2953 - val_accuracy: 0.8964\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2066 - accuracy: 0.9230 - val_loss: 0.2939 - val_accuracy: 0.8974\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1988 - accuracy: 0.9259 - val_loss: 0.2831 - val_accuracy: 0.8967\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.1900 - accuracy: 0.9286 - val_loss: 0.2923 - val_accuracy: 0.9016\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.1754 - accuracy: 0.9345 - val_loss: 0.2869 - val_accuracy: 0.9003\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1647 - accuracy: 0.9384 - val_loss: 0.2863 - val_accuracy: 0.9008\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2831 - accuracy: 0.8967\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.6749 - accuracy: 0.7571 - val_loss: 0.4881 - val_accuracy: 0.8208\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.4160 - accuracy: 0.8519 - val_loss: 0.4041 - val_accuracy: 0.8544\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.3645 - accuracy: 0.8690 - val_loss: 0.3864 - val_accuracy: 0.8598\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.3298 - accuracy: 0.8811 - val_loss: 0.3475 - val_accuracy: 0.8755\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.3039 - accuracy: 0.8892 - val_loss: 0.3497 - val_accuracy: 0.8718\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2854 - accuracy: 0.8951 - val_loss: 0.3184 - val_accuracy: 0.8841\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.2745 - accuracy: 0.8983 - val_loss: 0.3046 - val_accuracy: 0.8911\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2549 - accuracy: 0.9058 - val_loss: 0.3001 - val_accuracy: 0.8899\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.2449 - accuracy: 0.9098 - val_loss: 0.3027 - val_accuracy: 0.8898\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2318 - accuracy: 0.9154 - val_loss: 0.2837 - val_accuracy: 0.8959\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2225 - accuracy: 0.9174 - val_loss: 0.2906 - val_accuracy: 0.8960\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2124 - accuracy: 0.9220 - val_loss: 0.2861 - val_accuracy: 0.8980\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2052 - accuracy: 0.9250 - val_loss: 0.2920 - val_accuracy: 0.8981\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2837 - accuracy: 0.8959\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.5945 - accuracy: 0.7855 - val_loss: 0.4003 - val_accuracy: 0.8619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.3925 - accuracy: 0.8600 - val_loss: 0.3472 - val_accuracy: 0.8767\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.3404 - accuracy: 0.8773 - val_loss: 0.3065 - val_accuracy: 0.8894\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.3125 - accuracy: 0.8882 - val_loss: 0.2984 - val_accuracy: 0.8886\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2900 - accuracy: 0.8942 - val_loss: 0.2966 - val_accuracy: 0.8893\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2738 - accuracy: 0.8992 - val_loss: 0.2795 - val_accuracy: 0.8965\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2582 - accuracy: 0.9061 - val_loss: 0.2802 - val_accuracy: 0.8952\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2475 - accuracy: 0.9090 - val_loss: 0.2757 - val_accuracy: 0.9016\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2334 - accuracy: 0.9136 - val_loss: 0.2819 - val_accuracy: 0.8987\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2278 - accuracy: 0.9168 - val_loss: 0.2761 - val_accuracy: 0.8998\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2157 - accuracy: 0.9183 - val_loss: 0.3059 - val_accuracy: 0.8938\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2757 - accuracy: 0.9016\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.7085 - accuracy: 0.7441 - val_loss: 0.4517 - val_accuracy: 0.8361\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.4426 - accuracy: 0.8412 - val_loss: 0.3815 - val_accuracy: 0.8623\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.3836 - accuracy: 0.8612 - val_loss: 0.3444 - val_accuracy: 0.8763\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.3509 - accuracy: 0.8732 - val_loss: 0.3227 - val_accuracy: 0.8855\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.3252 - accuracy: 0.8817 - val_loss: 0.3160 - val_accuracy: 0.8856\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.3064 - accuracy: 0.8880 - val_loss: 0.2990 - val_accuracy: 0.8918\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2905 - accuracy: 0.8943 - val_loss: 0.2908 - val_accuracy: 0.8950\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2775 - accuracy: 0.8979 - val_loss: 0.2870 - val_accuracy: 0.8980\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.2654 - accuracy: 0.9021 - val_loss: 0.2853 - val_accuracy: 0.8989\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2526 - accuracy: 0.9072 - val_loss: 0.2909 - val_accuracy: 0.8957\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2438 - accuracy: 0.9102 - val_loss: 0.2868 - val_accuracy: 0.9006\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.2365 - accuracy: 0.9129 - val_loss: 0.2714 - val_accuracy: 0.9051\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.2245 - accuracy: 0.9169 - val_loss: 0.2730 - val_accuracy: 0.9064\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.2198 - accuracy: 0.9191 - val_loss: 0.2657 - val_accuracy: 0.9080\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.2094 - accuracy: 0.9219 - val_loss: 0.2658 - val_accuracy: 0.9086\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2058 - accuracy: 0.9227 - val_loss: 0.2789 - val_accuracy: 0.9067\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.1969 - accuracy: 0.9268 - val_loss: 0.2659 - val_accuracy: 0.9096\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2657 - accuracy: 0.9080\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.8695 - accuracy: 0.6822 - val_loss: 0.5205 - val_accuracy: 0.8040\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.5225 - accuracy: 0.8104 - val_loss: 0.4187 - val_accuracy: 0.8493\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.4347 - accuracy: 0.8431 - val_loss: 0.3812 - val_accuracy: 0.8605\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3938 - accuracy: 0.8594 - val_loss: 0.3643 - val_accuracy: 0.8666\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.3682 - accuracy: 0.8679 - val_loss: 0.3337 - val_accuracy: 0.8822\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.3426 - accuracy: 0.8768 - val_loss: 0.3227 - val_accuracy: 0.8809\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3244 - accuracy: 0.8830 - val_loss: 0.3116 - val_accuracy: 0.8869\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.3114 - accuracy: 0.8877 - val_loss: 0.2993 - val_accuracy: 0.8946\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2966 - accuracy: 0.8921 - val_loss: 0.2949 - val_accuracy: 0.8909\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2868 - accuracy: 0.8964 - val_loss: 0.2838 - val_accuracy: 0.8998\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2785 - accuracy: 0.8985 - val_loss: 0.2784 - val_accuracy: 0.8983\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2697 - accuracy: 0.9036 - val_loss: 0.2725 - val_accuracy: 0.9009\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2587 - accuracy: 0.9067 - val_loss: 0.2728 - val_accuracy: 0.9013\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2522 - accuracy: 0.9085 - val_loss: 0.2651 - val_accuracy: 0.9050\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2462 - accuracy: 0.9099 - val_loss: 0.2707 - val_accuracy: 0.9034\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.2349 - accuracy: 0.9142 - val_loss: 0.2699 - val_accuracy: 0.9044\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2323 - accuracy: 0.9140 - val_loss: 0.2601 - val_accuracy: 0.9057\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.2260 - accuracy: 0.9174 - val_loss: 0.2695 - val_accuracy: 0.9035\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2181 - accuracy: 0.9192 - val_loss: 0.2634 - val_accuracy: 0.9057\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2121 - accuracy: 0.9225 - val_loss: 0.2569 - val_accuracy: 0.9093\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.2060 - accuracy: 0.9241 - val_loss: 0.2597 - val_accuracy: 0.9074\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.2015 - accuracy: 0.9254 - val_loss: 0.2585 - val_accuracy: 0.9096\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.1967 - accuracy: 0.9261 - val_loss: 0.2654 - val_accuracy: 0.9095\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2569 - accuracy: 0.9093\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.5789 - accuracy: 0.7880 - val_loss: 0.4458 - val_accuracy: 0.8404\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.3936 - accuracy: 0.8609 - val_loss: 0.3778 - val_accuracy: 0.8649\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.3404 - accuracy: 0.8786 - val_loss: 0.3513 - val_accuracy: 0.8750\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.3072 - accuracy: 0.8890 - val_loss: 0.3275 - val_accuracy: 0.8816\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2815 - accuracy: 0.8980 - val_loss: 0.3504 - val_accuracy: 0.8746\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2618 - accuracy: 0.9040 - val_loss: 0.2997 - val_accuracy: 0.8943\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2432 - accuracy: 0.9115 - val_loss: 0.3044 - val_accuracy: 0.8891\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2287 - accuracy: 0.9158 - val_loss: 0.2765 - val_accuracy: 0.8997\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2137 - accuracy: 0.9215 - val_loss: 0.2770 - val_accuracy: 0.8992\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.1997 - accuracy: 0.9258 - val_loss: 0.2743 - val_accuracy: 0.9024\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1866 - accuracy: 0.9305 - val_loss: 0.2875 - val_accuracy: 0.9001\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.1759 - accuracy: 0.9349 - val_loss: 0.2722 - val_accuracy: 0.9042\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.1657 - accuracy: 0.9388 - val_loss: 0.2767 - val_accuracy: 0.9043\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1556 - accuracy: 0.9426 - val_loss: 0.2747 - val_accuracy: 0.9057\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1431 - accuracy: 0.9472 - val_loss: 0.2852 - val_accuracy: 0.9056\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2722 - accuracy: 0.9042\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.6357 - accuracy: 0.7823 - val_loss: 0.4857 - val_accuracy: 0.8261\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.4223 - accuracy: 0.8523 - val_loss: 0.4493 - val_accuracy: 0.8452\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 27s 115ms/step - loss: 0.3703 - accuracy: 0.8694 - val_loss: 0.3768 - val_accuracy: 0.8683\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.3361 - accuracy: 0.8808 - val_loss: 0.3603 - val_accuracy: 0.8712\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3157 - accuracy: 0.8872 - val_loss: 0.3380 - val_accuracy: 0.8774\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2939 - accuracy: 0.8953 - val_loss: 0.3418 - val_accuracy: 0.8761\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2790 - accuracy: 0.9006 - val_loss: 0.3276 - val_accuracy: 0.8798\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2656 - accuracy: 0.9048 - val_loss: 0.3177 - val_accuracy: 0.8878\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2528 - accuracy: 0.9082 - val_loss: 0.3104 - val_accuracy: 0.8913\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2396 - accuracy: 0.9128 - val_loss: 0.3121 - val_accuracy: 0.8866\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2310 - accuracy: 0.9158 - val_loss: 0.2981 - val_accuracy: 0.8970\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2203 - accuracy: 0.9192 - val_loss: 0.2999 - val_accuracy: 0.8962\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2093 - accuracy: 0.9234 - val_loss: 0.2872 - val_accuracy: 0.8989\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1986 - accuracy: 0.9269 - val_loss: 0.2796 - val_accuracy: 0.9016\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1916 - accuracy: 0.9297 - val_loss: 0.2808 - val_accuracy: 0.9021\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 27s 115ms/step - loss: 0.1841 - accuracy: 0.9319 - val_loss: 0.2985 - val_accuracy: 0.8950\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1740 - accuracy: 0.9373 - val_loss: 0.2963 - val_accuracy: 0.9012\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2796 - accuracy: 0.9016\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.7896 - accuracy: 0.7173 - val_loss: 0.5548 - val_accuracy: 0.7960\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.4989 - accuracy: 0.8190 - val_loss: 0.4795 - val_accuracy: 0.8268\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.4450 - accuracy: 0.8414 - val_loss: 0.4456 - val_accuracy: 0.8372\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.4065 - accuracy: 0.8547 - val_loss: 0.4254 - val_accuracy: 0.8500\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3857 - accuracy: 0.8618 - val_loss: 0.3912 - val_accuracy: 0.8596\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3666 - accuracy: 0.8697 - val_loss: 0.3861 - val_accuracy: 0.8627\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3492 - accuracy: 0.8748 - val_loss: 0.3676 - val_accuracy: 0.8692\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3305 - accuracy: 0.8824 - val_loss: 0.3600 - val_accuracy: 0.8719\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3169 - accuracy: 0.8870 - val_loss: 0.3403 - val_accuracy: 0.8796\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3064 - accuracy: 0.8896 - val_loss: 0.3330 - val_accuracy: 0.8798\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2953 - accuracy: 0.8944 - val_loss: 0.3438 - val_accuracy: 0.8772\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2871 - accuracy: 0.8973 - val_loss: 0.3267 - val_accuracy: 0.8832\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2794 - accuracy: 0.8993 - val_loss: 0.3284 - val_accuracy: 0.8829\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.2719 - accuracy: 0.9029 - val_loss: 0.3121 - val_accuracy: 0.8897\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2665 - accuracy: 0.9040 - val_loss: 0.3155 - val_accuracy: 0.8856\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2522 - accuracy: 0.9090 - val_loss: 0.3074 - val_accuracy: 0.8879\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2457 - accuracy: 0.9119 - val_loss: 0.2977 - val_accuracy: 0.8918\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2439 - accuracy: 0.9128 - val_loss: 0.3067 - val_accuracy: 0.8879\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2410 - accuracy: 0.9128 - val_loss: 0.3173 - val_accuracy: 0.8846\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2333 - accuracy: 0.9148 - val_loss: 0.2954 - val_accuracy: 0.8958\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2268 - accuracy: 0.9174 - val_loss: 0.3018 - val_accuracy: 0.8912\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2244 - accuracy: 0.9176 - val_loss: 0.2858 - val_accuracy: 0.8978\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.2161 - accuracy: 0.9220 - val_loss: 0.2929 - val_accuracy: 0.8955\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2107 - accuracy: 0.9239 - val_loss: 0.2898 - val_accuracy: 0.8967\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2077 - accuracy: 0.9252 - val_loss: 0.2958 - val_accuracy: 0.8945\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2858 - accuracy: 0.8978\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.7096 - accuracy: 0.7435 - val_loss: 0.4745 - val_accuracy: 0.8298\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 33s 55ms/step - loss: 0.4699 - accuracy: 0.8310 - val_loss: 0.3827 - val_accuracy: 0.8618\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.4031 - accuracy: 0.8552 - val_loss: 0.3641 - val_accuracy: 0.8675\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.3642 - accuracy: 0.8693 - val_loss: 0.3253 - val_accuracy: 0.8847\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.3401 - accuracy: 0.8775 - val_loss: 0.3182 - val_accuracy: 0.8860\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.3180 - accuracy: 0.8851 - val_loss: 0.3011 - val_accuracy: 0.8898\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.3021 - accuracy: 0.8906 - val_loss: 0.2917 - val_accuracy: 0.8953\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.2859 - accuracy: 0.8958 - val_loss: 0.2895 - val_accuracy: 0.8959\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2758 - accuracy: 0.8998 - val_loss: 0.2811 - val_accuracy: 0.8995\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2654 - accuracy: 0.9035 - val_loss: 0.2824 - val_accuracy: 0.8981\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2548 - accuracy: 0.9070 - val_loss: 0.2697 - val_accuracy: 0.9030\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2449 - accuracy: 0.9111 - val_loss: 0.2690 - val_accuracy: 0.9032\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2366 - accuracy: 0.9138 - val_loss: 0.2587 - val_accuracy: 0.9093\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2297 - accuracy: 0.9160 - val_loss: 0.2656 - val_accuracy: 0.9039\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2206 - accuracy: 0.9187 - val_loss: 0.2653 - val_accuracy: 0.9059\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2152 - accuracy: 0.9197 - val_loss: 0.2519 - val_accuracy: 0.9100\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2051 - accuracy: 0.9251 - val_loss: 0.2537 - val_accuracy: 0.9097\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1989 - accuracy: 0.9276 - val_loss: 0.2514 - val_accuracy: 0.9112\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.1937 - accuracy: 0.9277 - val_loss: 0.2552 - val_accuracy: 0.9127\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.1880 - accuracy: 0.9300 - val_loss: 0.2570 - val_accuracy: 0.9133\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1823 - accuracy: 0.9320 - val_loss: 0.2594 - val_accuracy: 0.9092\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2514 - accuracy: 0.9112\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.8056 - accuracy: 0.7154 - val_loss: 0.4880 - val_accuracy: 0.8208\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.4921 - accuracy: 0.8242 - val_loss: 0.4075 - val_accuracy: 0.8541\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.4202 - accuracy: 0.8492 - val_loss: 0.3666 - val_accuracy: 0.8693\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3776 - accuracy: 0.8654 - val_loss: 0.3363 - val_accuracy: 0.8778\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.3513 - accuracy: 0.8749 - val_loss: 0.3202 - val_accuracy: 0.8844\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3331 - accuracy: 0.8799 - val_loss: 0.3066 - val_accuracy: 0.8888\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.3145 - accuracy: 0.8869 - val_loss: 0.2941 - val_accuracy: 0.8951\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3021 - accuracy: 0.8912 - val_loss: 0.2920 - val_accuracy: 0.8952\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2897 - accuracy: 0.8963 - val_loss: 0.2798 - val_accuracy: 0.8975\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2794 - accuracy: 0.8990 - val_loss: 0.2762 - val_accuracy: 0.9010\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2691 - accuracy: 0.9026 - val_loss: 0.2764 - val_accuracy: 0.9016\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2590 - accuracy: 0.9060 - val_loss: 0.2629 - val_accuracy: 0.9038\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2501 - accuracy: 0.9094 - val_loss: 0.2663 - val_accuracy: 0.9058\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2418 - accuracy: 0.9123 - val_loss: 0.2617 - val_accuracy: 0.9071\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2357 - accuracy: 0.9144 - val_loss: 0.2555 - val_accuracy: 0.9072\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2290 - accuracy: 0.9169 - val_loss: 0.2563 - val_accuracy: 0.9074\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2215 - accuracy: 0.9186 - val_loss: 0.2608 - val_accuracy: 0.9044\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2187 - accuracy: 0.9206 - val_loss: 0.2563 - val_accuracy: 0.9076\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2555 - accuracy: 0.9072\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.9344 - accuracy: 0.6676 - val_loss: 0.5466 - val_accuracy: 0.7977\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.5611 - accuracy: 0.7954 - val_loss: 0.4569 - val_accuracy: 0.8343\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.4805 - accuracy: 0.8277 - val_loss: 0.4082 - val_accuracy: 0.8533\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.4395 - accuracy: 0.8458 - val_loss: 0.3855 - val_accuracy: 0.8617\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.4058 - accuracy: 0.8550 - val_loss: 0.3712 - val_accuracy: 0.8672\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3823 - accuracy: 0.8632 - val_loss: 0.3489 - val_accuracy: 0.8733\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3631 - accuracy: 0.8699 - val_loss: 0.3399 - val_accuracy: 0.8770\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.3472 - accuracy: 0.8757 - val_loss: 0.3265 - val_accuracy: 0.8846\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3326 - accuracy: 0.8795 - val_loss: 0.3122 - val_accuracy: 0.8880\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.3205 - accuracy: 0.8851 - val_loss: 0.3053 - val_accuracy: 0.8901\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3086 - accuracy: 0.8887 - val_loss: 0.3028 - val_accuracy: 0.8906\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3005 - accuracy: 0.8917 - val_loss: 0.2906 - val_accuracy: 0.8953\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2911 - accuracy: 0.8950 - val_loss: 0.2858 - val_accuracy: 0.8967\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2816 - accuracy: 0.8978 - val_loss: 0.2796 - val_accuracy: 0.8977\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2742 - accuracy: 0.9004 - val_loss: 0.2829 - val_accuracy: 0.8981\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2709 - accuracy: 0.9012 - val_loss: 0.2754 - val_accuracy: 0.8996\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2624 - accuracy: 0.9041 - val_loss: 0.2733 - val_accuracy: 0.8984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2555 - accuracy: 0.9071 - val_loss: 0.2701 - val_accuracy: 0.8998\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2532 - accuracy: 0.9084 - val_loss: 0.2675 - val_accuracy: 0.9026\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2436 - accuracy: 0.9113 - val_loss: 0.2630 - val_accuracy: 0.9046\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2403 - accuracy: 0.9132 - val_loss: 0.2613 - val_accuracy: 0.9045\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2360 - accuracy: 0.9142 - val_loss: 0.2602 - val_accuracy: 0.9069\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2296 - accuracy: 0.9155 - val_loss: 0.2596 - val_accuracy: 0.9062\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2265 - accuracy: 0.9171 - val_loss: 0.2632 - val_accuracy: 0.9081\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2232 - accuracy: 0.9187 - val_loss: 0.2682 - val_accuracy: 0.9075\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2169 - accuracy: 0.9204 - val_loss: 0.2556 - val_accuracy: 0.9086\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2105 - accuracy: 0.9230 - val_loss: 0.2522 - val_accuracy: 0.9088\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2109 - accuracy: 0.9227 - val_loss: 0.2475 - val_accuracy: 0.9113\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2023 - accuracy: 0.9255 - val_loss: 0.2532 - val_accuracy: 0.9091\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.1994 - accuracy: 0.9266 - val_loss: 0.2485 - val_accuracy: 0.9103\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.1981 - accuracy: 0.9266 - val_loss: 0.2566 - val_accuracy: 0.9083\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2475 - accuracy: 0.9113\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.7003 - accuracy: 0.7388 - val_loss: 0.5324 - val_accuracy: 0.7923\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4490 - accuracy: 0.8367 - val_loss: 0.4194 - val_accuracy: 0.8484\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3847 - accuracy: 0.8617 - val_loss: 0.3954 - val_accuracy: 0.8593\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3508 - accuracy: 0.8744 - val_loss: 0.3703 - val_accuracy: 0.8667\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3268 - accuracy: 0.8820 - val_loss: 0.3560 - val_accuracy: 0.8748\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3117 - accuracy: 0.8867 - val_loss: 0.3386 - val_accuracy: 0.8771\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2969 - accuracy: 0.8917 - val_loss: 0.3298 - val_accuracy: 0.8817\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2823 - accuracy: 0.8976 - val_loss: 0.3187 - val_accuracy: 0.8855\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2730 - accuracy: 0.8993 - val_loss: 0.3266 - val_accuracy: 0.8838\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2631 - accuracy: 0.9023 - val_loss: 0.3090 - val_accuracy: 0.8875\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2537 - accuracy: 0.9067 - val_loss: 0.3172 - val_accuracy: 0.8892\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2462 - accuracy: 0.9093 - val_loss: 0.3020 - val_accuracy: 0.8912\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2364 - accuracy: 0.9130 - val_loss: 0.3206 - val_accuracy: 0.8852\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2311 - accuracy: 0.9141 - val_loss: 0.3053 - val_accuracy: 0.8913\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2219 - accuracy: 0.9184 - val_loss: 0.3112 - val_accuracy: 0.8921\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3020 - accuracy: 0.8912\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.8194 - accuracy: 0.7048 - val_loss: 0.5816 - val_accuracy: 0.7797\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.5139 - accuracy: 0.8103 - val_loss: 0.4989 - val_accuracy: 0.8190\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4521 - accuracy: 0.8372 - val_loss: 0.4571 - val_accuracy: 0.8305\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4123 - accuracy: 0.8507 - val_loss: 0.4249 - val_accuracy: 0.8454\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3852 - accuracy: 0.8598 - val_loss: 0.4073 - val_accuracy: 0.8535\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3640 - accuracy: 0.8680 - val_loss: 0.3820 - val_accuracy: 0.8613\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3506 - accuracy: 0.8726 - val_loss: 0.3709 - val_accuracy: 0.8659\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3363 - accuracy: 0.8775 - val_loss: 0.3710 - val_accuracy: 0.8687\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3249 - accuracy: 0.8814 - val_loss: 0.3497 - val_accuracy: 0.8727\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3130 - accuracy: 0.8862 - val_loss: 0.3553 - val_accuracy: 0.8701\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3076 - accuracy: 0.8874 - val_loss: 0.3340 - val_accuracy: 0.8804\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3002 - accuracy: 0.8910 - val_loss: 0.3298 - val_accuracy: 0.8800\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2916 - accuracy: 0.8940 - val_loss: 0.3244 - val_accuracy: 0.8819\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2841 - accuracy: 0.8970 - val_loss: 0.3277 - val_accuracy: 0.8834\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 9s 40ms/step - loss: 0.2773 - accuracy: 0.8985 - val_loss: 0.3282 - val_accuracy: 0.8821\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2698 - accuracy: 0.9008 - val_loss: 0.3165 - val_accuracy: 0.8840\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2649 - accuracy: 0.9030 - val_loss: 0.3173 - val_accuracy: 0.8853\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2614 - accuracy: 0.9044 - val_loss: 0.3146 - val_accuracy: 0.8879\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2553 - accuracy: 0.9075 - val_loss: 0.3181 - val_accuracy: 0.8839\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2505 - accuracy: 0.9083 - val_loss: 0.3296 - val_accuracy: 0.8833\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2449 - accuracy: 0.9104 - val_loss: 0.3068 - val_accuracy: 0.8899\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2423 - accuracy: 0.9119 - val_loss: 0.3092 - val_accuracy: 0.8882\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2362 - accuracy: 0.9137 - val_loss: 0.3106 - val_accuracy: 0.8895\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2333 - accuracy: 0.9144 - val_loss: 0.3018 - val_accuracy: 0.8937\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2288 - accuracy: 0.9165 - val_loss: 0.3087 - val_accuracy: 0.8907\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2255 - accuracy: 0.9171 - val_loss: 0.3014 - val_accuracy: 0.8938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2221 - accuracy: 0.9187 - val_loss: 0.3092 - val_accuracy: 0.8883\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2196 - accuracy: 0.9187 - val_loss: 0.3056 - val_accuracy: 0.8921\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 10s 42ms/step - loss: 0.2154 - accuracy: 0.9201 - val_loss: 0.3104 - val_accuracy: 0.8886\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3014 - accuracy: 0.8938\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.9354 - accuracy: 0.6776 - val_loss: 0.6267 - val_accuracy: 0.7699\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.5611 - accuracy: 0.7904 - val_loss: 0.5443 - val_accuracy: 0.7993\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4917 - accuracy: 0.8201 - val_loss: 0.4951 - val_accuracy: 0.8204\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4535 - accuracy: 0.8375 - val_loss: 0.4638 - val_accuracy: 0.8336\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 9s 78ms/step - loss: 0.4250 - accuracy: 0.8480 - val_loss: 0.4450 - val_accuracy: 0.8429\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4025 - accuracy: 0.8554 - val_loss: 0.4103 - val_accuracy: 0.8532\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3819 - accuracy: 0.8619 - val_loss: 0.4122 - val_accuracy: 0.8509\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3747 - accuracy: 0.8647 - val_loss: 0.3922 - val_accuracy: 0.8610\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3568 - accuracy: 0.8707 - val_loss: 0.3825 - val_accuracy: 0.8612\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 9s 78ms/step - loss: 0.3441 - accuracy: 0.8753 - val_loss: 0.3673 - val_accuracy: 0.8726\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3370 - accuracy: 0.8782 - val_loss: 0.3694 - val_accuracy: 0.8703\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3244 - accuracy: 0.8826 - val_loss: 0.3561 - val_accuracy: 0.8742\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3176 - accuracy: 0.8859 - val_loss: 0.3520 - val_accuracy: 0.8760\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3104 - accuracy: 0.8877 - val_loss: 0.3425 - val_accuracy: 0.8762\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3050 - accuracy: 0.8885 - val_loss: 0.3408 - val_accuracy: 0.8784\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3009 - accuracy: 0.8905 - val_loss: 0.3429 - val_accuracy: 0.8795\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2934 - accuracy: 0.8927 - val_loss: 0.3311 - val_accuracy: 0.8823\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.2885 - accuracy: 0.8957 - val_loss: 0.3389 - val_accuracy: 0.8796\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2863 - accuracy: 0.8956 - val_loss: 0.3250 - val_accuracy: 0.8844\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.2820 - accuracy: 0.8963 - val_loss: 0.3377 - val_accuracy: 0.8832\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2765 - accuracy: 0.8983 - val_loss: 0.3243 - val_accuracy: 0.8847\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.2731 - accuracy: 0.9000 - val_loss: 0.3162 - val_accuracy: 0.8885\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.2686 - accuracy: 0.9017 - val_loss: 0.3192 - val_accuracy: 0.8877\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.2643 - accuracy: 0.9029 - val_loss: 0.3231 - val_accuracy: 0.8861\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.2626 - accuracy: 0.9039 - val_loss: 0.3218 - val_accuracy: 0.8874\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3162 - accuracy: 0.8885\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.8358 - accuracy: 0.6847 - val_loss: 0.5415 - val_accuracy: 0.7919\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.5630 - accuracy: 0.7913 - val_loss: 0.4646 - val_accuracy: 0.8264\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4958 - accuracy: 0.8187 - val_loss: 0.4201 - val_accuracy: 0.8474\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.4535 - accuracy: 0.8345 - val_loss: 0.3880 - val_accuracy: 0.8589\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4259 - accuracy: 0.8443 - val_loss: 0.3718 - val_accuracy: 0.8618\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4061 - accuracy: 0.8509 - val_loss: 0.3565 - val_accuracy: 0.8644\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3891 - accuracy: 0.8571 - val_loss: 0.3444 - val_accuracy: 0.8710\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3776 - accuracy: 0.8618 - val_loss: 0.3363 - val_accuracy: 0.8759\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3678 - accuracy: 0.8649 - val_loss: 0.3266 - val_accuracy: 0.8809\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3557 - accuracy: 0.8689 - val_loss: 0.3180 - val_accuracy: 0.8819\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3454 - accuracy: 0.8739 - val_loss: 0.3198 - val_accuracy: 0.8838\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3414 - accuracy: 0.8736 - val_loss: 0.3183 - val_accuracy: 0.8810\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3313 - accuracy: 0.8775 - val_loss: 0.3138 - val_accuracy: 0.8864\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3255 - accuracy: 0.8801 - val_loss: 0.3071 - val_accuracy: 0.8883\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.3240 - accuracy: 0.8817 - val_loss: 0.3024 - val_accuracy: 0.8919\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3173 - accuracy: 0.8828 - val_loss: 0.2995 - val_accuracy: 0.8889\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3146 - accuracy: 0.8838 - val_loss: 0.3127 - val_accuracy: 0.8835\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3098 - accuracy: 0.8853 - val_loss: 0.3113 - val_accuracy: 0.8848\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3095 - accuracy: 0.8853 - val_loss: 0.3026 - val_accuracy: 0.8917\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2995 - accuracy: 0.8889\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.9920 - accuracy: 0.6336 - val_loss: 0.5683 - val_accuracy: 0.7765\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.5956 - accuracy: 0.7749 - val_loss: 0.4749 - val_accuracy: 0.8275\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.5199 - accuracy: 0.8096 - val_loss: 0.4348 - val_accuracy: 0.8386\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4773 - accuracy: 0.8265 - val_loss: 0.4014 - val_accuracy: 0.8530\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4476 - accuracy: 0.8390 - val_loss: 0.3760 - val_accuracy: 0.8620\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4221 - accuracy: 0.8464 - val_loss: 0.3606 - val_accuracy: 0.8663\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4071 - accuracy: 0.8522 - val_loss: 0.3549 - val_accuracy: 0.8672\n",
      "Epoch 8/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3901 - accuracy: 0.8585 - val_loss: 0.3416 - val_accuracy: 0.8755\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3785 - accuracy: 0.8621 - val_loss: 0.3358 - val_accuracy: 0.8777\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3729 - accuracy: 0.8630 - val_loss: 0.3325 - val_accuracy: 0.8795\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3616 - accuracy: 0.8674 - val_loss: 0.3253 - val_accuracy: 0.8819\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3562 - accuracy: 0.8695 - val_loss: 0.3183 - val_accuracy: 0.8837\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3517 - accuracy: 0.8719 - val_loss: 0.3139 - val_accuracy: 0.8862\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3416 - accuracy: 0.8750 - val_loss: 0.3097 - val_accuracy: 0.8884\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3360 - accuracy: 0.8773 - val_loss: 0.3099 - val_accuracy: 0.8884\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3331 - accuracy: 0.8763 - val_loss: 0.3088 - val_accuracy: 0.8900\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3289 - accuracy: 0.8785 - val_loss: 0.3078 - val_accuracy: 0.8915\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3211 - accuracy: 0.8831 - val_loss: 0.2979 - val_accuracy: 0.8919\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3206 - accuracy: 0.8810 - val_loss: 0.3045 - val_accuracy: 0.8880\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3143 - accuracy: 0.8837 - val_loss: 0.2946 - val_accuracy: 0.8939\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3118 - accuracy: 0.8854 - val_loss: 0.2927 - val_accuracy: 0.8950\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3088 - accuracy: 0.8864 - val_loss: 0.2922 - val_accuracy: 0.8943\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3050 - accuracy: 0.8866 - val_loss: 0.2897 - val_accuracy: 0.8938\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3048 - accuracy: 0.8880 - val_loss: 0.2907 - val_accuracy: 0.8971\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3013 - accuracy: 0.8887 - val_loss: 0.2977 - val_accuracy: 0.8917\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3006 - accuracy: 0.8884 - val_loss: 0.2852 - val_accuracy: 0.8987\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2973 - accuracy: 0.8899 - val_loss: 0.2871 - val_accuracy: 0.8946\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2963 - accuracy: 0.8906 - val_loss: 0.2897 - val_accuracy: 0.8971\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2914 - accuracy: 0.8915 - val_loss: 0.2807 - val_accuracy: 0.8991\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2905 - accuracy: 0.8929 - val_loss: 0.2815 - val_accuracy: 0.8990\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2881 - accuracy: 0.8939 - val_loss: 0.2762 - val_accuracy: 0.9005\n",
      "Epoch 32/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2836 - accuracy: 0.8950 - val_loss: 0.2744 - val_accuracy: 0.9024\n",
      "Epoch 33/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2824 - accuracy: 0.8958 - val_loss: 0.2778 - val_accuracy: 0.8994\n",
      "Epoch 34/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2806 - accuracy: 0.8959 - val_loss: 0.2758 - val_accuracy: 0.9000\n",
      "Epoch 35/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2793 - accuracy: 0.8961 - val_loss: 0.2747 - val_accuracy: 0.9012\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2744 - accuracy: 0.9024\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 1.2195 - accuracy: 0.5474 - val_loss: 0.6596 - val_accuracy: 0.7500\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.6890 - accuracy: 0.7387 - val_loss: 0.5625 - val_accuracy: 0.7876\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.6063 - accuracy: 0.7727 - val_loss: 0.5027 - val_accuracy: 0.8138\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 10s 85ms/step - loss: 0.5567 - accuracy: 0.7957 - val_loss: 0.4735 - val_accuracy: 0.8281\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.5188 - accuracy: 0.8102 - val_loss: 0.4422 - val_accuracy: 0.8384\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.4876 - accuracy: 0.8238 - val_loss: 0.4193 - val_accuracy: 0.8470\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.4625 - accuracy: 0.8339 - val_loss: 0.4007 - val_accuracy: 0.8551\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4442 - accuracy: 0.8383 - val_loss: 0.3915 - val_accuracy: 0.8567\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.4278 - accuracy: 0.8462 - val_loss: 0.3755 - val_accuracy: 0.8637\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.4159 - accuracy: 0.8488 - val_loss: 0.3683 - val_accuracy: 0.8660\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 10s 82ms/step - loss: 0.4080 - accuracy: 0.8521 - val_loss: 0.3664 - val_accuracy: 0.8697\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3986 - accuracy: 0.8554 - val_loss: 0.3574 - val_accuracy: 0.8729\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3892 - accuracy: 0.8585 - val_loss: 0.3482 - val_accuracy: 0.8745\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3821 - accuracy: 0.8616 - val_loss: 0.3509 - val_accuracy: 0.8756\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3727 - accuracy: 0.8626 - val_loss: 0.3381 - val_accuracy: 0.8773\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3686 - accuracy: 0.8666 - val_loss: 0.3318 - val_accuracy: 0.8807\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3632 - accuracy: 0.8661 - val_loss: 0.3364 - val_accuracy: 0.8787\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3586 - accuracy: 0.8699 - val_loss: 0.3294 - val_accuracy: 0.8793\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3530 - accuracy: 0.8708 - val_loss: 0.3288 - val_accuracy: 0.8820\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3487 - accuracy: 0.8721 - val_loss: 0.3196 - val_accuracy: 0.8845\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3433 - accuracy: 0.8743 - val_loss: 0.3210 - val_accuracy: 0.8852\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3407 - accuracy: 0.8758 - val_loss: 0.3187 - val_accuracy: 0.8827\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3349 - accuracy: 0.8764 - val_loss: 0.3195 - val_accuracy: 0.8842\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3327 - accuracy: 0.8777 - val_loss: 0.3199 - val_accuracy: 0.8844\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3307 - accuracy: 0.8784 - val_loss: 0.3098 - val_accuracy: 0.8890\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3286 - accuracy: 0.8794 - val_loss: 0.3091 - val_accuracy: 0.8874\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3240 - accuracy: 0.8792 - val_loss: 0.3126 - val_accuracy: 0.8855\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3203 - accuracy: 0.8824 - val_loss: 0.3049 - val_accuracy: 0.8876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3224 - accuracy: 0.8818 - val_loss: 0.3095 - val_accuracy: 0.8879\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3173 - accuracy: 0.8820 - val_loss: 0.2968 - val_accuracy: 0.8902\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3156 - accuracy: 0.8838 - val_loss: 0.3012 - val_accuracy: 0.8888\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3108 - accuracy: 0.8859 - val_loss: 0.3018 - val_accuracy: 0.8878\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3122 - accuracy: 0.8847 - val_loss: 0.2944 - val_accuracy: 0.8915\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3093 - accuracy: 0.8849 - val_loss: 0.2973 - val_accuracy: 0.8916\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3048 - accuracy: 0.8878 - val_loss: 0.2936 - val_accuracy: 0.8906\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3061 - accuracy: 0.8871 - val_loss: 0.2926 - val_accuracy: 0.8926\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3006 - accuracy: 0.8886 - val_loss: 0.2895 - val_accuracy: 0.8931\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.2990 - accuracy: 0.8886 - val_loss: 0.2877 - val_accuracy: 0.8937\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2983 - accuracy: 0.8883 - val_loss: 0.2896 - val_accuracy: 0.8931\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2978 - accuracy: 0.8897 - val_loss: 0.2878 - val_accuracy: 0.8935\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.2941 - accuracy: 0.8911 - val_loss: 0.2925 - val_accuracy: 0.8906\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2877 - accuracy: 0.8937\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.7765 - accuracy: 0.7244 - val_loss: 0.5686 - val_accuracy: 0.7877\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.5056 - accuracy: 0.8134 - val_loss: 0.4846 - val_accuracy: 0.8228\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4473 - accuracy: 0.8364 - val_loss: 0.4410 - val_accuracy: 0.8389\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4107 - accuracy: 0.8503 - val_loss: 0.4347 - val_accuracy: 0.8416\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3834 - accuracy: 0.8616 - val_loss: 0.3956 - val_accuracy: 0.8586\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3637 - accuracy: 0.8679 - val_loss: 0.3918 - val_accuracy: 0.8606\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3486 - accuracy: 0.8735 - val_loss: 0.3671 - val_accuracy: 0.8676\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3353 - accuracy: 0.8781 - val_loss: 0.3626 - val_accuracy: 0.8707\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3223 - accuracy: 0.8830 - val_loss: 0.3529 - val_accuracy: 0.8733\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3123 - accuracy: 0.8860 - val_loss: 0.3557 - val_accuracy: 0.8736\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3041 - accuracy: 0.8890 - val_loss: 0.3538 - val_accuracy: 0.8708\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2978 - accuracy: 0.8907 - val_loss: 0.3432 - val_accuracy: 0.8763\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2901 - accuracy: 0.8943 - val_loss: 0.3296 - val_accuracy: 0.8840\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2835 - accuracy: 0.8960 - val_loss: 0.3367 - val_accuracy: 0.8806\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2758 - accuracy: 0.8991 - val_loss: 0.3284 - val_accuracy: 0.8804\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.2720 - accuracy: 0.9009 - val_loss: 0.3222 - val_accuracy: 0.8838\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2649 - accuracy: 0.9043 - val_loss: 0.3175 - val_accuracy: 0.8881\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2609 - accuracy: 0.9048 - val_loss: 0.3197 - val_accuracy: 0.8855\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2527 - accuracy: 0.9072 - val_loss: 0.3100 - val_accuracy: 0.8898\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2495 - accuracy: 0.9082 - val_loss: 0.3090 - val_accuracy: 0.8904\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2446 - accuracy: 0.9105 - val_loss: 0.3145 - val_accuracy: 0.8858\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2417 - accuracy: 0.9116 - val_loss: 0.3110 - val_accuracy: 0.8904\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2367 - accuracy: 0.9132 - val_loss: 0.2999 - val_accuracy: 0.8917\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2317 - accuracy: 0.9158 - val_loss: 0.3174 - val_accuracy: 0.8878\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2291 - accuracy: 0.9161 - val_loss: 0.3101 - val_accuracy: 0.8882\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.2242 - accuracy: 0.9176 - val_loss: 0.3073 - val_accuracy: 0.8920\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2999 - accuracy: 0.8917\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.9640 - accuracy: 0.6713 - val_loss: 0.6337 - val_accuracy: 0.7684\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.5716 - accuracy: 0.7912 - val_loss: 0.5598 - val_accuracy: 0.7925\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.5060 - accuracy: 0.8175 - val_loss: 0.5374 - val_accuracy: 0.8020\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4706 - accuracy: 0.8321 - val_loss: 0.4810 - val_accuracy: 0.8253\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4423 - accuracy: 0.8426 - val_loss: 0.4625 - val_accuracy: 0.8349\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4213 - accuracy: 0.8505 - val_loss: 0.4279 - val_accuracy: 0.8508\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4013 - accuracy: 0.8573 - val_loss: 0.4128 - val_accuracy: 0.8555\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3878 - accuracy: 0.8623 - val_loss: 0.4044 - val_accuracy: 0.8575\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3748 - accuracy: 0.8666 - val_loss: 0.3947 - val_accuracy: 0.8609\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3625 - accuracy: 0.8713 - val_loss: 0.3794 - val_accuracy: 0.8677\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3485 - accuracy: 0.8752 - val_loss: 0.3636 - val_accuracy: 0.8705\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3422 - accuracy: 0.8776 - val_loss: 0.3568 - val_accuracy: 0.8753\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3334 - accuracy: 0.8813 - val_loss: 0.3594 - val_accuracy: 0.8725\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3248 - accuracy: 0.8835 - val_loss: 0.3499 - val_accuracy: 0.8763\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3177 - accuracy: 0.8870 - val_loss: 0.3454 - val_accuracy: 0.8771\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3131 - accuracy: 0.8879 - val_loss: 0.3437 - val_accuracy: 0.8792\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3086 - accuracy: 0.8893 - val_loss: 0.3386 - val_accuracy: 0.8808\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 10s 42ms/step - loss: 0.3020 - accuracy: 0.8904 - val_loss: 0.3304 - val_accuracy: 0.8854\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2974 - accuracy: 0.8936 - val_loss: 0.3364 - val_accuracy: 0.8788\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2919 - accuracy: 0.8956 - val_loss: 0.3273 - val_accuracy: 0.8825\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2881 - accuracy: 0.8962 - val_loss: 0.3297 - val_accuracy: 0.8819\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2838 - accuracy: 0.8972 - val_loss: 0.3206 - val_accuracy: 0.8885\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2773 - accuracy: 0.9000 - val_loss: 0.3284 - val_accuracy: 0.8841\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2747 - accuracy: 0.9016 - val_loss: 0.3263 - val_accuracy: 0.8852\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2731 - accuracy: 0.9007 - val_loss: 0.3177 - val_accuracy: 0.8876\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2695 - accuracy: 0.9023 - val_loss: 0.3189 - val_accuracy: 0.8858\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2647 - accuracy: 0.9048 - val_loss: 0.3105 - val_accuracy: 0.8917\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2625 - accuracy: 0.9042 - val_loss: 0.3112 - val_accuracy: 0.8908\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2581 - accuracy: 0.9062 - val_loss: 0.3115 - val_accuracy: 0.8889\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2580 - accuracy: 0.9053 - val_loss: 0.3110 - val_accuracy: 0.8924\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3105 - accuracy: 0.8917\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 1.2066 - accuracy: 0.6279 - val_loss: 0.7185 - val_accuracy: 0.7288\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.6308 - accuracy: 0.7608 - val_loss: 0.5929 - val_accuracy: 0.7771\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.5530 - accuracy: 0.7921 - val_loss: 0.5774 - val_accuracy: 0.7823\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.5076 - accuracy: 0.8133 - val_loss: 0.5176 - val_accuracy: 0.8073\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4743 - accuracy: 0.8281 - val_loss: 0.4799 - val_accuracy: 0.8280\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4549 - accuracy: 0.8348 - val_loss: 0.4606 - val_accuracy: 0.8357\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4336 - accuracy: 0.8452 - val_loss: 0.4668 - val_accuracy: 0.8240\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4216 - accuracy: 0.8494 - val_loss: 0.4335 - val_accuracy: 0.8491\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4039 - accuracy: 0.8572 - val_loss: 0.4162 - val_accuracy: 0.8540\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3927 - accuracy: 0.8615 - val_loss: 0.4053 - val_accuracy: 0.8560\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3848 - accuracy: 0.8629 - val_loss: 0.3980 - val_accuracy: 0.8599\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3765 - accuracy: 0.8657 - val_loss: 0.3954 - val_accuracy: 0.8606\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3682 - accuracy: 0.8678 - val_loss: 0.3910 - val_accuracy: 0.8611\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3613 - accuracy: 0.8710 - val_loss: 0.3771 - val_accuracy: 0.8658\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3546 - accuracy: 0.8729 - val_loss: 0.3868 - val_accuracy: 0.8599\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3477 - accuracy: 0.8748 - val_loss: 0.3715 - val_accuracy: 0.8649\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3432 - accuracy: 0.8771 - val_loss: 0.3747 - val_accuracy: 0.8666\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3371 - accuracy: 0.8781 - val_loss: 0.3684 - val_accuracy: 0.8712\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3335 - accuracy: 0.8805 - val_loss: 0.3650 - val_accuracy: 0.8708\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3287 - accuracy: 0.8809 - val_loss: 0.3738 - val_accuracy: 0.8662\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3262 - accuracy: 0.8815 - val_loss: 0.3561 - val_accuracy: 0.8712\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3219 - accuracy: 0.8836 - val_loss: 0.3548 - val_accuracy: 0.8731\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3166 - accuracy: 0.8848 - val_loss: 0.3437 - val_accuracy: 0.8753\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3146 - accuracy: 0.8868 - val_loss: 0.3537 - val_accuracy: 0.8719\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3102 - accuracy: 0.8864 - val_loss: 0.3436 - val_accuracy: 0.8760\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3082 - accuracy: 0.8887 - val_loss: 0.3493 - val_accuracy: 0.8740\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3022 - accuracy: 0.8899 - val_loss: 0.3373 - val_accuracy: 0.8788\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2984 - accuracy: 0.8919 - val_loss: 0.3407 - val_accuracy: 0.8802\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2975 - accuracy: 0.8914 - val_loss: 0.3414 - val_accuracy: 0.8774\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2937 - accuracy: 0.8929 - val_loss: 0.3345 - val_accuracy: 0.8809\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2896 - accuracy: 0.8948 - val_loss: 0.3302 - val_accuracy: 0.8847\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.2896 - accuracy: 0.8946 - val_loss: 0.3420 - val_accuracy: 0.8779\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2878 - accuracy: 0.8958 - val_loss: 0.3296 - val_accuracy: 0.8809\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2835 - accuracy: 0.8970 - val_loss: 0.3333 - val_accuracy: 0.8794\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2802 - accuracy: 0.8974 - val_loss: 0.3258 - val_accuracy: 0.8841\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.2803 - accuracy: 0.8987 - val_loss: 0.3502 - val_accuracy: 0.8716\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2769 - accuracy: 0.8992 - val_loss: 0.3239 - val_accuracy: 0.8856\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.2722 - accuracy: 0.9010 - val_loss: 0.3258 - val_accuracy: 0.8840\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2733 - accuracy: 0.9004 - val_loss: 0.3247 - val_accuracy: 0.8850\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2700 - accuracy: 0.9010 - val_loss: 0.3319 - val_accuracy: 0.8793\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3239 - accuracy: 0.8856\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.9297 - accuracy: 0.6570 - val_loss: 0.5692 - val_accuracy: 0.7810\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 11s 19ms/step - loss: 0.5995 - accuracy: 0.7737 - val_loss: 0.4946 - val_accuracy: 0.8134\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.5313 - accuracy: 0.8032 - val_loss: 0.4459 - val_accuracy: 0.8384\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4892 - accuracy: 0.8198 - val_loss: 0.4135 - val_accuracy: 0.8484\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4555 - accuracy: 0.8334 - val_loss: 0.3899 - val_accuracy: 0.8581\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4333 - accuracy: 0.8408 - val_loss: 0.3693 - val_accuracy: 0.8615\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4162 - accuracy: 0.8487 - val_loss: 0.3652 - val_accuracy: 0.8646\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4008 - accuracy: 0.8534 - val_loss: 0.3552 - val_accuracy: 0.8724\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3894 - accuracy: 0.8562 - val_loss: 0.3433 - val_accuracy: 0.8741\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3781 - accuracy: 0.8595 - val_loss: 0.3386 - val_accuracy: 0.8742\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3667 - accuracy: 0.8649 - val_loss: 0.3294 - val_accuracy: 0.8780\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3608 - accuracy: 0.8671 - val_loss: 0.3278 - val_accuracy: 0.8821\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3528 - accuracy: 0.8700 - val_loss: 0.3251 - val_accuracy: 0.8814\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3451 - accuracy: 0.8727 - val_loss: 0.3220 - val_accuracy: 0.8814\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.3401 - accuracy: 0.8742 - val_loss: 0.3093 - val_accuracy: 0.8882\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.3352 - accuracy: 0.8759 - val_loss: 0.3083 - val_accuracy: 0.8874\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3306 - accuracy: 0.8774 - val_loss: 0.3077 - val_accuracy: 0.8888\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3271 - accuracy: 0.8793 - val_loss: 0.3046 - val_accuracy: 0.8884\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3245 - accuracy: 0.8806 - val_loss: 0.3016 - val_accuracy: 0.8910\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3212 - accuracy: 0.8808 - val_loss: 0.2987 - val_accuracy: 0.8918\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3163 - accuracy: 0.8830 - val_loss: 0.2960 - val_accuracy: 0.8937\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3150 - accuracy: 0.8840 - val_loss: 0.3000 - val_accuracy: 0.8920\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3105 - accuracy: 0.8859 - val_loss: 0.2898 - val_accuracy: 0.8958\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3089 - accuracy: 0.8855 - val_loss: 0.2918 - val_accuracy: 0.8941\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3063 - accuracy: 0.8866 - val_loss: 0.2886 - val_accuracy: 0.8960\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3029 - accuracy: 0.8888 - val_loss: 0.2846 - val_accuracy: 0.8984\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2999 - accuracy: 0.8896 - val_loss: 0.2908 - val_accuracy: 0.8953\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2960 - accuracy: 0.8912 - val_loss: 0.2828 - val_accuracy: 0.8969\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.2930 - accuracy: 0.8921 - val_loss: 0.2829 - val_accuracy: 0.8978\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2976 - accuracy: 0.8905 - val_loss: 0.2840 - val_accuracy: 0.8965\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2906 - accuracy: 0.8924 - val_loss: 0.2821 - val_accuracy: 0.8977\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2889 - accuracy: 0.8931 - val_loss: 0.2845 - val_accuracy: 0.8957\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2863 - accuracy: 0.8941 - val_loss: 0.2813 - val_accuracy: 0.8981\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2860 - accuracy: 0.8945 - val_loss: 0.2761 - val_accuracy: 0.9003\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.2848 - accuracy: 0.8956 - val_loss: 0.2836 - val_accuracy: 0.8962\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2813 - accuracy: 0.8953 - val_loss: 0.2799 - val_accuracy: 0.8992\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2820 - accuracy: 0.8961 - val_loss: 0.2743 - val_accuracy: 0.9002\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2777 - accuracy: 0.8967 - val_loss: 0.2752 - val_accuracy: 0.9010\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2778 - accuracy: 0.8971 - val_loss: 0.2907 - val_accuracy: 0.8921\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.2761 - accuracy: 0.8983 - val_loss: 0.2744 - val_accuracy: 0.9008\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2743 - accuracy: 0.9002\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 1.1811 - accuracy: 0.5705 - val_loss: 0.6315 - val_accuracy: 0.7596\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.6625 - accuracy: 0.7523 - val_loss: 0.5397 - val_accuracy: 0.7936\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.5828 - accuracy: 0.7825 - val_loss: 0.4890 - val_accuracy: 0.8178\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.5342 - accuracy: 0.8033 - val_loss: 0.4606 - val_accuracy: 0.8260\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4999 - accuracy: 0.8175 - val_loss: 0.4317 - val_accuracy: 0.8382\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4757 - accuracy: 0.8271 - val_loss: 0.4104 - val_accuracy: 0.8454\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4532 - accuracy: 0.8349 - val_loss: 0.3988 - val_accuracy: 0.8515\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4371 - accuracy: 0.8416 - val_loss: 0.3812 - val_accuracy: 0.8605\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4240 - accuracy: 0.8466 - val_loss: 0.3723 - val_accuracy: 0.8625\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4106 - accuracy: 0.8512 - val_loss: 0.3648 - val_accuracy: 0.8641\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3998 - accuracy: 0.8565 - val_loss: 0.3595 - val_accuracy: 0.8669\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3929 - accuracy: 0.8563 - val_loss: 0.3481 - val_accuracy: 0.8710\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3819 - accuracy: 0.8605 - val_loss: 0.3435 - val_accuracy: 0.8733\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3788 - accuracy: 0.8611 - val_loss: 0.3397 - val_accuracy: 0.8754\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3719 - accuracy: 0.8647 - val_loss: 0.3313 - val_accuracy: 0.8764\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3625 - accuracy: 0.8672 - val_loss: 0.3280 - val_accuracy: 0.8768\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3595 - accuracy: 0.8689 - val_loss: 0.3253 - val_accuracy: 0.8807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3512 - accuracy: 0.8707 - val_loss: 0.3227 - val_accuracy: 0.8823\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3480 - accuracy: 0.8718 - val_loss: 0.3183 - val_accuracy: 0.8816\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3429 - accuracy: 0.8736 - val_loss: 0.3154 - val_accuracy: 0.8829\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3383 - accuracy: 0.8767 - val_loss: 0.3141 - val_accuracy: 0.8842\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3347 - accuracy: 0.8749 - val_loss: 0.3116 - val_accuracy: 0.8834\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3328 - accuracy: 0.8787 - val_loss: 0.3089 - val_accuracy: 0.8857\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3308 - accuracy: 0.8777 - val_loss: 0.3056 - val_accuracy: 0.8899\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3272 - accuracy: 0.8805 - val_loss: 0.3012 - val_accuracy: 0.8877\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3236 - accuracy: 0.8813 - val_loss: 0.3009 - val_accuracy: 0.8883\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3171 - accuracy: 0.8830 - val_loss: 0.2991 - val_accuracy: 0.8891\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3140 - accuracy: 0.8855 - val_loss: 0.2996 - val_accuracy: 0.8907\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3157 - accuracy: 0.8843 - val_loss: 0.2973 - val_accuracy: 0.8905\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3128 - accuracy: 0.8839 - val_loss: 0.2947 - val_accuracy: 0.8920\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3118 - accuracy: 0.8863 - val_loss: 0.2932 - val_accuracy: 0.8908\n",
      "Epoch 32/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3073 - accuracy: 0.8864 - val_loss: 0.2951 - val_accuracy: 0.8927\n",
      "Epoch 33/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3088 - accuracy: 0.8860 - val_loss: 0.2913 - val_accuracy: 0.8926\n",
      "Epoch 34/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3042 - accuracy: 0.8878 - val_loss: 0.2883 - val_accuracy: 0.8943\n",
      "Epoch 35/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3038 - accuracy: 0.8871 - val_loss: 0.2859 - val_accuracy: 0.8938\n",
      "Epoch 36/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3027 - accuracy: 0.8874 - val_loss: 0.2884 - val_accuracy: 0.8932\n",
      "Epoch 37/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2956 - accuracy: 0.8914 - val_loss: 0.2852 - val_accuracy: 0.8955\n",
      "Epoch 38/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2955 - accuracy: 0.8902 - val_loss: 0.2838 - val_accuracy: 0.8934\n",
      "Epoch 39/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2918 - accuracy: 0.8925 - val_loss: 0.2837 - val_accuracy: 0.8970\n",
      "Epoch 40/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2911 - accuracy: 0.8918 - val_loss: 0.2870 - val_accuracy: 0.8926\n",
      "Epoch 41/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2922 - accuracy: 0.8929 - val_loss: 0.2813 - val_accuracy: 0.8965\n",
      "Epoch 42/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2867 - accuracy: 0.8946 - val_loss: 0.2816 - val_accuracy: 0.8957\n",
      "Epoch 43/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2904 - accuracy: 0.8919 - val_loss: 0.2799 - val_accuracy: 0.8975\n",
      "Epoch 44/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2893 - accuracy: 0.8927 - val_loss: 0.2800 - val_accuracy: 0.8962\n",
      "Epoch 45/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2870 - accuracy: 0.8942 - val_loss: 0.2822 - val_accuracy: 0.8975\n",
      "Epoch 46/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2854 - accuracy: 0.8950 - val_loss: 0.2776 - val_accuracy: 0.8973\n",
      "Epoch 47/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2820 - accuracy: 0.8953 - val_loss: 0.2759 - val_accuracy: 0.8987\n",
      "Epoch 48/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2827 - accuracy: 0.8954 - val_loss: 0.2771 - val_accuracy: 0.8967\n",
      "Epoch 49/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2775 - accuracy: 0.8974 - val_loss: 0.2771 - val_accuracy: 0.8998\n",
      "Epoch 50/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2793 - accuracy: 0.8966 - val_loss: 0.2748 - val_accuracy: 0.8995\n",
      "Epoch 51/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2768 - accuracy: 0.8968 - val_loss: 0.2780 - val_accuracy: 0.8977\n",
      "Epoch 52/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2776 - accuracy: 0.8954 - val_loss: 0.2758 - val_accuracy: 0.8986\n",
      "Epoch 53/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2745 - accuracy: 0.8981 - val_loss: 0.2770 - val_accuracy: 0.8979\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2748 - accuracy: 0.8995\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 1.4685 - accuracy: 0.4763 - val_loss: 0.7708 - val_accuracy: 0.7299\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.8078 - accuracy: 0.7019 - val_loss: 0.6283 - val_accuracy: 0.7646\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.6902 - accuracy: 0.7419 - val_loss: 0.5682 - val_accuracy: 0.7833\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.6308 - accuracy: 0.7657 - val_loss: 0.5334 - val_accuracy: 0.7993\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.5899 - accuracy: 0.7808 - val_loss: 0.5008 - val_accuracy: 0.8171\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.5577 - accuracy: 0.7961 - val_loss: 0.4792 - val_accuracy: 0.8257\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.5295 - accuracy: 0.8062 - val_loss: 0.4593 - val_accuracy: 0.8327\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.5072 - accuracy: 0.8159 - val_loss: 0.4438 - val_accuracy: 0.8391\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 9s 78ms/step - loss: 0.4905 - accuracy: 0.8223 - val_loss: 0.4333 - val_accuracy: 0.8439\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4772 - accuracy: 0.8269 - val_loss: 0.4171 - val_accuracy: 0.8513\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4631 - accuracy: 0.8309 - val_loss: 0.4055 - val_accuracy: 0.8535\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4546 - accuracy: 0.8340 - val_loss: 0.3982 - val_accuracy: 0.8557\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4423 - accuracy: 0.8400 - val_loss: 0.3894 - val_accuracy: 0.8575\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.4350 - accuracy: 0.8407 - val_loss: 0.3844 - val_accuracy: 0.8607\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4264 - accuracy: 0.8446 - val_loss: 0.3769 - val_accuracy: 0.8632\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4170 - accuracy: 0.8490 - val_loss: 0.3690 - val_accuracy: 0.8641\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4091 - accuracy: 0.8527 - val_loss: 0.3651 - val_accuracy: 0.8677\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.4040 - accuracy: 0.8526 - val_loss: 0.3635 - val_accuracy: 0.8676\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4000 - accuracy: 0.8553 - val_loss: 0.3573 - val_accuracy: 0.8696\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3947 - accuracy: 0.8573 - val_loss: 0.3528 - val_accuracy: 0.8702\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3888 - accuracy: 0.8595 - val_loss: 0.3496 - val_accuracy: 0.8718\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3865 - accuracy: 0.8592 - val_loss: 0.3469 - val_accuracy: 0.8725\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3780 - accuracy: 0.8625 - val_loss: 0.3444 - val_accuracy: 0.8755\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3738 - accuracy: 0.8643 - val_loss: 0.3410 - val_accuracy: 0.8741\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3745 - accuracy: 0.8642 - val_loss: 0.3383 - val_accuracy: 0.8720\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3662 - accuracy: 0.8678 - val_loss: 0.3353 - val_accuracy: 0.8761\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3627 - accuracy: 0.8676 - val_loss: 0.3343 - val_accuracy: 0.8767\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3593 - accuracy: 0.8696 - val_loss: 0.3347 - val_accuracy: 0.8747\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3566 - accuracy: 0.8696 - val_loss: 0.3288 - val_accuracy: 0.8779\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3515 - accuracy: 0.8719 - val_loss: 0.3254 - val_accuracy: 0.8788\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3523 - accuracy: 0.8720 - val_loss: 0.3254 - val_accuracy: 0.8785\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3456 - accuracy: 0.8733 - val_loss: 0.3209 - val_accuracy: 0.8802\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3443 - accuracy: 0.8759 - val_loss: 0.3210 - val_accuracy: 0.8791\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3446 - accuracy: 0.8740 - val_loss: 0.3204 - val_accuracy: 0.8803\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3422 - accuracy: 0.8754 - val_loss: 0.3147 - val_accuracy: 0.8814\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3395 - accuracy: 0.8760 - val_loss: 0.3157 - val_accuracy: 0.8816\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3334 - accuracy: 0.8776 - val_loss: 0.3194 - val_accuracy: 0.8808\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3333 - accuracy: 0.8778 - val_loss: 0.3095 - val_accuracy: 0.8835\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3321 - accuracy: 0.8780 - val_loss: 0.3086 - val_accuracy: 0.8863\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3298 - accuracy: 0.8784 - val_loss: 0.3114 - val_accuracy: 0.8823\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3287 - accuracy: 0.8804 - val_loss: 0.3070 - val_accuracy: 0.8856\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3238 - accuracy: 0.8816 - val_loss: 0.3071 - val_accuracy: 0.8865\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3229 - accuracy: 0.8825 - val_loss: 0.3046 - val_accuracy: 0.8851\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3200 - accuracy: 0.8832 - val_loss: 0.3050 - val_accuracy: 0.8839\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3184 - accuracy: 0.8817 - val_loss: 0.3016 - val_accuracy: 0.8883\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3206 - accuracy: 0.8820 - val_loss: 0.3002 - val_accuracy: 0.8891\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3143 - accuracy: 0.8843 - val_loss: 0.2983 - val_accuracy: 0.8897\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3144 - accuracy: 0.8836 - val_loss: 0.2996 - val_accuracy: 0.8897\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3127 - accuracy: 0.8855 - val_loss: 0.3011 - val_accuracy: 0.8866\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3132 - accuracy: 0.8858 - val_loss: 0.2977 - val_accuracy: 0.8880\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 10s 82ms/step - loss: 0.3099 - accuracy: 0.8874 - val_loss: 0.2972 - val_accuracy: 0.8879\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3099 - accuracy: 0.8859 - val_loss: 0.2975 - val_accuracy: 0.8894\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3070 - accuracy: 0.8883 - val_loss: 0.2930 - val_accuracy: 0.8919\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3065 - accuracy: 0.8887 - val_loss: 0.2929 - val_accuracy: 0.8914\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3034 - accuracy: 0.8885 - val_loss: 0.2927 - val_accuracy: 0.8915\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3044 - accuracy: 0.8896 - val_loss: 0.2944 - val_accuracy: 0.8928\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3006 - accuracy: 0.8904 - val_loss: 0.2902 - val_accuracy: 0.8926\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3037 - accuracy: 0.8890 - val_loss: 0.2898 - val_accuracy: 0.8930\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2996 - accuracy: 0.8893 - val_loss: 0.2907 - val_accuracy: 0.8925\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.2973 - accuracy: 0.8911 - val_loss: 0.2943 - val_accuracy: 0.8881\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2966 - accuracy: 0.8903 - val_loss: 0.2922 - val_accuracy: 0.8904\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2898 - accuracy: 0.8930\n"
     ]
    }
   ],
   "source": [
    "all_models = []\n",
    "for params in full_params:\n",
    "    model = build_and_fit_model(**params)\n",
    "    all_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.sequential.Sequential at 0x217878c2e10>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x21787e7dfd0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217ac899d30>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b3f49668>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217ac8ceb00>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b020fc50>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b016e7b8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b022a5f8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b24e1a90>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b1017f98>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b0f3a828>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b1bb4c18>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b10d8828>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b6be1b38>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b73e5978>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b24e1630>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b6c3aeb8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b553ad68>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b586bbe0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b5823748>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b5711908>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b5bc5f98>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b5d4e5c0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b5e2d8d0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b551c828>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b526d1d0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217be1c6160>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217c31c3160>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217c394b710>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217c52e1208>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217c538c9b0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217c9e1c8d0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217ca070b00>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b0061a58>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217ba3f58d0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217ac82c400>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217c9f81c18>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cbd37ba8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cbd5bbe0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cc37fcf8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cc3f8748>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cc59bb70>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217d0962828>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217d0bcf048>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217d0cffeb8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217d0f4eb00>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cc3f1898>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cc3cdf60>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1687 - accuracy: 0.9378\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2667 - accuracy: 0.9019\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1594 - accuracy: 0.9431\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2670 - accuracy: 0.9047\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1418 - accuracy: 0.9484\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2570 - accuracy: 0.9109\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1506 - accuracy: 0.9444\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2529 - accuracy: 0.9095\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1322 - accuracy: 0.9526\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2479 - accuracy: 0.9127\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1858 - accuracy: 0.9309\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2569 - accuracy: 0.9057\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1466 - accuracy: 0.9472\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2484 - accuracy: 0.9115\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1709 - accuracy: 0.9378\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2614 - accuracy: 0.9070\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2221 - accuracy: 0.9217\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2867 - accuracy: 0.8971\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1266 - accuracy: 0.9560\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2458 - accuracy: 0.9115\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1421 - accuracy: 0.9477\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2430 - accuracy: 0.9117\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1500 - accuracy: 0.9464\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2451 - accuracy: 0.9093\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2359 - accuracy: 0.9117\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2962 - accuracy: 0.8929\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2207 - accuracy: 0.9183\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2996 - accuracy: 0.8897\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2724 - accuracy: 0.9009\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3194 - accuracy: 0.8871\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2119 - accuracy: 0.9208\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2746 - accuracy: 0.8983\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1931 - accuracy: 0.9286\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2738 - accuracy: 0.9015\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2157 - accuracy: 0.9204\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2724 - accuracy: 0.8993\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2364 - accuracy: 0.9131\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3003 - accuracy: 0.8944\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2945 - accuracy: 0.8953\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3365 - accuracy: 0.8777\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2676 - accuracy: 0.9042\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3268 - accuracy: 0.8819\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1967 - accuracy: 0.9282\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2595 - accuracy: 0.9055\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2064 - accuracy: 0.9245\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2693 - accuracy: 0.9034\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2327 - accuracy: 0.9153\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2840 - accuracy: 0.8946\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1859 - accuracy: 0.9333\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2704 - accuracy: 0.9022\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1780 - accuracy: 0.9342\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2831 - accuracy: 0.8967\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.2111 - accuracy: 0.9234\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2837 - accuracy: 0.8959\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1944 - accuracy: 0.9270\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2757 - accuracy: 0.9016\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1609 - accuracy: 0.9413\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2657 - accuracy: 0.9080\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1566 - accuracy: 0.9422\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2569 - accuracy: 0.9093\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1547 - accuracy: 0.9444\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2722 - accuracy: 0.9042\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1818 - accuracy: 0.9347\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2796 - accuracy: 0.9016\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.2049 - accuracy: 0.9260\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2858 - accuracy: 0.8978\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1445 - accuracy: 0.9480\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2514 - accuracy: 0.9112\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1800 - accuracy: 0.9335\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2555 - accuracy: 0.9072\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1524 - accuracy: 0.9449\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2475 - accuracy: 0.9113\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2261 - accuracy: 0.9175\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3020 - accuracy: 0.8912\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2133 - accuracy: 0.9208\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3014 - accuracy: 0.8938\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2589 - accuracy: 0.9059\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3162 - accuracy: 0.8885\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2478 - accuracy: 0.9069\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2995 - accuracy: 0.8889\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2169 - accuracy: 0.9200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2744 - accuracy: 0.9024\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2335 - accuracy: 0.9122\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2877 - accuracy: 0.8937\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2201 - accuracy: 0.9203\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2999 - accuracy: 0.8917\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2546 - accuracy: 0.9083\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3105 - accuracy: 0.8917\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2693 - accuracy: 0.9015\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3239 - accuracy: 0.8856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2132 - accuracy: 0.9208\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2743 - accuracy: 0.9002\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2129 - accuracy: 0.9204\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2748 - accuracy: 0.8995\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2368 - accuracy: 0.9128\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2898 - accuracy: 0.8930\n"
     ]
    }
   ],
   "source": [
    "all_model_results = []\n",
    "for cur_model in all_models:\n",
    "    cur_model_results = {}\n",
    "    cur_model_results['model'] = cur_model\n",
    "    cur_model_results['train_eval'] = cur_model.evaluate(train_images, train_labels)\n",
    "    cur_model_results['test_eval'] = cur_model.evaluate(test_images, test_labels)\n",
    "    all_model_results.append(cur_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_df = pd.DataFrame(all_model_results)\n",
    "all_df[['train_loss','train_acc']] = pd.DataFrame(all_df.train_eval.tolist(), index= all_df.index)\n",
    "all_df[['test_loss','test_acc']] = pd.DataFrame(all_df.test_eval.tolist(), index= all_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_eval</th>\n",
       "      <th>test_eval</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.16868141293525696, 0.9378499984741211]</td>\n",
       "      <td>[0.26669758558273315, 0.9018999934196472]</td>\n",
       "      <td>0.168681</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.266698</td>\n",
       "      <td>0.9019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15938624739646912, 0.9430500268936157]</td>\n",
       "      <td>[0.2670109272003174, 0.904699981212616]</td>\n",
       "      <td>0.159386</td>\n",
       "      <td>0.943050</td>\n",
       "      <td>0.267011</td>\n",
       "      <td>0.9047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1417781561613083, 0.948366641998291]</td>\n",
       "      <td>[0.25698399543762207, 0.9108999967575073]</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>0.948367</td>\n",
       "      <td>0.256984</td>\n",
       "      <td>0.9109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15062175691127777, 0.9443666934967041]</td>\n",
       "      <td>[0.252878874540329, 0.909500002861023]</td>\n",
       "      <td>0.150622</td>\n",
       "      <td>0.944367</td>\n",
       "      <td>0.252879</td>\n",
       "      <td>0.9095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.13222971558570862, 0.9526333212852478]</td>\n",
       "      <td>[0.24787791073322296, 0.9126999974250793]</td>\n",
       "      <td>0.132230</td>\n",
       "      <td>0.952633</td>\n",
       "      <td>0.247878</td>\n",
       "      <td>0.9127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1857813447713852, 0.930899977684021]</td>\n",
       "      <td>[0.25688549876213074, 0.9057000279426575]</td>\n",
       "      <td>0.185781</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>0.256885</td>\n",
       "      <td>0.9057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1465650051832199, 0.9471666812896729]</td>\n",
       "      <td>[0.24840134382247925, 0.9114999771118164]</td>\n",
       "      <td>0.146565</td>\n",
       "      <td>0.947167</td>\n",
       "      <td>0.248401</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1708768606185913, 0.9378499984741211]</td>\n",
       "      <td>[0.2613731920719147, 0.9070000052452087]</td>\n",
       "      <td>0.170877</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.261373</td>\n",
       "      <td>0.9070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22205980122089386, 0.92166668176651]</td>\n",
       "      <td>[0.28667232394218445, 0.8970999717712402]</td>\n",
       "      <td>0.222060</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>0.286672</td>\n",
       "      <td>0.8971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.12658622860908508, 0.9560166597366333]</td>\n",
       "      <td>[0.24576333165168762, 0.9114999771118164]</td>\n",
       "      <td>0.126586</td>\n",
       "      <td>0.956017</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1421215534210205, 0.9477333426475525]</td>\n",
       "      <td>[0.24299895763397217, 0.9117000102996826]</td>\n",
       "      <td>0.142122</td>\n",
       "      <td>0.947733</td>\n",
       "      <td>0.242999</td>\n",
       "      <td>0.9117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14996056258678436, 0.9464333057403564]</td>\n",
       "      <td>[0.24511033296585083, 0.9093000292778015]</td>\n",
       "      <td>0.149961</td>\n",
       "      <td>0.946433</td>\n",
       "      <td>0.245110</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23593302071094513, 0.9116500020027161]</td>\n",
       "      <td>[0.29616788029670715, 0.8928999900817871]</td>\n",
       "      <td>0.235933</td>\n",
       "      <td>0.911650</td>\n",
       "      <td>0.296168</td>\n",
       "      <td>0.8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22073142230510712, 0.9183333516120911]</td>\n",
       "      <td>[0.2996402978897095, 0.8896999955177307]</td>\n",
       "      <td>0.220731</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>0.299640</td>\n",
       "      <td>0.8897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.27241677045822144, 0.9009333252906799]</td>\n",
       "      <td>[0.31941139698028564, 0.8870999813079834]</td>\n",
       "      <td>0.272417</td>\n",
       "      <td>0.900933</td>\n",
       "      <td>0.319411</td>\n",
       "      <td>0.8871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21186839044094086, 0.9207500219345093]</td>\n",
       "      <td>[0.2745698392391205, 0.8982999920845032]</td>\n",
       "      <td>0.211868</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274570</td>\n",
       "      <td>0.8983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19306862354278564, 0.9285666942596436]</td>\n",
       "      <td>[0.2737598717212677, 0.9014999866485596]</td>\n",
       "      <td>0.193069</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.273760</td>\n",
       "      <td>0.9015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21572570502758026, 0.9204333424568176]</td>\n",
       "      <td>[0.2723506689071655, 0.8992999792098999]</td>\n",
       "      <td>0.215726</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.272351</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23643141984939575, 0.9130666851997375]</td>\n",
       "      <td>[0.30026862025260925, 0.8944000005722046]</td>\n",
       "      <td>0.236431</td>\n",
       "      <td>0.913067</td>\n",
       "      <td>0.300269</td>\n",
       "      <td>0.8944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.29450979828834534, 0.8953333497047424]</td>\n",
       "      <td>[0.3364871144294739, 0.8776999711990356]</td>\n",
       "      <td>0.294510</td>\n",
       "      <td>0.895333</td>\n",
       "      <td>0.336487</td>\n",
       "      <td>0.8777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2675584852695465, 0.9041666388511658]</td>\n",
       "      <td>[0.3267644941806793, 0.8819000124931335]</td>\n",
       "      <td>0.267558</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>0.326764</td>\n",
       "      <td>0.8819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19672514498233795, 0.9282000064849854]</td>\n",
       "      <td>[0.25953343510627747, 0.9054999947547913]</td>\n",
       "      <td>0.196725</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.259533</td>\n",
       "      <td>0.9055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20643432438373566, 0.9244666695594788]</td>\n",
       "      <td>[0.2693329453468323, 0.9034000039100647]</td>\n",
       "      <td>0.206434</td>\n",
       "      <td>0.924467</td>\n",
       "      <td>0.269333</td>\n",
       "      <td>0.9034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23273038864135742, 0.9152833223342896]</td>\n",
       "      <td>[0.2840319871902466, 0.894599974155426]</td>\n",
       "      <td>0.232730</td>\n",
       "      <td>0.915283</td>\n",
       "      <td>0.284032</td>\n",
       "      <td>0.8946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1859435737133026, 0.9332666397094727]</td>\n",
       "      <td>[0.2703552842140198, 0.9021999835968018]</td>\n",
       "      <td>0.185944</td>\n",
       "      <td>0.933267</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.9022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.17796066403388977, 0.9342333078384399]</td>\n",
       "      <td>[0.28307250142097473, 0.8967000246047974]</td>\n",
       "      <td>0.177961</td>\n",
       "      <td>0.934233</td>\n",
       "      <td>0.283073</td>\n",
       "      <td>0.8967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21107719838619232, 0.9233666658401489]</td>\n",
       "      <td>[0.28365108370780945, 0.8959000110626221]</td>\n",
       "      <td>0.211077</td>\n",
       "      <td>0.923367</td>\n",
       "      <td>0.283651</td>\n",
       "      <td>0.8959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19442103803157806, 0.9269833564758301]</td>\n",
       "      <td>[0.27573519945144653, 0.9016000032424927]</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.926983</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1608797013759613, 0.9412999749183655]</td>\n",
       "      <td>[0.26573246717453003, 0.9079999923706055]</td>\n",
       "      <td>0.160880</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.265732</td>\n",
       "      <td>0.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15656131505966187, 0.9422333240509033]</td>\n",
       "      <td>[0.25688880681991577, 0.9093000292778015]</td>\n",
       "      <td>0.156561</td>\n",
       "      <td>0.942233</td>\n",
       "      <td>0.256889</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1546628773212433, 0.9443833231925964]</td>\n",
       "      <td>[0.27218130230903625, 0.90420001745224]</td>\n",
       "      <td>0.154663</td>\n",
       "      <td>0.944383</td>\n",
       "      <td>0.272181</td>\n",
       "      <td>0.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18178533017635345, 0.9347333312034607]</td>\n",
       "      <td>[0.2796153426170349, 0.9016000032424927]</td>\n",
       "      <td>0.181785</td>\n",
       "      <td>0.934733</td>\n",
       "      <td>0.279615</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20485873520374298, 0.9259999990463257]</td>\n",
       "      <td>[0.285814106464386, 0.8978000283241272]</td>\n",
       "      <td>0.204859</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.285814</td>\n",
       "      <td>0.8978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14446735382080078, 0.948033332824707]</td>\n",
       "      <td>[0.25142455101013184, 0.9111999869346619]</td>\n",
       "      <td>0.144467</td>\n",
       "      <td>0.948033</td>\n",
       "      <td>0.251425</td>\n",
       "      <td>0.9112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18002688884735107, 0.9334666728973389]</td>\n",
       "      <td>[0.2554672062397003, 0.9071999788284302]</td>\n",
       "      <td>0.180027</td>\n",
       "      <td>0.933467</td>\n",
       "      <td>0.255467</td>\n",
       "      <td>0.9072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15237271785736084, 0.9448999762535095]</td>\n",
       "      <td>[0.24747571349143982, 0.911300003528595]</td>\n",
       "      <td>0.152373</td>\n",
       "      <td>0.944900</td>\n",
       "      <td>0.247476</td>\n",
       "      <td>0.9113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22614991664886475, 0.9175000190734863]</td>\n",
       "      <td>[0.3019619286060333, 0.8912000060081482]</td>\n",
       "      <td>0.226150</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.301962</td>\n",
       "      <td>0.8912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21326030790805817, 0.9208333492279053]</td>\n",
       "      <td>[0.30138877034187317, 0.8938000202178955]</td>\n",
       "      <td>0.213260</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.301389</td>\n",
       "      <td>0.8938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2588706910610199, 0.9059000015258789]</td>\n",
       "      <td>[0.3161528706550598, 0.8884999752044678]</td>\n",
       "      <td>0.258871</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.316153</td>\n",
       "      <td>0.8885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.24779362976551056, 0.9068999886512756]</td>\n",
       "      <td>[0.2994548976421356, 0.8888999819755554]</td>\n",
       "      <td>0.247794</td>\n",
       "      <td>0.906900</td>\n",
       "      <td>0.299455</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21686747670173645, 0.9199833273887634]</td>\n",
       "      <td>[0.2744320034980774, 0.902400016784668]</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.919983</td>\n",
       "      <td>0.274432</td>\n",
       "      <td>0.9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23349125683307648, 0.9122499823570251]</td>\n",
       "      <td>[0.287698894739151, 0.8937000036239624]</td>\n",
       "      <td>0.233491</td>\n",
       "      <td>0.912250</td>\n",
       "      <td>0.287699</td>\n",
       "      <td>0.8937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22014310956001282, 0.9203000068664551]</td>\n",
       "      <td>[0.2999141812324524, 0.891700029373169]</td>\n",
       "      <td>0.220143</td>\n",
       "      <td>0.920300</td>\n",
       "      <td>0.299914</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.25457531213760376, 0.90829998254776]</td>\n",
       "      <td>[0.3104911148548126, 0.891700029373169]</td>\n",
       "      <td>0.254575</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>0.310491</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2692805528640747, 0.9014833569526672]</td>\n",
       "      <td>[0.32393553853034973, 0.8855999708175659]</td>\n",
       "      <td>0.269281</td>\n",
       "      <td>0.901483</td>\n",
       "      <td>0.323936</td>\n",
       "      <td>0.8856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2132299542427063, 0.9207500219345093]</td>\n",
       "      <td>[0.27428877353668213, 0.9002000093460083]</td>\n",
       "      <td>0.213230</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274289</td>\n",
       "      <td>0.9002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21291086077690125, 0.9204333424568176]</td>\n",
       "      <td>[0.27476274967193604, 0.8995000123977661]</td>\n",
       "      <td>0.212911</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.274763</td>\n",
       "      <td>0.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23679600656032562, 0.9127500057220459]</td>\n",
       "      <td>[0.28981828689575195, 0.8930000066757202]</td>\n",
       "      <td>0.236796</td>\n",
       "      <td>0.912750</td>\n",
       "      <td>0.289818</td>\n",
       "      <td>0.8930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  \\\n",
       "0   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "1   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "2   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "3   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "4   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "5   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "6   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "7   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "8   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "9   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "10  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "11  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "12  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "13  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "14  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "15  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "16  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "17  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "18  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "19  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "20  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "21  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "22  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "23  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "24  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "25  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "26  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "27  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "28  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "29  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "30  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "31  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "32  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "33  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "34  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "35  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "36  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "37  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "38  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "39  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "40  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "41  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "42  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "43  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "44  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "45  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "46  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "47  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "\n",
       "                                   train_eval  \\\n",
       "0   [0.16868141293525696, 0.9378499984741211]   \n",
       "1   [0.15938624739646912, 0.9430500268936157]   \n",
       "2     [0.1417781561613083, 0.948366641998291]   \n",
       "3   [0.15062175691127777, 0.9443666934967041]   \n",
       "4   [0.13222971558570862, 0.9526333212852478]   \n",
       "5     [0.1857813447713852, 0.930899977684021]   \n",
       "6    [0.1465650051832199, 0.9471666812896729]   \n",
       "7    [0.1708768606185913, 0.9378499984741211]   \n",
       "8     [0.22205980122089386, 0.92166668176651]   \n",
       "9   [0.12658622860908508, 0.9560166597366333]   \n",
       "10   [0.1421215534210205, 0.9477333426475525]   \n",
       "11  [0.14996056258678436, 0.9464333057403564]   \n",
       "12  [0.23593302071094513, 0.9116500020027161]   \n",
       "13  [0.22073142230510712, 0.9183333516120911]   \n",
       "14  [0.27241677045822144, 0.9009333252906799]   \n",
       "15  [0.21186839044094086, 0.9207500219345093]   \n",
       "16  [0.19306862354278564, 0.9285666942596436]   \n",
       "17  [0.21572570502758026, 0.9204333424568176]   \n",
       "18  [0.23643141984939575, 0.9130666851997375]   \n",
       "19  [0.29450979828834534, 0.8953333497047424]   \n",
       "20   [0.2675584852695465, 0.9041666388511658]   \n",
       "21  [0.19672514498233795, 0.9282000064849854]   \n",
       "22  [0.20643432438373566, 0.9244666695594788]   \n",
       "23  [0.23273038864135742, 0.9152833223342896]   \n",
       "24   [0.1859435737133026, 0.9332666397094727]   \n",
       "25  [0.17796066403388977, 0.9342333078384399]   \n",
       "26  [0.21107719838619232, 0.9233666658401489]   \n",
       "27  [0.19442103803157806, 0.9269833564758301]   \n",
       "28   [0.1608797013759613, 0.9412999749183655]   \n",
       "29  [0.15656131505966187, 0.9422333240509033]   \n",
       "30   [0.1546628773212433, 0.9443833231925964]   \n",
       "31  [0.18178533017635345, 0.9347333312034607]   \n",
       "32  [0.20485873520374298, 0.9259999990463257]   \n",
       "33   [0.14446735382080078, 0.948033332824707]   \n",
       "34  [0.18002688884735107, 0.9334666728973389]   \n",
       "35  [0.15237271785736084, 0.9448999762535095]   \n",
       "36  [0.22614991664886475, 0.9175000190734863]   \n",
       "37  [0.21326030790805817, 0.9208333492279053]   \n",
       "38   [0.2588706910610199, 0.9059000015258789]   \n",
       "39  [0.24779362976551056, 0.9068999886512756]   \n",
       "40  [0.21686747670173645, 0.9199833273887634]   \n",
       "41  [0.23349125683307648, 0.9122499823570251]   \n",
       "42  [0.22014310956001282, 0.9203000068664551]   \n",
       "43    [0.25457531213760376, 0.90829998254776]   \n",
       "44   [0.2692805528640747, 0.9014833569526672]   \n",
       "45   [0.2132299542427063, 0.9207500219345093]   \n",
       "46  [0.21291086077690125, 0.9204333424568176]   \n",
       "47  [0.23679600656032562, 0.9127500057220459]   \n",
       "\n",
       "                                    test_eval  train_loss  train_acc  \\\n",
       "0   [0.26669758558273315, 0.9018999934196472]    0.168681   0.937850   \n",
       "1     [0.2670109272003174, 0.904699981212616]    0.159386   0.943050   \n",
       "2   [0.25698399543762207, 0.9108999967575073]    0.141778   0.948367   \n",
       "3      [0.252878874540329, 0.909500002861023]    0.150622   0.944367   \n",
       "4   [0.24787791073322296, 0.9126999974250793]    0.132230   0.952633   \n",
       "5   [0.25688549876213074, 0.9057000279426575]    0.185781   0.930900   \n",
       "6   [0.24840134382247925, 0.9114999771118164]    0.146565   0.947167   \n",
       "7    [0.2613731920719147, 0.9070000052452087]    0.170877   0.937850   \n",
       "8   [0.28667232394218445, 0.8970999717712402]    0.222060   0.921667   \n",
       "9   [0.24576333165168762, 0.9114999771118164]    0.126586   0.956017   \n",
       "10  [0.24299895763397217, 0.9117000102996826]    0.142122   0.947733   \n",
       "11  [0.24511033296585083, 0.9093000292778015]    0.149961   0.946433   \n",
       "12  [0.29616788029670715, 0.8928999900817871]    0.235933   0.911650   \n",
       "13   [0.2996402978897095, 0.8896999955177307]    0.220731   0.918333   \n",
       "14  [0.31941139698028564, 0.8870999813079834]    0.272417   0.900933   \n",
       "15   [0.2745698392391205, 0.8982999920845032]    0.211868   0.920750   \n",
       "16   [0.2737598717212677, 0.9014999866485596]    0.193069   0.928567   \n",
       "17   [0.2723506689071655, 0.8992999792098999]    0.215726   0.920433   \n",
       "18  [0.30026862025260925, 0.8944000005722046]    0.236431   0.913067   \n",
       "19   [0.3364871144294739, 0.8776999711990356]    0.294510   0.895333   \n",
       "20   [0.3267644941806793, 0.8819000124931335]    0.267558   0.904167   \n",
       "21  [0.25953343510627747, 0.9054999947547913]    0.196725   0.928200   \n",
       "22   [0.2693329453468323, 0.9034000039100647]    0.206434   0.924467   \n",
       "23    [0.2840319871902466, 0.894599974155426]    0.232730   0.915283   \n",
       "24   [0.2703552842140198, 0.9021999835968018]    0.185944   0.933267   \n",
       "25  [0.28307250142097473, 0.8967000246047974]    0.177961   0.934233   \n",
       "26  [0.28365108370780945, 0.8959000110626221]    0.211077   0.923367   \n",
       "27  [0.27573519945144653, 0.9016000032424927]    0.194421   0.926983   \n",
       "28  [0.26573246717453003, 0.9079999923706055]    0.160880   0.941300   \n",
       "29  [0.25688880681991577, 0.9093000292778015]    0.156561   0.942233   \n",
       "30    [0.27218130230903625, 0.90420001745224]    0.154663   0.944383   \n",
       "31   [0.2796153426170349, 0.9016000032424927]    0.181785   0.934733   \n",
       "32    [0.285814106464386, 0.8978000283241272]    0.204859   0.926000   \n",
       "33  [0.25142455101013184, 0.9111999869346619]    0.144467   0.948033   \n",
       "34   [0.2554672062397003, 0.9071999788284302]    0.180027   0.933467   \n",
       "35   [0.24747571349143982, 0.911300003528595]    0.152373   0.944900   \n",
       "36   [0.3019619286060333, 0.8912000060081482]    0.226150   0.917500   \n",
       "37  [0.30138877034187317, 0.8938000202178955]    0.213260   0.920833   \n",
       "38   [0.3161528706550598, 0.8884999752044678]    0.258871   0.905900   \n",
       "39   [0.2994548976421356, 0.8888999819755554]    0.247794   0.906900   \n",
       "40    [0.2744320034980774, 0.902400016784668]    0.216867   0.919983   \n",
       "41    [0.287698894739151, 0.8937000036239624]    0.233491   0.912250   \n",
       "42    [0.2999141812324524, 0.891700029373169]    0.220143   0.920300   \n",
       "43    [0.3104911148548126, 0.891700029373169]    0.254575   0.908300   \n",
       "44  [0.32393553853034973, 0.8855999708175659]    0.269281   0.901483   \n",
       "45  [0.27428877353668213, 0.9002000093460083]    0.213230   0.920750   \n",
       "46  [0.27476274967193604, 0.8995000123977661]    0.212911   0.920433   \n",
       "47  [0.28981828689575195, 0.8930000066757202]    0.236796   0.912750   \n",
       "\n",
       "    test_loss  test_acc  \n",
       "0    0.266698    0.9019  \n",
       "1    0.267011    0.9047  \n",
       "2    0.256984    0.9109  \n",
       "3    0.252879    0.9095  \n",
       "4    0.247878    0.9127  \n",
       "5    0.256885    0.9057  \n",
       "6    0.248401    0.9115  \n",
       "7    0.261373    0.9070  \n",
       "8    0.286672    0.8971  \n",
       "9    0.245763    0.9115  \n",
       "10   0.242999    0.9117  \n",
       "11   0.245110    0.9093  \n",
       "12   0.296168    0.8929  \n",
       "13   0.299640    0.8897  \n",
       "14   0.319411    0.8871  \n",
       "15   0.274570    0.8983  \n",
       "16   0.273760    0.9015  \n",
       "17   0.272351    0.8993  \n",
       "18   0.300269    0.8944  \n",
       "19   0.336487    0.8777  \n",
       "20   0.326764    0.8819  \n",
       "21   0.259533    0.9055  \n",
       "22   0.269333    0.9034  \n",
       "23   0.284032    0.8946  \n",
       "24   0.270355    0.9022  \n",
       "25   0.283073    0.8967  \n",
       "26   0.283651    0.8959  \n",
       "27   0.275735    0.9016  \n",
       "28   0.265732    0.9080  \n",
       "29   0.256889    0.9093  \n",
       "30   0.272181    0.9042  \n",
       "31   0.279615    0.9016  \n",
       "32   0.285814    0.8978  \n",
       "33   0.251425    0.9112  \n",
       "34   0.255467    0.9072  \n",
       "35   0.247476    0.9113  \n",
       "36   0.301962    0.8912  \n",
       "37   0.301389    0.8938  \n",
       "38   0.316153    0.8885  \n",
       "39   0.299455    0.8889  \n",
       "40   0.274432    0.9024  \n",
       "41   0.287699    0.8937  \n",
       "42   0.299914    0.8917  \n",
       "43   0.310491    0.8917  \n",
       "44   0.323936    0.8856  \n",
       "45   0.274289    0.9002  \n",
       "46   0.274763    0.8995  \n",
       "47   0.289818    0.8930  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO setup similar decisions as the Torch notebook - best test_acc, lowest loss, train one, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_eval</th>\n",
       "      <th>test_eval</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.13222971558570862, 0.9526333212852478]</td>\n",
       "      <td>[0.24787791073322296, 0.9126999974250793]</td>\n",
       "      <td>0.132230</td>\n",
       "      <td>0.952633</td>\n",
       "      <td>0.247878</td>\n",
       "      <td>0.9127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1421215534210205, 0.9477333426475525]</td>\n",
       "      <td>[0.24299895763397217, 0.9117000102996826]</td>\n",
       "      <td>0.142122</td>\n",
       "      <td>0.947733</td>\n",
       "      <td>0.242999</td>\n",
       "      <td>0.9117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.12658622860908508, 0.9560166597366333]</td>\n",
       "      <td>[0.24576333165168762, 0.9114999771118164]</td>\n",
       "      <td>0.126586</td>\n",
       "      <td>0.956017</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1465650051832199, 0.9471666812896729]</td>\n",
       "      <td>[0.24840134382247925, 0.9114999771118164]</td>\n",
       "      <td>0.146565</td>\n",
       "      <td>0.947167</td>\n",
       "      <td>0.248401</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15237271785736084, 0.9448999762535095]</td>\n",
       "      <td>[0.24747571349143982, 0.911300003528595]</td>\n",
       "      <td>0.152373</td>\n",
       "      <td>0.944900</td>\n",
       "      <td>0.247476</td>\n",
       "      <td>0.9113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14446735382080078, 0.948033332824707]</td>\n",
       "      <td>[0.25142455101013184, 0.9111999869346619]</td>\n",
       "      <td>0.144467</td>\n",
       "      <td>0.948033</td>\n",
       "      <td>0.251425</td>\n",
       "      <td>0.9112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1417781561613083, 0.948366641998291]</td>\n",
       "      <td>[0.25698399543762207, 0.9108999967575073]</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>0.948367</td>\n",
       "      <td>0.256984</td>\n",
       "      <td>0.9109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15062175691127777, 0.9443666934967041]</td>\n",
       "      <td>[0.252878874540329, 0.909500002861023]</td>\n",
       "      <td>0.150622</td>\n",
       "      <td>0.944367</td>\n",
       "      <td>0.252879</td>\n",
       "      <td>0.9095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14996056258678436, 0.9464333057403564]</td>\n",
       "      <td>[0.24511033296585083, 0.9093000292778015]</td>\n",
       "      <td>0.149961</td>\n",
       "      <td>0.946433</td>\n",
       "      <td>0.245110</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15656131505966187, 0.9422333240509033]</td>\n",
       "      <td>[0.25688880681991577, 0.9093000292778015]</td>\n",
       "      <td>0.156561</td>\n",
       "      <td>0.942233</td>\n",
       "      <td>0.256889</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1608797013759613, 0.9412999749183655]</td>\n",
       "      <td>[0.26573246717453003, 0.9079999923706055]</td>\n",
       "      <td>0.160880</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.265732</td>\n",
       "      <td>0.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18002688884735107, 0.9334666728973389]</td>\n",
       "      <td>[0.2554672062397003, 0.9071999788284302]</td>\n",
       "      <td>0.180027</td>\n",
       "      <td>0.933467</td>\n",
       "      <td>0.255467</td>\n",
       "      <td>0.9072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1708768606185913, 0.9378499984741211]</td>\n",
       "      <td>[0.2613731920719147, 0.9070000052452087]</td>\n",
       "      <td>0.170877</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.261373</td>\n",
       "      <td>0.9070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1857813447713852, 0.930899977684021]</td>\n",
       "      <td>[0.25688549876213074, 0.9057000279426575]</td>\n",
       "      <td>0.185781</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>0.256885</td>\n",
       "      <td>0.9057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19672514498233795, 0.9282000064849854]</td>\n",
       "      <td>[0.25953343510627747, 0.9054999947547913]</td>\n",
       "      <td>0.196725</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.259533</td>\n",
       "      <td>0.9055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15938624739646912, 0.9430500268936157]</td>\n",
       "      <td>[0.2670109272003174, 0.904699981212616]</td>\n",
       "      <td>0.159386</td>\n",
       "      <td>0.943050</td>\n",
       "      <td>0.267011</td>\n",
       "      <td>0.9047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1546628773212433, 0.9443833231925964]</td>\n",
       "      <td>[0.27218130230903625, 0.90420001745224]</td>\n",
       "      <td>0.154663</td>\n",
       "      <td>0.944383</td>\n",
       "      <td>0.272181</td>\n",
       "      <td>0.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20643432438373566, 0.9244666695594788]</td>\n",
       "      <td>[0.2693329453468323, 0.9034000039100647]</td>\n",
       "      <td>0.206434</td>\n",
       "      <td>0.924467</td>\n",
       "      <td>0.269333</td>\n",
       "      <td>0.9034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21686747670173645, 0.9199833273887634]</td>\n",
       "      <td>[0.2744320034980774, 0.902400016784668]</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.919983</td>\n",
       "      <td>0.274432</td>\n",
       "      <td>0.9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1859435737133026, 0.9332666397094727]</td>\n",
       "      <td>[0.2703552842140198, 0.9021999835968018]</td>\n",
       "      <td>0.185944</td>\n",
       "      <td>0.933267</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.9022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.16868141293525696, 0.9378499984741211]</td>\n",
       "      <td>[0.26669758558273315, 0.9018999934196472]</td>\n",
       "      <td>0.168681</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.266698</td>\n",
       "      <td>0.9019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18178533017635345, 0.9347333312034607]</td>\n",
       "      <td>[0.2796153426170349, 0.9016000032424927]</td>\n",
       "      <td>0.181785</td>\n",
       "      <td>0.934733</td>\n",
       "      <td>0.279615</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19442103803157806, 0.9269833564758301]</td>\n",
       "      <td>[0.27573519945144653, 0.9016000032424927]</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.926983</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19306862354278564, 0.9285666942596436]</td>\n",
       "      <td>[0.2737598717212677, 0.9014999866485596]</td>\n",
       "      <td>0.193069</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.273760</td>\n",
       "      <td>0.9015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2132299542427063, 0.9207500219345093]</td>\n",
       "      <td>[0.27428877353668213, 0.9002000093460083]</td>\n",
       "      <td>0.213230</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274289</td>\n",
       "      <td>0.9002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21291086077690125, 0.9204333424568176]</td>\n",
       "      <td>[0.27476274967193604, 0.8995000123977661]</td>\n",
       "      <td>0.212911</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.274763</td>\n",
       "      <td>0.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21572570502758026, 0.9204333424568176]</td>\n",
       "      <td>[0.2723506689071655, 0.8992999792098999]</td>\n",
       "      <td>0.215726</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.272351</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21186839044094086, 0.9207500219345093]</td>\n",
       "      <td>[0.2745698392391205, 0.8982999920845032]</td>\n",
       "      <td>0.211868</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274570</td>\n",
       "      <td>0.8983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20485873520374298, 0.9259999990463257]</td>\n",
       "      <td>[0.285814106464386, 0.8978000283241272]</td>\n",
       "      <td>0.204859</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.285814</td>\n",
       "      <td>0.8978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22205980122089386, 0.92166668176651]</td>\n",
       "      <td>[0.28667232394218445, 0.8970999717712402]</td>\n",
       "      <td>0.222060</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>0.286672</td>\n",
       "      <td>0.8971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.17796066403388977, 0.9342333078384399]</td>\n",
       "      <td>[0.28307250142097473, 0.8967000246047974]</td>\n",
       "      <td>0.177961</td>\n",
       "      <td>0.934233</td>\n",
       "      <td>0.283073</td>\n",
       "      <td>0.8967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21107719838619232, 0.9233666658401489]</td>\n",
       "      <td>[0.28365108370780945, 0.8959000110626221]</td>\n",
       "      <td>0.211077</td>\n",
       "      <td>0.923367</td>\n",
       "      <td>0.283651</td>\n",
       "      <td>0.8959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23273038864135742, 0.9152833223342896]</td>\n",
       "      <td>[0.2840319871902466, 0.894599974155426]</td>\n",
       "      <td>0.232730</td>\n",
       "      <td>0.915283</td>\n",
       "      <td>0.284032</td>\n",
       "      <td>0.8946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23643141984939575, 0.9130666851997375]</td>\n",
       "      <td>[0.30026862025260925, 0.8944000005722046]</td>\n",
       "      <td>0.236431</td>\n",
       "      <td>0.913067</td>\n",
       "      <td>0.300269</td>\n",
       "      <td>0.8944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21326030790805817, 0.9208333492279053]</td>\n",
       "      <td>[0.30138877034187317, 0.8938000202178955]</td>\n",
       "      <td>0.213260</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.301389</td>\n",
       "      <td>0.8938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23349125683307648, 0.9122499823570251]</td>\n",
       "      <td>[0.287698894739151, 0.8937000036239624]</td>\n",
       "      <td>0.233491</td>\n",
       "      <td>0.912250</td>\n",
       "      <td>0.287699</td>\n",
       "      <td>0.8937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23679600656032562, 0.9127500057220459]</td>\n",
       "      <td>[0.28981828689575195, 0.8930000066757202]</td>\n",
       "      <td>0.236796</td>\n",
       "      <td>0.912750</td>\n",
       "      <td>0.289818</td>\n",
       "      <td>0.8930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23593302071094513, 0.9116500020027161]</td>\n",
       "      <td>[0.29616788029670715, 0.8928999900817871]</td>\n",
       "      <td>0.235933</td>\n",
       "      <td>0.911650</td>\n",
       "      <td>0.296168</td>\n",
       "      <td>0.8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.25457531213760376, 0.90829998254776]</td>\n",
       "      <td>[0.3104911148548126, 0.891700029373169]</td>\n",
       "      <td>0.254575</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>0.310491</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22014310956001282, 0.9203000068664551]</td>\n",
       "      <td>[0.2999141812324524, 0.891700029373169]</td>\n",
       "      <td>0.220143</td>\n",
       "      <td>0.920300</td>\n",
       "      <td>0.299914</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22614991664886475, 0.9175000190734863]</td>\n",
       "      <td>[0.3019619286060333, 0.8912000060081482]</td>\n",
       "      <td>0.226150</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.301962</td>\n",
       "      <td>0.8912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22073142230510712, 0.9183333516120911]</td>\n",
       "      <td>[0.2996402978897095, 0.8896999955177307]</td>\n",
       "      <td>0.220731</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>0.299640</td>\n",
       "      <td>0.8897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.24779362976551056, 0.9068999886512756]</td>\n",
       "      <td>[0.2994548976421356, 0.8888999819755554]</td>\n",
       "      <td>0.247794</td>\n",
       "      <td>0.906900</td>\n",
       "      <td>0.299455</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2588706910610199, 0.9059000015258789]</td>\n",
       "      <td>[0.3161528706550598, 0.8884999752044678]</td>\n",
       "      <td>0.258871</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.316153</td>\n",
       "      <td>0.8885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.27241677045822144, 0.9009333252906799]</td>\n",
       "      <td>[0.31941139698028564, 0.8870999813079834]</td>\n",
       "      <td>0.272417</td>\n",
       "      <td>0.900933</td>\n",
       "      <td>0.319411</td>\n",
       "      <td>0.8871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2692805528640747, 0.9014833569526672]</td>\n",
       "      <td>[0.32393553853034973, 0.8855999708175659]</td>\n",
       "      <td>0.269281</td>\n",
       "      <td>0.901483</td>\n",
       "      <td>0.323936</td>\n",
       "      <td>0.8856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2675584852695465, 0.9041666388511658]</td>\n",
       "      <td>[0.3267644941806793, 0.8819000124931335]</td>\n",
       "      <td>0.267558</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>0.326764</td>\n",
       "      <td>0.8819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.29450979828834534, 0.8953333497047424]</td>\n",
       "      <td>[0.3364871144294739, 0.8776999711990356]</td>\n",
       "      <td>0.294510</td>\n",
       "      <td>0.895333</td>\n",
       "      <td>0.336487</td>\n",
       "      <td>0.8777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  \\\n",
       "4   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "10  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "9   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "6   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "35  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "33  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "2   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "3   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "11  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "29  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "28  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "34  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "7   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "5   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "21  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "1   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "30  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "22  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "40  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "24  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "0   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "31  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "27  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "16  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "45  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "46  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "17  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "15  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "32  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "8   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "25  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "26  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "23  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "18  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "37  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "41  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "47  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "12  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "43  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "42  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "36  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "13  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "39  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "38  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "14  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "44  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "20  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "19  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "\n",
       "                                   train_eval  \\\n",
       "4   [0.13222971558570862, 0.9526333212852478]   \n",
       "10   [0.1421215534210205, 0.9477333426475525]   \n",
       "9   [0.12658622860908508, 0.9560166597366333]   \n",
       "6    [0.1465650051832199, 0.9471666812896729]   \n",
       "35  [0.15237271785736084, 0.9448999762535095]   \n",
       "33   [0.14446735382080078, 0.948033332824707]   \n",
       "2     [0.1417781561613083, 0.948366641998291]   \n",
       "3   [0.15062175691127777, 0.9443666934967041]   \n",
       "11  [0.14996056258678436, 0.9464333057403564]   \n",
       "29  [0.15656131505966187, 0.9422333240509033]   \n",
       "28   [0.1608797013759613, 0.9412999749183655]   \n",
       "34  [0.18002688884735107, 0.9334666728973389]   \n",
       "7    [0.1708768606185913, 0.9378499984741211]   \n",
       "5     [0.1857813447713852, 0.930899977684021]   \n",
       "21  [0.19672514498233795, 0.9282000064849854]   \n",
       "1   [0.15938624739646912, 0.9430500268936157]   \n",
       "30   [0.1546628773212433, 0.9443833231925964]   \n",
       "22  [0.20643432438373566, 0.9244666695594788]   \n",
       "40  [0.21686747670173645, 0.9199833273887634]   \n",
       "24   [0.1859435737133026, 0.9332666397094727]   \n",
       "0   [0.16868141293525696, 0.9378499984741211]   \n",
       "31  [0.18178533017635345, 0.9347333312034607]   \n",
       "27  [0.19442103803157806, 0.9269833564758301]   \n",
       "16  [0.19306862354278564, 0.9285666942596436]   \n",
       "45   [0.2132299542427063, 0.9207500219345093]   \n",
       "46  [0.21291086077690125, 0.9204333424568176]   \n",
       "17  [0.21572570502758026, 0.9204333424568176]   \n",
       "15  [0.21186839044094086, 0.9207500219345093]   \n",
       "32  [0.20485873520374298, 0.9259999990463257]   \n",
       "8     [0.22205980122089386, 0.92166668176651]   \n",
       "25  [0.17796066403388977, 0.9342333078384399]   \n",
       "26  [0.21107719838619232, 0.9233666658401489]   \n",
       "23  [0.23273038864135742, 0.9152833223342896]   \n",
       "18  [0.23643141984939575, 0.9130666851997375]   \n",
       "37  [0.21326030790805817, 0.9208333492279053]   \n",
       "41  [0.23349125683307648, 0.9122499823570251]   \n",
       "47  [0.23679600656032562, 0.9127500057220459]   \n",
       "12  [0.23593302071094513, 0.9116500020027161]   \n",
       "43    [0.25457531213760376, 0.90829998254776]   \n",
       "42  [0.22014310956001282, 0.9203000068664551]   \n",
       "36  [0.22614991664886475, 0.9175000190734863]   \n",
       "13  [0.22073142230510712, 0.9183333516120911]   \n",
       "39  [0.24779362976551056, 0.9068999886512756]   \n",
       "38   [0.2588706910610199, 0.9059000015258789]   \n",
       "14  [0.27241677045822144, 0.9009333252906799]   \n",
       "44   [0.2692805528640747, 0.9014833569526672]   \n",
       "20   [0.2675584852695465, 0.9041666388511658]   \n",
       "19  [0.29450979828834534, 0.8953333497047424]   \n",
       "\n",
       "                                    test_eval  train_loss  train_acc  \\\n",
       "4   [0.24787791073322296, 0.9126999974250793]    0.132230   0.952633   \n",
       "10  [0.24299895763397217, 0.9117000102996826]    0.142122   0.947733   \n",
       "9   [0.24576333165168762, 0.9114999771118164]    0.126586   0.956017   \n",
       "6   [0.24840134382247925, 0.9114999771118164]    0.146565   0.947167   \n",
       "35   [0.24747571349143982, 0.911300003528595]    0.152373   0.944900   \n",
       "33  [0.25142455101013184, 0.9111999869346619]    0.144467   0.948033   \n",
       "2   [0.25698399543762207, 0.9108999967575073]    0.141778   0.948367   \n",
       "3      [0.252878874540329, 0.909500002861023]    0.150622   0.944367   \n",
       "11  [0.24511033296585083, 0.9093000292778015]    0.149961   0.946433   \n",
       "29  [0.25688880681991577, 0.9093000292778015]    0.156561   0.942233   \n",
       "28  [0.26573246717453003, 0.9079999923706055]    0.160880   0.941300   \n",
       "34   [0.2554672062397003, 0.9071999788284302]    0.180027   0.933467   \n",
       "7    [0.2613731920719147, 0.9070000052452087]    0.170877   0.937850   \n",
       "5   [0.25688549876213074, 0.9057000279426575]    0.185781   0.930900   \n",
       "21  [0.25953343510627747, 0.9054999947547913]    0.196725   0.928200   \n",
       "1     [0.2670109272003174, 0.904699981212616]    0.159386   0.943050   \n",
       "30    [0.27218130230903625, 0.90420001745224]    0.154663   0.944383   \n",
       "22   [0.2693329453468323, 0.9034000039100647]    0.206434   0.924467   \n",
       "40    [0.2744320034980774, 0.902400016784668]    0.216867   0.919983   \n",
       "24   [0.2703552842140198, 0.9021999835968018]    0.185944   0.933267   \n",
       "0   [0.26669758558273315, 0.9018999934196472]    0.168681   0.937850   \n",
       "31   [0.2796153426170349, 0.9016000032424927]    0.181785   0.934733   \n",
       "27  [0.27573519945144653, 0.9016000032424927]    0.194421   0.926983   \n",
       "16   [0.2737598717212677, 0.9014999866485596]    0.193069   0.928567   \n",
       "45  [0.27428877353668213, 0.9002000093460083]    0.213230   0.920750   \n",
       "46  [0.27476274967193604, 0.8995000123977661]    0.212911   0.920433   \n",
       "17   [0.2723506689071655, 0.8992999792098999]    0.215726   0.920433   \n",
       "15   [0.2745698392391205, 0.8982999920845032]    0.211868   0.920750   \n",
       "32    [0.285814106464386, 0.8978000283241272]    0.204859   0.926000   \n",
       "8   [0.28667232394218445, 0.8970999717712402]    0.222060   0.921667   \n",
       "25  [0.28307250142097473, 0.8967000246047974]    0.177961   0.934233   \n",
       "26  [0.28365108370780945, 0.8959000110626221]    0.211077   0.923367   \n",
       "23    [0.2840319871902466, 0.894599974155426]    0.232730   0.915283   \n",
       "18  [0.30026862025260925, 0.8944000005722046]    0.236431   0.913067   \n",
       "37  [0.30138877034187317, 0.8938000202178955]    0.213260   0.920833   \n",
       "41    [0.287698894739151, 0.8937000036239624]    0.233491   0.912250   \n",
       "47  [0.28981828689575195, 0.8930000066757202]    0.236796   0.912750   \n",
       "12  [0.29616788029670715, 0.8928999900817871]    0.235933   0.911650   \n",
       "43    [0.3104911148548126, 0.891700029373169]    0.254575   0.908300   \n",
       "42    [0.2999141812324524, 0.891700029373169]    0.220143   0.920300   \n",
       "36   [0.3019619286060333, 0.8912000060081482]    0.226150   0.917500   \n",
       "13   [0.2996402978897095, 0.8896999955177307]    0.220731   0.918333   \n",
       "39   [0.2994548976421356, 0.8888999819755554]    0.247794   0.906900   \n",
       "38   [0.3161528706550598, 0.8884999752044678]    0.258871   0.905900   \n",
       "14  [0.31941139698028564, 0.8870999813079834]    0.272417   0.900933   \n",
       "44  [0.32393553853034973, 0.8855999708175659]    0.269281   0.901483   \n",
       "20   [0.3267644941806793, 0.8819000124931335]    0.267558   0.904167   \n",
       "19   [0.3364871144294739, 0.8776999711990356]    0.294510   0.895333   \n",
       "\n",
       "    test_loss  test_acc  \n",
       "4    0.247878    0.9127  \n",
       "10   0.242999    0.9117  \n",
       "9    0.245763    0.9115  \n",
       "6    0.248401    0.9115  \n",
       "35   0.247476    0.9113  \n",
       "33   0.251425    0.9112  \n",
       "2    0.256984    0.9109  \n",
       "3    0.252879    0.9095  \n",
       "11   0.245110    0.9093  \n",
       "29   0.256889    0.9093  \n",
       "28   0.265732    0.9080  \n",
       "34   0.255467    0.9072  \n",
       "7    0.261373    0.9070  \n",
       "5    0.256885    0.9057  \n",
       "21   0.259533    0.9055  \n",
       "1    0.267011    0.9047  \n",
       "30   0.272181    0.9042  \n",
       "22   0.269333    0.9034  \n",
       "40   0.274432    0.9024  \n",
       "24   0.270355    0.9022  \n",
       "0    0.266698    0.9019  \n",
       "31   0.279615    0.9016  \n",
       "27   0.275735    0.9016  \n",
       "16   0.273760    0.9015  \n",
       "45   0.274289    0.9002  \n",
       "46   0.274763    0.8995  \n",
       "17   0.272351    0.8993  \n",
       "15   0.274570    0.8983  \n",
       "32   0.285814    0.8978  \n",
       "8    0.286672    0.8971  \n",
       "25   0.283073    0.8967  \n",
       "26   0.283651    0.8959  \n",
       "23   0.284032    0.8946  \n",
       "18   0.300269    0.8944  \n",
       "37   0.301389    0.8938  \n",
       "41   0.287699    0.8937  \n",
       "47   0.289818    0.8930  \n",
       "12   0.296168    0.8929  \n",
       "43   0.310491    0.8917  \n",
       "42   0.299914    0.8917  \n",
       "36   0.301962    0.8912  \n",
       "13   0.299640    0.8897  \n",
       "39   0.299455    0.8889  \n",
       "38   0.316153    0.8885  \n",
       "14   0.319411    0.8871  \n",
       "44   0.323936    0.8856  \n",
       "20   0.326764    0.8819  \n",
       "19   0.336487    0.8777  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.sort_values(by=['test_acc'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_eval</th>\n",
       "      <th>test_eval</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1421215534210205, 0.9477333426475525]</td>\n",
       "      <td>[0.24299895763397217, 0.9117000102996826]</td>\n",
       "      <td>0.142122</td>\n",
       "      <td>0.947733</td>\n",
       "      <td>0.242999</td>\n",
       "      <td>0.9117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14996056258678436, 0.9464333057403564]</td>\n",
       "      <td>[0.24511033296585083, 0.9093000292778015]</td>\n",
       "      <td>0.149961</td>\n",
       "      <td>0.946433</td>\n",
       "      <td>0.245110</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.12658622860908508, 0.9560166597366333]</td>\n",
       "      <td>[0.24576333165168762, 0.9114999771118164]</td>\n",
       "      <td>0.126586</td>\n",
       "      <td>0.956017</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15237271785736084, 0.9448999762535095]</td>\n",
       "      <td>[0.24747571349143982, 0.911300003528595]</td>\n",
       "      <td>0.152373</td>\n",
       "      <td>0.944900</td>\n",
       "      <td>0.247476</td>\n",
       "      <td>0.9113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.13222971558570862, 0.9526333212852478]</td>\n",
       "      <td>[0.24787791073322296, 0.9126999974250793]</td>\n",
       "      <td>0.132230</td>\n",
       "      <td>0.952633</td>\n",
       "      <td>0.247878</td>\n",
       "      <td>0.9127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1465650051832199, 0.9471666812896729]</td>\n",
       "      <td>[0.24840134382247925, 0.9114999771118164]</td>\n",
       "      <td>0.146565</td>\n",
       "      <td>0.947167</td>\n",
       "      <td>0.248401</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14446735382080078, 0.948033332824707]</td>\n",
       "      <td>[0.25142455101013184, 0.9111999869346619]</td>\n",
       "      <td>0.144467</td>\n",
       "      <td>0.948033</td>\n",
       "      <td>0.251425</td>\n",
       "      <td>0.9112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15062175691127777, 0.9443666934967041]</td>\n",
       "      <td>[0.252878874540329, 0.909500002861023]</td>\n",
       "      <td>0.150622</td>\n",
       "      <td>0.944367</td>\n",
       "      <td>0.252879</td>\n",
       "      <td>0.9095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18002688884735107, 0.9334666728973389]</td>\n",
       "      <td>[0.2554672062397003, 0.9071999788284302]</td>\n",
       "      <td>0.180027</td>\n",
       "      <td>0.933467</td>\n",
       "      <td>0.255467</td>\n",
       "      <td>0.9072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1857813447713852, 0.930899977684021]</td>\n",
       "      <td>[0.25688549876213074, 0.9057000279426575]</td>\n",
       "      <td>0.185781</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>0.256885</td>\n",
       "      <td>0.9057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15656131505966187, 0.9422333240509033]</td>\n",
       "      <td>[0.25688880681991577, 0.9093000292778015]</td>\n",
       "      <td>0.156561</td>\n",
       "      <td>0.942233</td>\n",
       "      <td>0.256889</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1417781561613083, 0.948366641998291]</td>\n",
       "      <td>[0.25698399543762207, 0.9108999967575073]</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>0.948367</td>\n",
       "      <td>0.256984</td>\n",
       "      <td>0.9109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19672514498233795, 0.9282000064849854]</td>\n",
       "      <td>[0.25953343510627747, 0.9054999947547913]</td>\n",
       "      <td>0.196725</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.259533</td>\n",
       "      <td>0.9055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1708768606185913, 0.9378499984741211]</td>\n",
       "      <td>[0.2613731920719147, 0.9070000052452087]</td>\n",
       "      <td>0.170877</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.261373</td>\n",
       "      <td>0.9070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1608797013759613, 0.9412999749183655]</td>\n",
       "      <td>[0.26573246717453003, 0.9079999923706055]</td>\n",
       "      <td>0.160880</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.265732</td>\n",
       "      <td>0.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.16868141293525696, 0.9378499984741211]</td>\n",
       "      <td>[0.26669758558273315, 0.9018999934196472]</td>\n",
       "      <td>0.168681</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.266698</td>\n",
       "      <td>0.9019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15938624739646912, 0.9430500268936157]</td>\n",
       "      <td>[0.2670109272003174, 0.904699981212616]</td>\n",
       "      <td>0.159386</td>\n",
       "      <td>0.943050</td>\n",
       "      <td>0.267011</td>\n",
       "      <td>0.9047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20643432438373566, 0.9244666695594788]</td>\n",
       "      <td>[0.2693329453468323, 0.9034000039100647]</td>\n",
       "      <td>0.206434</td>\n",
       "      <td>0.924467</td>\n",
       "      <td>0.269333</td>\n",
       "      <td>0.9034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1859435737133026, 0.9332666397094727]</td>\n",
       "      <td>[0.2703552842140198, 0.9021999835968018]</td>\n",
       "      <td>0.185944</td>\n",
       "      <td>0.933267</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.9022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1546628773212433, 0.9443833231925964]</td>\n",
       "      <td>[0.27218130230903625, 0.90420001745224]</td>\n",
       "      <td>0.154663</td>\n",
       "      <td>0.944383</td>\n",
       "      <td>0.272181</td>\n",
       "      <td>0.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21572570502758026, 0.9204333424568176]</td>\n",
       "      <td>[0.2723506689071655, 0.8992999792098999]</td>\n",
       "      <td>0.215726</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.272351</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19306862354278564, 0.9285666942596436]</td>\n",
       "      <td>[0.2737598717212677, 0.9014999866485596]</td>\n",
       "      <td>0.193069</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.273760</td>\n",
       "      <td>0.9015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2132299542427063, 0.9207500219345093]</td>\n",
       "      <td>[0.27428877353668213, 0.9002000093460083]</td>\n",
       "      <td>0.213230</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274289</td>\n",
       "      <td>0.9002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21686747670173645, 0.9199833273887634]</td>\n",
       "      <td>[0.2744320034980774, 0.902400016784668]</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.919983</td>\n",
       "      <td>0.274432</td>\n",
       "      <td>0.9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21186839044094086, 0.9207500219345093]</td>\n",
       "      <td>[0.2745698392391205, 0.8982999920845032]</td>\n",
       "      <td>0.211868</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274570</td>\n",
       "      <td>0.8983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21291086077690125, 0.9204333424568176]</td>\n",
       "      <td>[0.27476274967193604, 0.8995000123977661]</td>\n",
       "      <td>0.212911</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.274763</td>\n",
       "      <td>0.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19442103803157806, 0.9269833564758301]</td>\n",
       "      <td>[0.27573519945144653, 0.9016000032424927]</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.926983</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18178533017635345, 0.9347333312034607]</td>\n",
       "      <td>[0.2796153426170349, 0.9016000032424927]</td>\n",
       "      <td>0.181785</td>\n",
       "      <td>0.934733</td>\n",
       "      <td>0.279615</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.17796066403388977, 0.9342333078384399]</td>\n",
       "      <td>[0.28307250142097473, 0.8967000246047974]</td>\n",
       "      <td>0.177961</td>\n",
       "      <td>0.934233</td>\n",
       "      <td>0.283073</td>\n",
       "      <td>0.8967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21107719838619232, 0.9233666658401489]</td>\n",
       "      <td>[0.28365108370780945, 0.8959000110626221]</td>\n",
       "      <td>0.211077</td>\n",
       "      <td>0.923367</td>\n",
       "      <td>0.283651</td>\n",
       "      <td>0.8959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23273038864135742, 0.9152833223342896]</td>\n",
       "      <td>[0.2840319871902466, 0.894599974155426]</td>\n",
       "      <td>0.232730</td>\n",
       "      <td>0.915283</td>\n",
       "      <td>0.284032</td>\n",
       "      <td>0.8946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20485873520374298, 0.9259999990463257]</td>\n",
       "      <td>[0.285814106464386, 0.8978000283241272]</td>\n",
       "      <td>0.204859</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.285814</td>\n",
       "      <td>0.8978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22205980122089386, 0.92166668176651]</td>\n",
       "      <td>[0.28667232394218445, 0.8970999717712402]</td>\n",
       "      <td>0.222060</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>0.286672</td>\n",
       "      <td>0.8971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23349125683307648, 0.9122499823570251]</td>\n",
       "      <td>[0.287698894739151, 0.8937000036239624]</td>\n",
       "      <td>0.233491</td>\n",
       "      <td>0.912250</td>\n",
       "      <td>0.287699</td>\n",
       "      <td>0.8937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23679600656032562, 0.9127500057220459]</td>\n",
       "      <td>[0.28981828689575195, 0.8930000066757202]</td>\n",
       "      <td>0.236796</td>\n",
       "      <td>0.912750</td>\n",
       "      <td>0.289818</td>\n",
       "      <td>0.8930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23593302071094513, 0.9116500020027161]</td>\n",
       "      <td>[0.29616788029670715, 0.8928999900817871]</td>\n",
       "      <td>0.235933</td>\n",
       "      <td>0.911650</td>\n",
       "      <td>0.296168</td>\n",
       "      <td>0.8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.24779362976551056, 0.9068999886512756]</td>\n",
       "      <td>[0.2994548976421356, 0.8888999819755554]</td>\n",
       "      <td>0.247794</td>\n",
       "      <td>0.906900</td>\n",
       "      <td>0.299455</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22073142230510712, 0.9183333516120911]</td>\n",
       "      <td>[0.2996402978897095, 0.8896999955177307]</td>\n",
       "      <td>0.220731</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>0.299640</td>\n",
       "      <td>0.8897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22014310956001282, 0.9203000068664551]</td>\n",
       "      <td>[0.2999141812324524, 0.891700029373169]</td>\n",
       "      <td>0.220143</td>\n",
       "      <td>0.920300</td>\n",
       "      <td>0.299914</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23643141984939575, 0.9130666851997375]</td>\n",
       "      <td>[0.30026862025260925, 0.8944000005722046]</td>\n",
       "      <td>0.236431</td>\n",
       "      <td>0.913067</td>\n",
       "      <td>0.300269</td>\n",
       "      <td>0.8944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21326030790805817, 0.9208333492279053]</td>\n",
       "      <td>[0.30138877034187317, 0.8938000202178955]</td>\n",
       "      <td>0.213260</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.301389</td>\n",
       "      <td>0.8938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22614991664886475, 0.9175000190734863]</td>\n",
       "      <td>[0.3019619286060333, 0.8912000060081482]</td>\n",
       "      <td>0.226150</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.301962</td>\n",
       "      <td>0.8912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.25457531213760376, 0.90829998254776]</td>\n",
       "      <td>[0.3104911148548126, 0.891700029373169]</td>\n",
       "      <td>0.254575</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>0.310491</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2588706910610199, 0.9059000015258789]</td>\n",
       "      <td>[0.3161528706550598, 0.8884999752044678]</td>\n",
       "      <td>0.258871</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.316153</td>\n",
       "      <td>0.8885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.27241677045822144, 0.9009333252906799]</td>\n",
       "      <td>[0.31941139698028564, 0.8870999813079834]</td>\n",
       "      <td>0.272417</td>\n",
       "      <td>0.900933</td>\n",
       "      <td>0.319411</td>\n",
       "      <td>0.8871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2692805528640747, 0.9014833569526672]</td>\n",
       "      <td>[0.32393553853034973, 0.8855999708175659]</td>\n",
       "      <td>0.269281</td>\n",
       "      <td>0.901483</td>\n",
       "      <td>0.323936</td>\n",
       "      <td>0.8856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2675584852695465, 0.9041666388511658]</td>\n",
       "      <td>[0.3267644941806793, 0.8819000124931335]</td>\n",
       "      <td>0.267558</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>0.326764</td>\n",
       "      <td>0.8819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.29450979828834534, 0.8953333497047424]</td>\n",
       "      <td>[0.3364871144294739, 0.8776999711990356]</td>\n",
       "      <td>0.294510</td>\n",
       "      <td>0.895333</td>\n",
       "      <td>0.336487</td>\n",
       "      <td>0.8777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  \\\n",
       "10  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "11  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "9   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "35  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "4   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "6   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "33  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "3   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "34  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "5   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "29  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "2   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "21  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "7   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "28  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "0   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "1   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "22  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "24  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "30  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "17  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "16  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "45  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "40  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "15  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "46  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "27  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "31  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "25  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "26  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "23  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "32  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "8   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "41  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "47  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "12  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "39  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "13  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "42  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "18  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "37  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "36  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "43  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "38  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "14  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "44  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "20  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "19  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "\n",
       "                                   train_eval  \\\n",
       "10   [0.1421215534210205, 0.9477333426475525]   \n",
       "11  [0.14996056258678436, 0.9464333057403564]   \n",
       "9   [0.12658622860908508, 0.9560166597366333]   \n",
       "35  [0.15237271785736084, 0.9448999762535095]   \n",
       "4   [0.13222971558570862, 0.9526333212852478]   \n",
       "6    [0.1465650051832199, 0.9471666812896729]   \n",
       "33   [0.14446735382080078, 0.948033332824707]   \n",
       "3   [0.15062175691127777, 0.9443666934967041]   \n",
       "34  [0.18002688884735107, 0.9334666728973389]   \n",
       "5     [0.1857813447713852, 0.930899977684021]   \n",
       "29  [0.15656131505966187, 0.9422333240509033]   \n",
       "2     [0.1417781561613083, 0.948366641998291]   \n",
       "21  [0.19672514498233795, 0.9282000064849854]   \n",
       "7    [0.1708768606185913, 0.9378499984741211]   \n",
       "28   [0.1608797013759613, 0.9412999749183655]   \n",
       "0   [0.16868141293525696, 0.9378499984741211]   \n",
       "1   [0.15938624739646912, 0.9430500268936157]   \n",
       "22  [0.20643432438373566, 0.9244666695594788]   \n",
       "24   [0.1859435737133026, 0.9332666397094727]   \n",
       "30   [0.1546628773212433, 0.9443833231925964]   \n",
       "17  [0.21572570502758026, 0.9204333424568176]   \n",
       "16  [0.19306862354278564, 0.9285666942596436]   \n",
       "45   [0.2132299542427063, 0.9207500219345093]   \n",
       "40  [0.21686747670173645, 0.9199833273887634]   \n",
       "15  [0.21186839044094086, 0.9207500219345093]   \n",
       "46  [0.21291086077690125, 0.9204333424568176]   \n",
       "27  [0.19442103803157806, 0.9269833564758301]   \n",
       "31  [0.18178533017635345, 0.9347333312034607]   \n",
       "25  [0.17796066403388977, 0.9342333078384399]   \n",
       "26  [0.21107719838619232, 0.9233666658401489]   \n",
       "23  [0.23273038864135742, 0.9152833223342896]   \n",
       "32  [0.20485873520374298, 0.9259999990463257]   \n",
       "8     [0.22205980122089386, 0.92166668176651]   \n",
       "41  [0.23349125683307648, 0.9122499823570251]   \n",
       "47  [0.23679600656032562, 0.9127500057220459]   \n",
       "12  [0.23593302071094513, 0.9116500020027161]   \n",
       "39  [0.24779362976551056, 0.9068999886512756]   \n",
       "13  [0.22073142230510712, 0.9183333516120911]   \n",
       "42  [0.22014310956001282, 0.9203000068664551]   \n",
       "18  [0.23643141984939575, 0.9130666851997375]   \n",
       "37  [0.21326030790805817, 0.9208333492279053]   \n",
       "36  [0.22614991664886475, 0.9175000190734863]   \n",
       "43    [0.25457531213760376, 0.90829998254776]   \n",
       "38   [0.2588706910610199, 0.9059000015258789]   \n",
       "14  [0.27241677045822144, 0.9009333252906799]   \n",
       "44   [0.2692805528640747, 0.9014833569526672]   \n",
       "20   [0.2675584852695465, 0.9041666388511658]   \n",
       "19  [0.29450979828834534, 0.8953333497047424]   \n",
       "\n",
       "                                    test_eval  train_loss  train_acc  \\\n",
       "10  [0.24299895763397217, 0.9117000102996826]    0.142122   0.947733   \n",
       "11  [0.24511033296585083, 0.9093000292778015]    0.149961   0.946433   \n",
       "9   [0.24576333165168762, 0.9114999771118164]    0.126586   0.956017   \n",
       "35   [0.24747571349143982, 0.911300003528595]    0.152373   0.944900   \n",
       "4   [0.24787791073322296, 0.9126999974250793]    0.132230   0.952633   \n",
       "6   [0.24840134382247925, 0.9114999771118164]    0.146565   0.947167   \n",
       "33  [0.25142455101013184, 0.9111999869346619]    0.144467   0.948033   \n",
       "3      [0.252878874540329, 0.909500002861023]    0.150622   0.944367   \n",
       "34   [0.2554672062397003, 0.9071999788284302]    0.180027   0.933467   \n",
       "5   [0.25688549876213074, 0.9057000279426575]    0.185781   0.930900   \n",
       "29  [0.25688880681991577, 0.9093000292778015]    0.156561   0.942233   \n",
       "2   [0.25698399543762207, 0.9108999967575073]    0.141778   0.948367   \n",
       "21  [0.25953343510627747, 0.9054999947547913]    0.196725   0.928200   \n",
       "7    [0.2613731920719147, 0.9070000052452087]    0.170877   0.937850   \n",
       "28  [0.26573246717453003, 0.9079999923706055]    0.160880   0.941300   \n",
       "0   [0.26669758558273315, 0.9018999934196472]    0.168681   0.937850   \n",
       "1     [0.2670109272003174, 0.904699981212616]    0.159386   0.943050   \n",
       "22   [0.2693329453468323, 0.9034000039100647]    0.206434   0.924467   \n",
       "24   [0.2703552842140198, 0.9021999835968018]    0.185944   0.933267   \n",
       "30    [0.27218130230903625, 0.90420001745224]    0.154663   0.944383   \n",
       "17   [0.2723506689071655, 0.8992999792098999]    0.215726   0.920433   \n",
       "16   [0.2737598717212677, 0.9014999866485596]    0.193069   0.928567   \n",
       "45  [0.27428877353668213, 0.9002000093460083]    0.213230   0.920750   \n",
       "40    [0.2744320034980774, 0.902400016784668]    0.216867   0.919983   \n",
       "15   [0.2745698392391205, 0.8982999920845032]    0.211868   0.920750   \n",
       "46  [0.27476274967193604, 0.8995000123977661]    0.212911   0.920433   \n",
       "27  [0.27573519945144653, 0.9016000032424927]    0.194421   0.926983   \n",
       "31   [0.2796153426170349, 0.9016000032424927]    0.181785   0.934733   \n",
       "25  [0.28307250142097473, 0.8967000246047974]    0.177961   0.934233   \n",
       "26  [0.28365108370780945, 0.8959000110626221]    0.211077   0.923367   \n",
       "23    [0.2840319871902466, 0.894599974155426]    0.232730   0.915283   \n",
       "32    [0.285814106464386, 0.8978000283241272]    0.204859   0.926000   \n",
       "8   [0.28667232394218445, 0.8970999717712402]    0.222060   0.921667   \n",
       "41    [0.287698894739151, 0.8937000036239624]    0.233491   0.912250   \n",
       "47  [0.28981828689575195, 0.8930000066757202]    0.236796   0.912750   \n",
       "12  [0.29616788029670715, 0.8928999900817871]    0.235933   0.911650   \n",
       "39   [0.2994548976421356, 0.8888999819755554]    0.247794   0.906900   \n",
       "13   [0.2996402978897095, 0.8896999955177307]    0.220731   0.918333   \n",
       "42    [0.2999141812324524, 0.891700029373169]    0.220143   0.920300   \n",
       "18  [0.30026862025260925, 0.8944000005722046]    0.236431   0.913067   \n",
       "37  [0.30138877034187317, 0.8938000202178955]    0.213260   0.920833   \n",
       "36   [0.3019619286060333, 0.8912000060081482]    0.226150   0.917500   \n",
       "43    [0.3104911148548126, 0.891700029373169]    0.254575   0.908300   \n",
       "38   [0.3161528706550598, 0.8884999752044678]    0.258871   0.905900   \n",
       "14  [0.31941139698028564, 0.8870999813079834]    0.272417   0.900933   \n",
       "44  [0.32393553853034973, 0.8855999708175659]    0.269281   0.901483   \n",
       "20   [0.3267644941806793, 0.8819000124931335]    0.267558   0.904167   \n",
       "19   [0.3364871144294739, 0.8776999711990356]    0.294510   0.895333   \n",
       "\n",
       "    test_loss  test_acc  \n",
       "10   0.242999    0.9117  \n",
       "11   0.245110    0.9093  \n",
       "9    0.245763    0.9115  \n",
       "35   0.247476    0.9113  \n",
       "4    0.247878    0.9127  \n",
       "6    0.248401    0.9115  \n",
       "33   0.251425    0.9112  \n",
       "3    0.252879    0.9095  \n",
       "34   0.255467    0.9072  \n",
       "5    0.256885    0.9057  \n",
       "29   0.256889    0.9093  \n",
       "2    0.256984    0.9109  \n",
       "21   0.259533    0.9055  \n",
       "7    0.261373    0.9070  \n",
       "28   0.265732    0.9080  \n",
       "0    0.266698    0.9019  \n",
       "1    0.267011    0.9047  \n",
       "22   0.269333    0.9034  \n",
       "24   0.270355    0.9022  \n",
       "30   0.272181    0.9042  \n",
       "17   0.272351    0.8993  \n",
       "16   0.273760    0.9015  \n",
       "45   0.274289    0.9002  \n",
       "40   0.274432    0.9024  \n",
       "15   0.274570    0.8983  \n",
       "46   0.274763    0.8995  \n",
       "27   0.275735    0.9016  \n",
       "31   0.279615    0.9016  \n",
       "25   0.283073    0.8967  \n",
       "26   0.283651    0.8959  \n",
       "23   0.284032    0.8946  \n",
       "32   0.285814    0.8978  \n",
       "8    0.286672    0.8971  \n",
       "41   0.287699    0.8937  \n",
       "47   0.289818    0.8930  \n",
       "12   0.296168    0.8929  \n",
       "39   0.299455    0.8889  \n",
       "13   0.299640    0.8897  \n",
       "42   0.299914    0.8917  \n",
       "18   0.300269    0.8944  \n",
       "37   0.301389    0.8938  \n",
       "36   0.301962    0.8912  \n",
       "43   0.310491    0.8917  \n",
       "38   0.316153    0.8885  \n",
       "14   0.319411    0.8871  \n",
       "44   0.323936    0.8856  \n",
       "20   0.326764    0.8819  \n",
       "19   0.336487    0.8777  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.sort_values(by=['test_loss'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_eval</th>\n",
       "      <th>test_eval</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.12658622860908508, 0.9560166597366333]</td>\n",
       "      <td>[0.24576333165168762, 0.9114999771118164]</td>\n",
       "      <td>0.126586</td>\n",
       "      <td>0.956017</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.13222971558570862, 0.9526333212852478]</td>\n",
       "      <td>[0.24787791073322296, 0.9126999974250793]</td>\n",
       "      <td>0.132230</td>\n",
       "      <td>0.952633</td>\n",
       "      <td>0.247878</td>\n",
       "      <td>0.9127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1417781561613083, 0.948366641998291]</td>\n",
       "      <td>[0.25698399543762207, 0.9108999967575073]</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>0.948367</td>\n",
       "      <td>0.256984</td>\n",
       "      <td>0.9109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1421215534210205, 0.9477333426475525]</td>\n",
       "      <td>[0.24299895763397217, 0.9117000102996826]</td>\n",
       "      <td>0.142122</td>\n",
       "      <td>0.947733</td>\n",
       "      <td>0.242999</td>\n",
       "      <td>0.9117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14446735382080078, 0.948033332824707]</td>\n",
       "      <td>[0.25142455101013184, 0.9111999869346619]</td>\n",
       "      <td>0.144467</td>\n",
       "      <td>0.948033</td>\n",
       "      <td>0.251425</td>\n",
       "      <td>0.9112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1465650051832199, 0.9471666812896729]</td>\n",
       "      <td>[0.24840134382247925, 0.9114999771118164]</td>\n",
       "      <td>0.146565</td>\n",
       "      <td>0.947167</td>\n",
       "      <td>0.248401</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14996056258678436, 0.9464333057403564]</td>\n",
       "      <td>[0.24511033296585083, 0.9093000292778015]</td>\n",
       "      <td>0.149961</td>\n",
       "      <td>0.946433</td>\n",
       "      <td>0.245110</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15062175691127777, 0.9443666934967041]</td>\n",
       "      <td>[0.252878874540329, 0.909500002861023]</td>\n",
       "      <td>0.150622</td>\n",
       "      <td>0.944367</td>\n",
       "      <td>0.252879</td>\n",
       "      <td>0.9095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15237271785736084, 0.9448999762535095]</td>\n",
       "      <td>[0.24747571349143982, 0.911300003528595]</td>\n",
       "      <td>0.152373</td>\n",
       "      <td>0.944900</td>\n",
       "      <td>0.247476</td>\n",
       "      <td>0.9113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1546628773212433, 0.9443833231925964]</td>\n",
       "      <td>[0.27218130230903625, 0.90420001745224]</td>\n",
       "      <td>0.154663</td>\n",
       "      <td>0.944383</td>\n",
       "      <td>0.272181</td>\n",
       "      <td>0.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15656131505966187, 0.9422333240509033]</td>\n",
       "      <td>[0.25688880681991577, 0.9093000292778015]</td>\n",
       "      <td>0.156561</td>\n",
       "      <td>0.942233</td>\n",
       "      <td>0.256889</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15938624739646912, 0.9430500268936157]</td>\n",
       "      <td>[0.2670109272003174, 0.904699981212616]</td>\n",
       "      <td>0.159386</td>\n",
       "      <td>0.943050</td>\n",
       "      <td>0.267011</td>\n",
       "      <td>0.9047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1608797013759613, 0.9412999749183655]</td>\n",
       "      <td>[0.26573246717453003, 0.9079999923706055]</td>\n",
       "      <td>0.160880</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.265732</td>\n",
       "      <td>0.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.16868141293525696, 0.9378499984741211]</td>\n",
       "      <td>[0.26669758558273315, 0.9018999934196472]</td>\n",
       "      <td>0.168681</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.266698</td>\n",
       "      <td>0.9019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1708768606185913, 0.9378499984741211]</td>\n",
       "      <td>[0.2613731920719147, 0.9070000052452087]</td>\n",
       "      <td>0.170877</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.261373</td>\n",
       "      <td>0.9070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.17796066403388977, 0.9342333078384399]</td>\n",
       "      <td>[0.28307250142097473, 0.8967000246047974]</td>\n",
       "      <td>0.177961</td>\n",
       "      <td>0.934233</td>\n",
       "      <td>0.283073</td>\n",
       "      <td>0.8967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18002688884735107, 0.9334666728973389]</td>\n",
       "      <td>[0.2554672062397003, 0.9071999788284302]</td>\n",
       "      <td>0.180027</td>\n",
       "      <td>0.933467</td>\n",
       "      <td>0.255467</td>\n",
       "      <td>0.9072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18178533017635345, 0.9347333312034607]</td>\n",
       "      <td>[0.2796153426170349, 0.9016000032424927]</td>\n",
       "      <td>0.181785</td>\n",
       "      <td>0.934733</td>\n",
       "      <td>0.279615</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1857813447713852, 0.930899977684021]</td>\n",
       "      <td>[0.25688549876213074, 0.9057000279426575]</td>\n",
       "      <td>0.185781</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>0.256885</td>\n",
       "      <td>0.9057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1859435737133026, 0.9332666397094727]</td>\n",
       "      <td>[0.2703552842140198, 0.9021999835968018]</td>\n",
       "      <td>0.185944</td>\n",
       "      <td>0.933267</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.9022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19306862354278564, 0.9285666942596436]</td>\n",
       "      <td>[0.2737598717212677, 0.9014999866485596]</td>\n",
       "      <td>0.193069</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.273760</td>\n",
       "      <td>0.9015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19442103803157806, 0.9269833564758301]</td>\n",
       "      <td>[0.27573519945144653, 0.9016000032424927]</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.926983</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19672514498233795, 0.9282000064849854]</td>\n",
       "      <td>[0.25953343510627747, 0.9054999947547913]</td>\n",
       "      <td>0.196725</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.259533</td>\n",
       "      <td>0.9055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20485873520374298, 0.9259999990463257]</td>\n",
       "      <td>[0.285814106464386, 0.8978000283241272]</td>\n",
       "      <td>0.204859</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.285814</td>\n",
       "      <td>0.8978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20643432438373566, 0.9244666695594788]</td>\n",
       "      <td>[0.2693329453468323, 0.9034000039100647]</td>\n",
       "      <td>0.206434</td>\n",
       "      <td>0.924467</td>\n",
       "      <td>0.269333</td>\n",
       "      <td>0.9034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21107719838619232, 0.9233666658401489]</td>\n",
       "      <td>[0.28365108370780945, 0.8959000110626221]</td>\n",
       "      <td>0.211077</td>\n",
       "      <td>0.923367</td>\n",
       "      <td>0.283651</td>\n",
       "      <td>0.8959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21186839044094086, 0.9207500219345093]</td>\n",
       "      <td>[0.2745698392391205, 0.8982999920845032]</td>\n",
       "      <td>0.211868</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274570</td>\n",
       "      <td>0.8983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21291086077690125, 0.9204333424568176]</td>\n",
       "      <td>[0.27476274967193604, 0.8995000123977661]</td>\n",
       "      <td>0.212911</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.274763</td>\n",
       "      <td>0.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2132299542427063, 0.9207500219345093]</td>\n",
       "      <td>[0.27428877353668213, 0.9002000093460083]</td>\n",
       "      <td>0.213230</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274289</td>\n",
       "      <td>0.9002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21326030790805817, 0.9208333492279053]</td>\n",
       "      <td>[0.30138877034187317, 0.8938000202178955]</td>\n",
       "      <td>0.213260</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.301389</td>\n",
       "      <td>0.8938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21572570502758026, 0.9204333424568176]</td>\n",
       "      <td>[0.2723506689071655, 0.8992999792098999]</td>\n",
       "      <td>0.215726</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.272351</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21686747670173645, 0.9199833273887634]</td>\n",
       "      <td>[0.2744320034980774, 0.902400016784668]</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.919983</td>\n",
       "      <td>0.274432</td>\n",
       "      <td>0.9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22014310956001282, 0.9203000068664551]</td>\n",
       "      <td>[0.2999141812324524, 0.891700029373169]</td>\n",
       "      <td>0.220143</td>\n",
       "      <td>0.920300</td>\n",
       "      <td>0.299914</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22073142230510712, 0.9183333516120911]</td>\n",
       "      <td>[0.2996402978897095, 0.8896999955177307]</td>\n",
       "      <td>0.220731</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>0.299640</td>\n",
       "      <td>0.8897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22205980122089386, 0.92166668176651]</td>\n",
       "      <td>[0.28667232394218445, 0.8970999717712402]</td>\n",
       "      <td>0.222060</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>0.286672</td>\n",
       "      <td>0.8971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22614991664886475, 0.9175000190734863]</td>\n",
       "      <td>[0.3019619286060333, 0.8912000060081482]</td>\n",
       "      <td>0.226150</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.301962</td>\n",
       "      <td>0.8912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23273038864135742, 0.9152833223342896]</td>\n",
       "      <td>[0.2840319871902466, 0.894599974155426]</td>\n",
       "      <td>0.232730</td>\n",
       "      <td>0.915283</td>\n",
       "      <td>0.284032</td>\n",
       "      <td>0.8946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23349125683307648, 0.9122499823570251]</td>\n",
       "      <td>[0.287698894739151, 0.8937000036239624]</td>\n",
       "      <td>0.233491</td>\n",
       "      <td>0.912250</td>\n",
       "      <td>0.287699</td>\n",
       "      <td>0.8937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23593302071094513, 0.9116500020027161]</td>\n",
       "      <td>[0.29616788029670715, 0.8928999900817871]</td>\n",
       "      <td>0.235933</td>\n",
       "      <td>0.911650</td>\n",
       "      <td>0.296168</td>\n",
       "      <td>0.8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23643141984939575, 0.9130666851997375]</td>\n",
       "      <td>[0.30026862025260925, 0.8944000005722046]</td>\n",
       "      <td>0.236431</td>\n",
       "      <td>0.913067</td>\n",
       "      <td>0.300269</td>\n",
       "      <td>0.8944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23679600656032562, 0.9127500057220459]</td>\n",
       "      <td>[0.28981828689575195, 0.8930000066757202]</td>\n",
       "      <td>0.236796</td>\n",
       "      <td>0.912750</td>\n",
       "      <td>0.289818</td>\n",
       "      <td>0.8930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.24779362976551056, 0.9068999886512756]</td>\n",
       "      <td>[0.2994548976421356, 0.8888999819755554]</td>\n",
       "      <td>0.247794</td>\n",
       "      <td>0.906900</td>\n",
       "      <td>0.299455</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.25457531213760376, 0.90829998254776]</td>\n",
       "      <td>[0.3104911148548126, 0.891700029373169]</td>\n",
       "      <td>0.254575</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>0.310491</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2588706910610199, 0.9059000015258789]</td>\n",
       "      <td>[0.3161528706550598, 0.8884999752044678]</td>\n",
       "      <td>0.258871</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.316153</td>\n",
       "      <td>0.8885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2675584852695465, 0.9041666388511658]</td>\n",
       "      <td>[0.3267644941806793, 0.8819000124931335]</td>\n",
       "      <td>0.267558</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>0.326764</td>\n",
       "      <td>0.8819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2692805528640747, 0.9014833569526672]</td>\n",
       "      <td>[0.32393553853034973, 0.8855999708175659]</td>\n",
       "      <td>0.269281</td>\n",
       "      <td>0.901483</td>\n",
       "      <td>0.323936</td>\n",
       "      <td>0.8856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.27241677045822144, 0.9009333252906799]</td>\n",
       "      <td>[0.31941139698028564, 0.8870999813079834]</td>\n",
       "      <td>0.272417</td>\n",
       "      <td>0.900933</td>\n",
       "      <td>0.319411</td>\n",
       "      <td>0.8871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.29450979828834534, 0.8953333497047424]</td>\n",
       "      <td>[0.3364871144294739, 0.8776999711990356]</td>\n",
       "      <td>0.294510</td>\n",
       "      <td>0.895333</td>\n",
       "      <td>0.336487</td>\n",
       "      <td>0.8777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  \\\n",
       "9   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "4   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "2   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "10  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "33  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "6   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "11  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "3   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "35  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "30  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "29  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "1   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "28  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "0   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "7   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "25  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "34  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "31  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "5   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "24  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "16  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "27  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "21  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "32  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "22  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "26  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "15  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "46  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "45  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "37  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "17  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "40  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "42  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "13  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "8   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "36  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "23  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "41  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "12  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "18  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "47  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "39  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "43  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "38  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "20  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "44  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "14  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "19  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "\n",
       "                                   train_eval  \\\n",
       "9   [0.12658622860908508, 0.9560166597366333]   \n",
       "4   [0.13222971558570862, 0.9526333212852478]   \n",
       "2     [0.1417781561613083, 0.948366641998291]   \n",
       "10   [0.1421215534210205, 0.9477333426475525]   \n",
       "33   [0.14446735382080078, 0.948033332824707]   \n",
       "6    [0.1465650051832199, 0.9471666812896729]   \n",
       "11  [0.14996056258678436, 0.9464333057403564]   \n",
       "3   [0.15062175691127777, 0.9443666934967041]   \n",
       "35  [0.15237271785736084, 0.9448999762535095]   \n",
       "30   [0.1546628773212433, 0.9443833231925964]   \n",
       "29  [0.15656131505966187, 0.9422333240509033]   \n",
       "1   [0.15938624739646912, 0.9430500268936157]   \n",
       "28   [0.1608797013759613, 0.9412999749183655]   \n",
       "0   [0.16868141293525696, 0.9378499984741211]   \n",
       "7    [0.1708768606185913, 0.9378499984741211]   \n",
       "25  [0.17796066403388977, 0.9342333078384399]   \n",
       "34  [0.18002688884735107, 0.9334666728973389]   \n",
       "31  [0.18178533017635345, 0.9347333312034607]   \n",
       "5     [0.1857813447713852, 0.930899977684021]   \n",
       "24   [0.1859435737133026, 0.9332666397094727]   \n",
       "16  [0.19306862354278564, 0.9285666942596436]   \n",
       "27  [0.19442103803157806, 0.9269833564758301]   \n",
       "21  [0.19672514498233795, 0.9282000064849854]   \n",
       "32  [0.20485873520374298, 0.9259999990463257]   \n",
       "22  [0.20643432438373566, 0.9244666695594788]   \n",
       "26  [0.21107719838619232, 0.9233666658401489]   \n",
       "15  [0.21186839044094086, 0.9207500219345093]   \n",
       "46  [0.21291086077690125, 0.9204333424568176]   \n",
       "45   [0.2132299542427063, 0.9207500219345093]   \n",
       "37  [0.21326030790805817, 0.9208333492279053]   \n",
       "17  [0.21572570502758026, 0.9204333424568176]   \n",
       "40  [0.21686747670173645, 0.9199833273887634]   \n",
       "42  [0.22014310956001282, 0.9203000068664551]   \n",
       "13  [0.22073142230510712, 0.9183333516120911]   \n",
       "8     [0.22205980122089386, 0.92166668176651]   \n",
       "36  [0.22614991664886475, 0.9175000190734863]   \n",
       "23  [0.23273038864135742, 0.9152833223342896]   \n",
       "41  [0.23349125683307648, 0.9122499823570251]   \n",
       "12  [0.23593302071094513, 0.9116500020027161]   \n",
       "18  [0.23643141984939575, 0.9130666851997375]   \n",
       "47  [0.23679600656032562, 0.9127500057220459]   \n",
       "39  [0.24779362976551056, 0.9068999886512756]   \n",
       "43    [0.25457531213760376, 0.90829998254776]   \n",
       "38   [0.2588706910610199, 0.9059000015258789]   \n",
       "20   [0.2675584852695465, 0.9041666388511658]   \n",
       "44   [0.2692805528640747, 0.9014833569526672]   \n",
       "14  [0.27241677045822144, 0.9009333252906799]   \n",
       "19  [0.29450979828834534, 0.8953333497047424]   \n",
       "\n",
       "                                    test_eval  train_loss  train_acc  \\\n",
       "9   [0.24576333165168762, 0.9114999771118164]    0.126586   0.956017   \n",
       "4   [0.24787791073322296, 0.9126999974250793]    0.132230   0.952633   \n",
       "2   [0.25698399543762207, 0.9108999967575073]    0.141778   0.948367   \n",
       "10  [0.24299895763397217, 0.9117000102996826]    0.142122   0.947733   \n",
       "33  [0.25142455101013184, 0.9111999869346619]    0.144467   0.948033   \n",
       "6   [0.24840134382247925, 0.9114999771118164]    0.146565   0.947167   \n",
       "11  [0.24511033296585083, 0.9093000292778015]    0.149961   0.946433   \n",
       "3      [0.252878874540329, 0.909500002861023]    0.150622   0.944367   \n",
       "35   [0.24747571349143982, 0.911300003528595]    0.152373   0.944900   \n",
       "30    [0.27218130230903625, 0.90420001745224]    0.154663   0.944383   \n",
       "29  [0.25688880681991577, 0.9093000292778015]    0.156561   0.942233   \n",
       "1     [0.2670109272003174, 0.904699981212616]    0.159386   0.943050   \n",
       "28  [0.26573246717453003, 0.9079999923706055]    0.160880   0.941300   \n",
       "0   [0.26669758558273315, 0.9018999934196472]    0.168681   0.937850   \n",
       "7    [0.2613731920719147, 0.9070000052452087]    0.170877   0.937850   \n",
       "25  [0.28307250142097473, 0.8967000246047974]    0.177961   0.934233   \n",
       "34   [0.2554672062397003, 0.9071999788284302]    0.180027   0.933467   \n",
       "31   [0.2796153426170349, 0.9016000032424927]    0.181785   0.934733   \n",
       "5   [0.25688549876213074, 0.9057000279426575]    0.185781   0.930900   \n",
       "24   [0.2703552842140198, 0.9021999835968018]    0.185944   0.933267   \n",
       "16   [0.2737598717212677, 0.9014999866485596]    0.193069   0.928567   \n",
       "27  [0.27573519945144653, 0.9016000032424927]    0.194421   0.926983   \n",
       "21  [0.25953343510627747, 0.9054999947547913]    0.196725   0.928200   \n",
       "32    [0.285814106464386, 0.8978000283241272]    0.204859   0.926000   \n",
       "22   [0.2693329453468323, 0.9034000039100647]    0.206434   0.924467   \n",
       "26  [0.28365108370780945, 0.8959000110626221]    0.211077   0.923367   \n",
       "15   [0.2745698392391205, 0.8982999920845032]    0.211868   0.920750   \n",
       "46  [0.27476274967193604, 0.8995000123977661]    0.212911   0.920433   \n",
       "45  [0.27428877353668213, 0.9002000093460083]    0.213230   0.920750   \n",
       "37  [0.30138877034187317, 0.8938000202178955]    0.213260   0.920833   \n",
       "17   [0.2723506689071655, 0.8992999792098999]    0.215726   0.920433   \n",
       "40    [0.2744320034980774, 0.902400016784668]    0.216867   0.919983   \n",
       "42    [0.2999141812324524, 0.891700029373169]    0.220143   0.920300   \n",
       "13   [0.2996402978897095, 0.8896999955177307]    0.220731   0.918333   \n",
       "8   [0.28667232394218445, 0.8970999717712402]    0.222060   0.921667   \n",
       "36   [0.3019619286060333, 0.8912000060081482]    0.226150   0.917500   \n",
       "23    [0.2840319871902466, 0.894599974155426]    0.232730   0.915283   \n",
       "41    [0.287698894739151, 0.8937000036239624]    0.233491   0.912250   \n",
       "12  [0.29616788029670715, 0.8928999900817871]    0.235933   0.911650   \n",
       "18  [0.30026862025260925, 0.8944000005722046]    0.236431   0.913067   \n",
       "47  [0.28981828689575195, 0.8930000066757202]    0.236796   0.912750   \n",
       "39   [0.2994548976421356, 0.8888999819755554]    0.247794   0.906900   \n",
       "43    [0.3104911148548126, 0.891700029373169]    0.254575   0.908300   \n",
       "38   [0.3161528706550598, 0.8884999752044678]    0.258871   0.905900   \n",
       "20   [0.3267644941806793, 0.8819000124931335]    0.267558   0.904167   \n",
       "44  [0.32393553853034973, 0.8855999708175659]    0.269281   0.901483   \n",
       "14  [0.31941139698028564, 0.8870999813079834]    0.272417   0.900933   \n",
       "19   [0.3364871144294739, 0.8776999711990356]    0.294510   0.895333   \n",
       "\n",
       "    test_loss  test_acc  \n",
       "9    0.245763    0.9115  \n",
       "4    0.247878    0.9127  \n",
       "2    0.256984    0.9109  \n",
       "10   0.242999    0.9117  \n",
       "33   0.251425    0.9112  \n",
       "6    0.248401    0.9115  \n",
       "11   0.245110    0.9093  \n",
       "3    0.252879    0.9095  \n",
       "35   0.247476    0.9113  \n",
       "30   0.272181    0.9042  \n",
       "29   0.256889    0.9093  \n",
       "1    0.267011    0.9047  \n",
       "28   0.265732    0.9080  \n",
       "0    0.266698    0.9019  \n",
       "7    0.261373    0.9070  \n",
       "25   0.283073    0.8967  \n",
       "34   0.255467    0.9072  \n",
       "31   0.279615    0.9016  \n",
       "5    0.256885    0.9057  \n",
       "24   0.270355    0.9022  \n",
       "16   0.273760    0.9015  \n",
       "27   0.275735    0.9016  \n",
       "21   0.259533    0.9055  \n",
       "32   0.285814    0.8978  \n",
       "22   0.269333    0.9034  \n",
       "26   0.283651    0.8959  \n",
       "15   0.274570    0.8983  \n",
       "46   0.274763    0.8995  \n",
       "45   0.274289    0.9002  \n",
       "37   0.301389    0.8938  \n",
       "17   0.272351    0.8993  \n",
       "40   0.274432    0.9024  \n",
       "42   0.299914    0.8917  \n",
       "13   0.299640    0.8897  \n",
       "8    0.286672    0.8971  \n",
       "36   0.301962    0.8912  \n",
       "23   0.284032    0.8946  \n",
       "41   0.287699    0.8937  \n",
       "12   0.296168    0.8929  \n",
       "18   0.300269    0.8944  \n",
       "47   0.289818    0.8930  \n",
       "39   0.299455    0.8889  \n",
       "43   0.310491    0.8917  \n",
       "38   0.316153    0.8885  \n",
       "20   0.326764    0.8819  \n",
       "44   0.323936    0.8856  \n",
       "14   0.319411    0.8871  \n",
       "19   0.336487    0.8777  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.sort_values(by=['train_loss'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, model         <tensorflow.python.keras.engine.sequential.Seq...\n",
      "train_eval             [0.1421215534210205, 0.9477333426475525]\n",
      "test_eval             [0.24299895763397217, 0.9117000102996826]\n",
      "train_loss                                             0.142122\n",
      "train_acc                                              0.947733\n",
      "test_loss                                              0.242999\n",
      "test_acc                                                 0.9117\n",
      "Name: 10, dtype: object)\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 25, 25, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 23, 23, 12)        660       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 22, 22, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 120)               697080    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 10)                610       \n",
      "=================================================================\n",
      "Total params: 705,670\n",
      "Trainable params: 705,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'kernel_size': 3, 'max_pool_size': 2, 'pool_strides': 1, 'filters': (6, 12), 'dense_size': (120, 60), 'learning_rate': 0.0005, 'dropout_d': 0.2, 'batch_size': 250, 'epochs': 100, 'shuffle': True, 'model_name': 'model-10', 'callbacks': [[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x00000217884E49B0>]], 'test_images': array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]]), 'test_labels': array([9, 2, 1, ..., 8, 1, 5], dtype=uint8), 'train_images': array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]]), 'train_labels': array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)}\n",
      "(9, model         <tensorflow.python.keras.engine.sequential.Seq...\n",
      "train_eval            [0.12658622860908508, 0.9560166597366333]\n",
      "test_eval             [0.24576333165168762, 0.9114999771118164]\n",
      "train_loss                                             0.126586\n",
      "train_acc                                              0.956017\n",
      "test_loss                                              0.245763\n",
      "test_acc                                                 0.9115\n",
      "Name: 9, dtype: object)\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 25, 25, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 23, 23, 12)        660       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 22, 22, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 120)               697080    \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 60)                0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 10)                610       \n",
      "=================================================================\n",
      "Total params: 705,670\n",
      "Trainable params: 705,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'kernel_size': 3, 'max_pool_size': 2, 'pool_strides': 1, 'filters': (6, 12), 'dense_size': (120, 60), 'learning_rate': 0.0005, 'dropout_d': 0.2, 'batch_size': 100, 'epochs': 100, 'shuffle': True, 'model_name': 'model-9', 'callbacks': [[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x00000217884E49B0>]], 'test_images': array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]]), 'test_labels': array([9, 2, 1, ..., 8, 1, 5], dtype=uint8), 'train_images': array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]]), 'train_labels': array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)}\n",
      "(4, model         <tensorflow.python.keras.engine.sequential.Seq...\n",
      "train_eval            [0.13222971558570862, 0.9526333212852478]\n",
      "test_eval             [0.24787791073322296, 0.9126999974250793]\n",
      "train_loss                                              0.13223\n",
      "train_acc                                              0.952633\n",
      "test_loss                                              0.247878\n",
      "test_acc                                                 0.9127\n",
      "Name: 4, dtype: object)\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 25, 25, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 23, 23, 12)        660       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 22, 22, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 120)               697080    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 10)                610       \n",
      "=================================================================\n",
      "Total params: 705,670\n",
      "Trainable params: 705,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'kernel_size': 3, 'max_pool_size': 2, 'pool_strides': 1, 'filters': (6, 12), 'dense_size': (120, 60), 'learning_rate': 0.001, 'dropout_d': 0.2, 'batch_size': 250, 'epochs': 100, 'shuffle': True, 'model_name': 'model-4', 'callbacks': [[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x00000217884E49B0>]], 'test_images': array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]]), 'test_labels': array([9, 2, 1, ..., 8, 1, 5], dtype=uint8), 'train_images': array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]]), 'train_labels': array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)}\n"
     ]
    }
   ],
   "source": [
    "# Chosen based on evaluation of the above losses and accuracy\n",
    "# However, all models are saved elsewhere (for now) so they can be grabbed\n",
    "best_models = [10, 9, 4]\n",
    "best_df = all_df.iloc[best_models]\n",
    "best_df\n",
    "for row in best_df.iterrows():\n",
    "    print(row)\n",
    "    print(row[1]['model'].summary())\n",
    "    print(full_params[row[0]])\n",
    "    row[1]['model'].save(f'./saved_models/tf-model-{row[0]}.h5')\n",
    "\n",
    "# print(full_params[*best_models])\n",
    "# for m in best_models:\n",
    "#     print(f\"model {m}\")\n",
    "#     print(f\"metrics: {all_df[[\"train_loss\",\"train_acc\",\"test_loss\",\"test_acc\"]][m]}\")\n",
    "#     print(all_df['model'][m].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\gibso\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\gibso\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\pool.py\", line 47, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"C:\\Users\\gibso\\Documents\\fashion-classification\\model\\build_and_fit_model.py\", line 34, in build_and_fit_model\n    model.add(keras.layers.Conv2D(filters=filters[0], kernel_size=kernel_size, activation='relu', input_shape=(28, 28,1), data_format='channels_last'))\n  File \"C:\\Users\\gibso\\Documents\\fashion-classification\\env\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 599, in __init__\n    **kwargs)\n  File \"C:\\Users\\gibso\\Documents\\fashion-classification\\env\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 128, in __init__\n    filters = int(filters)\nValueError: invalid literal for int() with base 10: 's'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-870fa4ecbc44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Reserve 1 processor for the os so it doesn't die\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdata_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_and_fit_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# To make sure processes are closed in the end, even if errors happen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         '''\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    642\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 's'"
     ]
    }
   ],
   "source": [
    "# I want to go to bed so I'm just gonna brute force it and not worry about multiprocessing for now\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "# try:\n",
    "#     pool = Pool(os.cpu_count()-1) # Reserve 1 processor for the os so it doesn't die\n",
    "#     data_outputs = pool.starmap(build_and_fit_model, full_params)\n",
    "# finally: # To make sure processes are closed in the end, even if errors happen\n",
    "#     pool.close()\n",
    "#     pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f-c",
   "language": "python",
   "name": "f-c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
