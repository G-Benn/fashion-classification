{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with Tensorflow to build the same predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rebuild the same model as the PyTorch one and see if they perform any different\n",
    "2. Try more layers, different parameters, etc to learn a bit more\n",
    "3. Stick in a GridSearchCV pipeline to see how it does with getting a better version\n",
    "4. Save out the models as before to use in an application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial code setup comes from the [Tensorflow Tutorial](https://www.tensorflow.org/tutorials/keras/classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the data - in this case it's scaling each of the pixel values from 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 25 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[train_labels[i]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example model is below - I'll be expanding on it later in the notebook (turning it into a CNN, experimenting with different layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.5015 - accuracy: 0.8241\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3788 - accuracy: 0.8643\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3370 - accuracy: 0.8772\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3131 - accuracy: 0.8848\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2935 - accuracy: 0.8918\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2817 - accuracy: 0.8954\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2689 - accuracy: 0.9008\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2568 - accuracy: 0.9047\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2477 - accuracy: 0.9075\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2393 - accuracy: 0.9117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17608985e10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.3351 - accuracy: 0.8844\n",
      "\n",
      "Test accuracy: 0.8844000101089478\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
    "\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "\n",
    "# for overfitting try a little L@ regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_model = tf.keras.Sequential([model, \n",
    "                                         tf.keras.layers.Softmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = probability_model.predict(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(i, predictions_array, true_label, img):\n",
    "    predictions_array, true_label, img = predictions_array, true_label[i], img[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "    if predicted_label == true_label:\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        color = 'red'\n",
    "\n",
    "    plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
    "                                100*np.max(predictions_array),\n",
    "                                class_names[true_label]),\n",
    "                                color=color)\n",
    "\n",
    "def plot_value_array(i, predictions_array, true_label):\n",
    "    predictions_array, true_label = predictions_array, true_label[i]\n",
    "    plt.grid(False)\n",
    "    plt.xticks(range(10))\n",
    "    plt.yticks([])\n",
    "    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
    "    plt.ylim([0, 1])\n",
    "    predicted_label = np.argmax(predictions_array)\n",
    "\n",
    "    thisplot[predicted_label].set_color('red')\n",
    "    thisplot[true_label].set_color('blue')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 0\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plot_image(i, predictions[i], test_labels, test_images)\n",
    "plt.subplot(1,2,2)\n",
    "plot_value_array(i, predictions[i],  test_labels)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the first X test images, their predicted labels, and the true labels.\n",
    "# Color correct predictions in blue and incorrect predictions in red.\n",
    "num_rows = 5\n",
    "num_cols = 3\n",
    "num_images = num_rows*num_cols\n",
    "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
    "for i in range(num_images):\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
    "  plot_image(i, predictions[i], test_labels, test_images)\n",
    "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
    "  plot_value_array(i, predictions[i], test_labels)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "(1, 28, 28)\n",
      "[[1.8109781e-06 6.0059138e-15 9.9838865e-01 4.1913273e-10 1.4869933e-03\n",
      "  5.4349437e-15 1.2260168e-04 1.2546644e-16 8.3540631e-12 4.7977166e-13]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Grab an image from the test dataset.\n",
    "img = test_images[1]\n",
    "\n",
    "print(img.shape)\n",
    "# Add the image to a batch where it's the only member.\n",
    "img = (np.expand_dims(img,0))\n",
    "\n",
    "print(img.shape)\n",
    "predictions_single = probability_model.predict(img)\n",
    "\n",
    "print(predictions_single)\n",
    "plot_value_array(1, predictions_single[0], test_labels)\n",
    "_ = plt.xticks(range(10), class_names, rotation=45)\n",
    "np.argmax(predictions_single[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape((train_images.shape[0], 28, 28, 1))\n",
    "test_images = test_images.reshape((test_images.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 12)          1812      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 12)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                610       \n",
      "=================================================================\n",
      "Total params: 32,998\n",
      "Trainable params: 32,998\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Mimic the format of the structure of the PyTorch model\n",
    "batch_size = 100\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(filters=6, kernel_size=5, activation='relu', input_shape=(28, 28,1), data_format='channels_last'))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "model.add(keras.layers.Conv2D(filters=12, kernel_size=5, activation='relu'))# input_shape=(6,12, 12)))\n",
    "model.add(keras.layers.MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(120, activation='relu'))#, input_shape=(12*4*4,)))\n",
    "model.add(keras.layers.Dense(60, activation='relu'))\n",
    "model.add(keras.layers.Dense(10))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out because I had to restart the kernel and didn't want to rerun this\n",
    "# model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit(train_images,\n",
    "#          train_labels,\n",
    "#          batch_size=500,\n",
    "#          epochs=100,\n",
    "#          shuffle=True,\n",
    "#          validation_data=(test_images, test_labels))\n",
    "# # It's overfit, but I'll include overfitting on the below one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out because I had to restart the kernel and didn't want to rerun this\n",
    "# model.save('saved_models/tf-mimic-3.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that the testing is done, lets recreate the model in a way we can use it with scikit-learn and GridSearchCV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper function for the Keras CNN that will allow easier experimentation with GridSearchCV\n",
    "def build_classifier(kernel_size=5, # kernel size of the Conv layers\n",
    "                     max_pool_size=2, #Size of the MaxPool layers\n",
    "                     pool_strides=2, # Stride of the MaxPool layers\n",
    "                     filters=(6,12), #Filter size\n",
    "                     dense_size=(120,60), #Size of the dense layers\n",
    "                     learning_rate=0.001, # Learnig rate for Adam\n",
    "                     dropout_d=0.0 # dropout rate in the dense layers\n",
    "                    ): \n",
    "    \n",
    "    # Starting model\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Convolutional / pooling layers\n",
    "    model.add(keras.layers.Conv2D(filters=filters[0], kernel_size=kernel_size, activation='relu', input_shape=(28, 28,1), data_format='channels_last'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=max_pool_size, strides=pool_strides))\n",
    "    model.add(keras.layers.Conv2D(filters=[1], kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=max_pool_size, strides=pool_strides))\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(dropout_d))\n",
    "    model.add(keras.layers.Dense(dense_size[0], activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_d))\n",
    "    model.add(keras.layers.Dense(dense_size[1], activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_d))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(10))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "             metrics='accuracy'\n",
    "                 )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_callback = keras.callbacks.EarlyStopping(\n",
    "  monitor='val_loss',\n",
    "  min_delta=0.0001,\n",
    "  patience=3,\n",
    "  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"kernel_size\": [3,5],\n",
    "    \"max_pool_size\": [2,3],\n",
    "    \"pool_strides\": [1,2],\n",
    "    \"filters\": [(6,12),(4,8)],\n",
    "    \"dense_size\": [(120,60), (100,50)],\n",
    "    \"learning_rate\": [0.005,0.001, 0.0005],\n",
    "    \"dropout_d\": [0.0, 0.2],\n",
    "    \"batch_size\": [100, 250, 500],\n",
    "    \"epochs\": [100]\n",
    "#     \"shuffle\": [True],\n",
    "#     \"callbacks\": [[earlystop_callback]],\n",
    "#     \"validation_data\": [(test_images, test_labels)]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this with the default args, and those that should be overwritten with the param_grid will be (I think)\n",
    "keras_model = KerasClassifier(\n",
    "    build_fn=build_classifier,\n",
    "    \n",
    "    # The param_grid args to the build function\n",
    "    kernel_size=5, # kernel size of the Conv layers\n",
    "    max_pool_size=2, #Size of the MaxPool layers\n",
    "    pool_strides=2, # Stride of the MaxPool layers\n",
    "    filters=(6,12), #Filter size\n",
    "    dense_size=(120,60), #Size of the dense layers\n",
    "    learning_rate=0.001, # Learnig rate for Adam\n",
    "    dropout_d=0.0, # dropout rate in the dense layers\n",
    "    \n",
    "    # The param_grid args to the fit function\n",
    "    epochs = 50, # This should really be kept under control by the early stopping\n",
    "    batch_size=100,\n",
    "    \n",
    "    # Other single args to the fit function\n",
    "    shuffle=True, # Shuffle the traiing data\n",
    "    callbacks=[earlystop_callback], # Utilize early stopping as defined above\n",
    "    validation_data=(test_images, test_labels) # Use test data as validation data\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator = keras_model,\n",
    "    param_grid = param_grid,\n",
    "    # Scoring will just be the default from the Keras model, aka Categorical cross entropy + accuracy\n",
    "    cv = 3,\n",
    "    n_jobs = -1,\n",
    "    verbose = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_search.fit(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.2'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issue Summary:\n",
    "There's a [known](https://github.com/keras-team/keras/issues/13586) [bug](https://github.com/scikit-learn/scikit-learn/issues/15722) That for me for whatever reason isn't fixed by reverting the sklearn version. Instead, what I'll be doing is running a knockoff of a gridsearch, without CV (but with teh validation set) of teh parameter sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_callback = keras.callbacks.EarlyStopping(\n",
    "  monitor='val_loss',\n",
    "  min_delta=0.0001,\n",
    "  patience=3,\n",
    "  restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The below cell is definitely duplicating the train/test images and labels, so just pass those in separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "param_grid = {\n",
    "    \"kernel_size\": [3,5],\n",
    "    \"max_pool_size\": [2],\n",
    "    \"pool_strides\": [1,2],\n",
    "    \"filters\": [(6,12)],\n",
    "    \"dense_size\": [(120,60)],\n",
    "    \"learning_rate\": [0.001, 0.0005],\n",
    "    \"dropout_d\": [0.0, 0.2],\n",
    "    \"batch_size\": [100, 250, 500],\n",
    "    \"epochs\": [100],\n",
    "    \"shuffle\": [True]\n",
    "}\n",
    "single_params = {\n",
    "    # Since none of these change, just pass them in independently\n",
    "    \"callbacks\": [[earlystop_callback]],\n",
    "    \"test_images\": test_images,\n",
    "    \"test_labels\": test_labels,\n",
    "    \"train_images\": train_images,\n",
    "    \"train_labels\": train_labels\n",
    "}\n",
    "\n",
    "keys, values = zip(*param_grid.items())\n",
    "permutations_dicts = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "for idx, d in enumerate(permutations_dicts):\n",
    "    name = \"model-\"+str(idx)\n",
    "    d[\"model_name\"] = name\n",
    "\n",
    "len(permutations_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because of the way pool works, have to define this in a separate function then import it\n",
    "def build_and_fit_model(\n",
    "                        # Positional, previously created objects\n",
    "                        model_name=\"nonegiven\",\n",
    "                        train_images=None,\n",
    "                        train_labels=None,\n",
    "                        test_images=None,\n",
    "                        test_labels=None,\n",
    "                        callbacks=None,\n",
    "                        kernel_size=5, # kernel size of the Conv layers\n",
    "                        max_pool_size=2, #Size of the MaxPool layers\n",
    "                        pool_strides=2, # Stride of the MaxPool layers\n",
    "                        filters=(6,12), #Filter size\n",
    "                        dense_size=(120,60), #Size of the dense layers\n",
    "                        learning_rate=0.001, # Learnig rate for Adam\n",
    "                        dropout_d=0.0, # dropout rate in the dense layers\n",
    "                        batch_size=500,\n",
    "                        shuffle=True,\n",
    "                        epochs=100\n",
    "                    ): \n",
    "    \n",
    "    # Starting model\n",
    "    model = keras.models.Sequential()\n",
    "    \n",
    "    # Convolutional / pooling layers\n",
    "    model.add(keras.layers.Conv2D(filters=filters[0], kernel_size=kernel_size, activation='relu', input_shape=(28, 28,1), data_format='channels_last'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=max_pool_size, strides=pool_strides))\n",
    "    model.add(keras.layers.Conv2D(filters=filters[1], kernel_size=kernel_size, activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=max_pool_size, strides=pool_strides))\n",
    "\n",
    "    # Dense layers\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dropout(dropout_d))\n",
    "    model.add(keras.layers.Dense(dense_size[0], activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_d))\n",
    "    model.add(keras.layers.Dense(dense_size[1], activation='relu'))\n",
    "    model.add(keras.layers.Dropout(dropout_d))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(10))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "             metrics='accuracy'\n",
    "                 )\n",
    "    \n",
    "    # Fit the model\n",
    "    history = model.fit(train_images,\n",
    "         train_labels,\n",
    "         batch_size=batch_size,\n",
    "         epochs=epochs,\n",
    "         shuffle=shuffle,\n",
    "         callbacks=callbacks,\n",
    "         validation_data=(test_images, test_labels))\n",
    "    \n",
    "    #save the model and the parameters of the model to files\n",
    "    path=\"./saved_models/fake-gridsearch-saves/\" + model_name\n",
    "    model.save(f'{path}.h5')\n",
    "    evals = model.evaluate(test_images, test_labels)\n",
    "    \n",
    "    with open('./saved_models/fake-gridsearch-saves/results-tf.txt', 'a') as f:\n",
    "        f.write(f\"{model_name}: {evals}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from build_and_fit_model import build_and_fit_model\n",
    "# build_and_fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params = permutations_dicts[0:2]\n",
    "full_params = [{**x, **single_params} for x in permutations_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(full_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel_size': 3,\n",
       " 'max_pool_size': 2,\n",
       " 'pool_strides': 1,\n",
       " 'filters': (6, 12),\n",
       " 'dense_size': (120, 60),\n",
       " 'learning_rate': 0.001,\n",
       " 'dropout_d': 0.2,\n",
       " 'batch_size': 250,\n",
       " 'epochs': 100,\n",
       " 'shuffle': True,\n",
       " 'model_name': 'model-4',\n",
       " 'callbacks': [[<tensorflow.python.keras.callbacks.EarlyStopping at 0x1760900ce10>]],\n",
       " 'test_images': array([[[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]]]),\n",
       " 'test_labels': array([9, 2, 1, ..., 8, 1, 5], dtype=uint8),\n",
       " 'train_images': array([[[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        ...,\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]],\n",
       " \n",
       " \n",
       "        [[[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]],\n",
       " \n",
       "         [[0.],\n",
       "          [0.],\n",
       "          [0.],\n",
       "          ...,\n",
       "          [0.],\n",
       "          [0.],\n",
       "          [0.]]]]),\n",
       " 'train_labels': array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_params[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_and_fit_model(**test_params, **single_params)\n",
    "# Works great on a single one! now to get it done with multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "600/600 [==============================] - 32s 53ms/step - loss: 0.4705 - accuracy: 0.8303 - val_loss: 0.3685 - val_accuracy: 0.8678\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 32s 53ms/step - loss: 0.3180 - accuracy: 0.8843 - val_loss: 0.3335 - val_accuracy: 0.8789\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 32s 53ms/step - loss: 0.2715 - accuracy: 0.9004 - val_loss: 0.2899 - val_accuracy: 0.8922\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2427 - accuracy: 0.9100 - val_loss: 0.2833 - val_accuracy: 0.8974\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 32s 53ms/step - loss: 0.2151 - accuracy: 0.9197 - val_loss: 0.2917 - val_accuracy: 0.8940\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1946 - accuracy: 0.9276 - val_loss: 0.2667 - val_accuracy: 0.9019\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1749 - accuracy: 0.9339 - val_loss: 0.2706 - val_accuracy: 0.9070\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1582 - accuracy: 0.9404 - val_loss: 0.2785 - val_accuracy: 0.9050\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1423 - accuracy: 0.9467 - val_loss: 0.2786 - val_accuracy: 0.9062\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2667 - accuracy: 0.9019\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 29s 120ms/step - loss: 0.5183 - accuracy: 0.8157 - val_loss: 0.3943 - val_accuracy: 0.8617\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 28s 119ms/step - loss: 0.3222 - accuracy: 0.8849 - val_loss: 0.3459 - val_accuracy: 0.8777\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 29s 120ms/step - loss: 0.2800 - accuracy: 0.8977 - val_loss: 0.3044 - val_accuracy: 0.8878\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 29s 122ms/step - loss: 0.2506 - accuracy: 0.9085 - val_loss: 0.2965 - val_accuracy: 0.8918\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 28s 117ms/step - loss: 0.2310 - accuracy: 0.9136 - val_loss: 0.2690 - val_accuracy: 0.9018\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.2123 - accuracy: 0.9212 - val_loss: 0.2753 - val_accuracy: 0.9028\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.1972 - accuracy: 0.9274 - val_loss: 0.2764 - val_accuracy: 0.9049\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.1851 - accuracy: 0.9305 - val_loss: 0.2670 - val_accuracy: 0.9047\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.1648 - accuracy: 0.9392 - val_loss: 0.2741 - val_accuracy: 0.9011\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.1537 - accuracy: 0.9424 - val_loss: 0.2855 - val_accuracy: 0.9041\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.1407 - accuracy: 0.9478 - val_loss: 0.2814 - val_accuracy: 0.9025\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2670 - accuracy: 0.9047\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 219ms/step - loss: 0.6167 - accuracy: 0.7810 - val_loss: 0.4342 - val_accuracy: 0.8465\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.3818 - accuracy: 0.8648 - val_loss: 0.3729 - val_accuracy: 0.8685\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.3286 - accuracy: 0.8835 - val_loss: 0.3349 - val_accuracy: 0.8815\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.2970 - accuracy: 0.8941 - val_loss: 0.3181 - val_accuracy: 0.8868\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2725 - accuracy: 0.9017 - val_loss: 0.3034 - val_accuracy: 0.8914\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.2551 - accuracy: 0.9076 - val_loss: 0.3025 - val_accuracy: 0.8934\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.2378 - accuracy: 0.9140 - val_loss: 0.2793 - val_accuracy: 0.9001\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.2286 - accuracy: 0.9157 - val_loss: 0.2833 - val_accuracy: 0.8944\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2137 - accuracy: 0.9217 - val_loss: 0.2743 - val_accuracy: 0.9003\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2005 - accuracy: 0.9262 - val_loss: 0.2636 - val_accuracy: 0.9051\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.1944 - accuracy: 0.9288 - val_loss: 0.2616 - val_accuracy: 0.9095\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.1802 - accuracy: 0.9344 - val_loss: 0.2585 - val_accuracy: 0.9055\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.1772 - accuracy: 0.9343 - val_loss: 0.2641 - val_accuracy: 0.9059\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.1647 - accuracy: 0.9401 - val_loss: 0.2628 - val_accuracy: 0.9091\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.1587 - accuracy: 0.9416 - val_loss: 0.2570 - val_accuracy: 0.9109\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.1468 - accuracy: 0.9460 - val_loss: 0.2643 - val_accuracy: 0.9083\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.1401 - accuracy: 0.9486 - val_loss: 0.2587 - val_accuracy: 0.9111\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.1383 - accuracy: 0.9484 - val_loss: 0.2664 - val_accuracy: 0.9104\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2570 - accuracy: 0.9109\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.5969 - accuracy: 0.7853 - val_loss: 0.3864 - val_accuracy: 0.8604\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.3955 - accuracy: 0.8585 - val_loss: 0.3494 - val_accuracy: 0.8739\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.3459 - accuracy: 0.8766 - val_loss: 0.3118 - val_accuracy: 0.8848\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.3176 - accuracy: 0.8855 - val_loss: 0.3096 - val_accuracy: 0.8852\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 32s 53ms/step - loss: 0.2922 - accuracy: 0.8929 - val_loss: 0.2919 - val_accuracy: 0.8922\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2711 - accuracy: 0.9005 - val_loss: 0.2959 - val_accuracy: 0.8911\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2579 - accuracy: 0.9056 - val_loss: 0.2693 - val_accuracy: 0.9009\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2433 - accuracy: 0.9108 - val_loss: 0.2583 - val_accuracy: 0.9094\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2311 - accuracy: 0.9144 - val_loss: 0.2586 - val_accuracy: 0.9062\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2225 - accuracy: 0.9182 - val_loss: 0.2634 - val_accuracy: 0.9049\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2127 - accuracy: 0.9206 - val_loss: 0.2529 - val_accuracy: 0.9095\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2027 - accuracy: 0.9254 - val_loss: 0.2591 - val_accuracy: 0.9104\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.1963 - accuracy: 0.9275 - val_loss: 0.2604 - val_accuracy: 0.9081\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.1873 - accuracy: 0.9309 - val_loss: 0.2581 - val_accuracy: 0.9116\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2529 - accuracy: 0.9095\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.6974 - accuracy: 0.7431 - val_loss: 0.4266 - val_accuracy: 0.8475\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.4364 - accuracy: 0.8433 - val_loss: 0.3723 - val_accuracy: 0.8681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3827 - accuracy: 0.8631 - val_loss: 0.3341 - val_accuracy: 0.8800\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3480 - accuracy: 0.8749 - val_loss: 0.3202 - val_accuracy: 0.8831\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3243 - accuracy: 0.8827 - val_loss: 0.2975 - val_accuracy: 0.8931\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.3073 - accuracy: 0.8882 - val_loss: 0.2944 - val_accuracy: 0.8944\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2944 - accuracy: 0.8925 - val_loss: 0.2884 - val_accuracy: 0.8946\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2769 - accuracy: 0.8992 - val_loss: 0.2928 - val_accuracy: 0.8944\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2670 - accuracy: 0.9014 - val_loss: 0.2666 - val_accuracy: 0.9017\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2533 - accuracy: 0.9059 - val_loss: 0.2725 - val_accuracy: 0.9003\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2448 - accuracy: 0.9110 - val_loss: 0.2644 - val_accuracy: 0.9043\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2377 - accuracy: 0.9131 - val_loss: 0.2616 - val_accuracy: 0.9068\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2261 - accuracy: 0.9162 - val_loss: 0.2576 - val_accuracy: 0.9098\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.2198 - accuracy: 0.9185 - val_loss: 0.2652 - val_accuracy: 0.9050\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2170 - accuracy: 0.9204 - val_loss: 0.2578 - val_accuracy: 0.9060\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2058 - accuracy: 0.9229 - val_loss: 0.2546 - val_accuracy: 0.9102\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1994 - accuracy: 0.9258 - val_loss: 0.2502 - val_accuracy: 0.9132\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1946 - accuracy: 0.9280 - val_loss: 0.2479 - val_accuracy: 0.9127\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1862 - accuracy: 0.9302 - val_loss: 0.2649 - val_accuracy: 0.9100\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1805 - accuracy: 0.9321 - val_loss: 0.2670 - val_accuracy: 0.9102\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1771 - accuracy: 0.9334 - val_loss: 0.2641 - val_accuracy: 0.9106\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2479 - accuracy: 0.9127\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.7100 - accuracy: 0.7543 - val_loss: 0.4339 - val_accuracy: 0.8435\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.4286 - accuracy: 0.8478 - val_loss: 0.3663 - val_accuracy: 0.8671\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.3653 - accuracy: 0.8705 - val_loss: 0.3219 - val_accuracy: 0.8838\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.3369 - accuracy: 0.8773 - val_loss: 0.3034 - val_accuracy: 0.8898\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3098 - accuracy: 0.8878 - val_loss: 0.2976 - val_accuracy: 0.8915\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2891 - accuracy: 0.8940 - val_loss: 0.2829 - val_accuracy: 0.8970\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2770 - accuracy: 0.8989 - val_loss: 0.2738 - val_accuracy: 0.9001\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2649 - accuracy: 0.9036 - val_loss: 0.2939 - val_accuracy: 0.8900\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2520 - accuracy: 0.9077 - val_loss: 0.2696 - val_accuracy: 0.9012\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2405 - accuracy: 0.9104 - val_loss: 0.2569 - val_accuracy: 0.9057\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2296 - accuracy: 0.9161 - val_loss: 0.2602 - val_accuracy: 0.9047\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2286 - accuracy: 0.9149 - val_loss: 0.2648 - val_accuracy: 0.9028\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2154 - accuracy: 0.9204 - val_loss: 0.2587 - val_accuracy: 0.9070\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2569 - accuracy: 0.9057\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.5134 - accuracy: 0.8219 - val_loss: 0.4031 - val_accuracy: 0.8567\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.3547 - accuracy: 0.8739 - val_loss: 0.3589 - val_accuracy: 0.8695\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.3079 - accuracy: 0.8897 - val_loss: 0.3078 - val_accuracy: 0.8877\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2774 - accuracy: 0.8993 - val_loss: 0.2962 - val_accuracy: 0.8900\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2537 - accuracy: 0.9083 - val_loss: 0.2931 - val_accuracy: 0.8918\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2346 - accuracy: 0.9147 - val_loss: 0.2865 - val_accuracy: 0.8933\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2164 - accuracy: 0.9206 - val_loss: 0.2635 - val_accuracy: 0.9064\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2016 - accuracy: 0.9262 - val_loss: 0.2602 - val_accuracy: 0.9057\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.1841 - accuracy: 0.9323 - val_loss: 0.2515 - val_accuracy: 0.9101\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.1714 - accuracy: 0.9366 - val_loss: 0.2484 - val_accuracy: 0.9115\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.1567 - accuracy: 0.9422 - val_loss: 0.2488 - val_accuracy: 0.9157\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.1438 - accuracy: 0.9478 - val_loss: 0.2487 - val_accuracy: 0.9153\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.1326 - accuracy: 0.9518 - val_loss: 0.2659 - val_accuracy: 0.9102\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2484 - accuracy: 0.9115\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.6115 - accuracy: 0.7831 - val_loss: 0.4781 - val_accuracy: 0.8202\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 27s 115ms/step - loss: 0.3975 - accuracy: 0.8576 - val_loss: 0.4027 - val_accuracy: 0.8558\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3484 - accuracy: 0.8771 - val_loss: 0.3538 - val_accuracy: 0.8744\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3167 - accuracy: 0.8868 - val_loss: 0.3468 - val_accuracy: 0.8736\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2955 - accuracy: 0.8938 - val_loss: 0.3239 - val_accuracy: 0.8810\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2769 - accuracy: 0.9008 - val_loss: 0.3130 - val_accuracy: 0.8881\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2610 - accuracy: 0.9045 - val_loss: 0.2964 - val_accuracy: 0.8913\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.2501 - accuracy: 0.9097 - val_loss: 0.3010 - val_accuracy: 0.8902\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 29s 120ms/step - loss: 0.2351 - accuracy: 0.9139 - val_loss: 0.2855 - val_accuracy: 0.8972\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.2226 - accuracy: 0.9194 - val_loss: 0.2792 - val_accuracy: 0.8975\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2128 - accuracy: 0.9219 - val_loss: 0.2721 - val_accuracy: 0.9011\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2047 - accuracy: 0.9253 - val_loss: 0.2680 - val_accuracy: 0.9041\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1957 - accuracy: 0.9279 - val_loss: 0.2626 - val_accuracy: 0.9048\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.1888 - accuracy: 0.9302 - val_loss: 0.2614 - val_accuracy: 0.9070\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1808 - accuracy: 0.9334 - val_loss: 0.2637 - val_accuracy: 0.9044\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1706 - accuracy: 0.9374 - val_loss: 0.2617 - val_accuracy: 0.9069\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.1650 - accuracy: 0.9392 - val_loss: 0.2769 - val_accuracy: 0.9018\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2614 - accuracy: 0.9070\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.6828 - accuracy: 0.7568 - val_loss: 0.4949 - val_accuracy: 0.8241\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.4499 - accuracy: 0.8376 - val_loss: 0.4473 - val_accuracy: 0.8415\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.4012 - accuracy: 0.8565 - val_loss: 0.4116 - val_accuracy: 0.8534\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3745 - accuracy: 0.8665 - val_loss: 0.3999 - val_accuracy: 0.8573\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3451 - accuracy: 0.8769 - val_loss: 0.3586 - val_accuracy: 0.8735\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3260 - accuracy: 0.8833 - val_loss: 0.3642 - val_accuracy: 0.8711\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3142 - accuracy: 0.8878 - val_loss: 0.3588 - val_accuracy: 0.8670\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2982 - accuracy: 0.8932 - val_loss: 0.3294 - val_accuracy: 0.8837\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2850 - accuracy: 0.8986 - val_loss: 0.3169 - val_accuracy: 0.8874\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2744 - accuracy: 0.9018 - val_loss: 0.3152 - val_accuracy: 0.8884\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2618 - accuracy: 0.9068 - val_loss: 0.3063 - val_accuracy: 0.8896\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2549 - accuracy: 0.9078 - val_loss: 0.2990 - val_accuracy: 0.8901\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2463 - accuracy: 0.9123 - val_loss: 0.2984 - val_accuracy: 0.8916\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2351 - accuracy: 0.9149 - val_loss: 0.2867 - val_accuracy: 0.8971\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2276 - accuracy: 0.9186 - val_loss: 0.2917 - val_accuracy: 0.8975\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2207 - accuracy: 0.9210 - val_loss: 0.2931 - val_accuracy: 0.8963\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2134 - accuracy: 0.9222 - val_loss: 0.2919 - val_accuracy: 0.8989\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2867 - accuracy: 0.8971\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.6525 - accuracy: 0.7703 - val_loss: 0.4222 - val_accuracy: 0.8504\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.4228 - accuracy: 0.8504 - val_loss: 0.3484 - val_accuracy: 0.8737\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.3688 - accuracy: 0.8686 - val_loss: 0.3281 - val_accuracy: 0.8803\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.3326 - accuracy: 0.8802 - val_loss: 0.3088 - val_accuracy: 0.8885\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.3100 - accuracy: 0.8871 - val_loss: 0.3014 - val_accuracy: 0.8895\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2894 - accuracy: 0.8948 - val_loss: 0.2814 - val_accuracy: 0.8946\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2723 - accuracy: 0.9017 - val_loss: 0.2681 - val_accuracy: 0.9012\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 31s 52ms/step - loss: 0.2578 - accuracy: 0.9056 - val_loss: 0.2615 - val_accuracy: 0.9021\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2467 - accuracy: 0.9108 - val_loss: 0.2709 - val_accuracy: 0.9016\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2334 - accuracy: 0.9145 - val_loss: 0.2582 - val_accuracy: 0.9049\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.2262 - accuracy: 0.9166 - val_loss: 0.2658 - val_accuracy: 0.9011\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2159 - accuracy: 0.9209 - val_loss: 0.2549 - val_accuracy: 0.9059\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.2075 - accuracy: 0.9246 - val_loss: 0.2608 - val_accuracy: 0.9075\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.1984 - accuracy: 0.9275 - val_loss: 0.2489 - val_accuracy: 0.9089\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.1917 - accuracy: 0.9294 - val_loss: 0.2482 - val_accuracy: 0.9117\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.1819 - accuracy: 0.9323 - val_loss: 0.2458 - val_accuracy: 0.9115\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.1786 - accuracy: 0.9337 - val_loss: 0.2527 - val_accuracy: 0.9112\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 31s 51ms/step - loss: 0.1701 - accuracy: 0.9374 - val_loss: 0.2468 - val_accuracy: 0.9152\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 30s 51ms/step - loss: 0.1661 - accuracy: 0.9381 - val_loss: 0.2520 - val_accuracy: 0.9134\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2458 - accuracy: 0.9115\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.7355 - accuracy: 0.7433 - val_loss: 0.4395 - val_accuracy: 0.8481\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.4531 - accuracy: 0.8407 - val_loss: 0.3771 - val_accuracy: 0.8627\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3875 - accuracy: 0.8625 - val_loss: 0.3380 - val_accuracy: 0.8775\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3538 - accuracy: 0.8731 - val_loss: 0.3144 - val_accuracy: 0.8876\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3244 - accuracy: 0.8854 - val_loss: 0.3059 - val_accuracy: 0.8864\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3072 - accuracy: 0.8903 - val_loss: 0.2931 - val_accuracy: 0.8938\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.2913 - accuracy: 0.8948 - val_loss: 0.2816 - val_accuracy: 0.8973\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.2759 - accuracy: 0.9007 - val_loss: 0.2791 - val_accuracy: 0.9002\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.2635 - accuracy: 0.9039 - val_loss: 0.2661 - val_accuracy: 0.9024\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2508 - accuracy: 0.9089 - val_loss: 0.2634 - val_accuracy: 0.9062\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2420 - accuracy: 0.9108 - val_loss: 0.2640 - val_accuracy: 0.9025\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2323 - accuracy: 0.9158 - val_loss: 0.2585 - val_accuracy: 0.9056\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2246 - accuracy: 0.9175 - val_loss: 0.2561 - val_accuracy: 0.9076\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2185 - accuracy: 0.9189 - val_loss: 0.2512 - val_accuracy: 0.9081\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 27s 115ms/step - loss: 0.2101 - accuracy: 0.9231 - val_loss: 0.2457 - val_accuracy: 0.9112\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2023 - accuracy: 0.9255 - val_loss: 0.2555 - val_accuracy: 0.9122\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.1979 - accuracy: 0.9267 - val_loss: 0.2430 - val_accuracy: 0.9117\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.1907 - accuracy: 0.9305 - val_loss: 0.2439 - val_accuracy: 0.9140\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 27s 115ms/step - loss: 0.1825 - accuracy: 0.9319 - val_loss: 0.2441 - val_accuracy: 0.9144\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.1774 - accuracy: 0.9351 - val_loss: 0.2516 - val_accuracy: 0.9108\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2430 - accuracy: 0.9117\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.8862 - accuracy: 0.6891 - val_loss: 0.5080 - val_accuracy: 0.8178\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.5133 - accuracy: 0.8186 - val_loss: 0.4126 - val_accuracy: 0.8521\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.4354 - accuracy: 0.8459 - val_loss: 0.3799 - val_accuracy: 0.8659\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3977 - accuracy: 0.8599 - val_loss: 0.3523 - val_accuracy: 0.8750\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3697 - accuracy: 0.8701 - val_loss: 0.3348 - val_accuracy: 0.8801\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3467 - accuracy: 0.8769 - val_loss: 0.3235 - val_accuracy: 0.8834\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3334 - accuracy: 0.8814 - val_loss: 0.3132 - val_accuracy: 0.8856\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3161 - accuracy: 0.8864 - val_loss: 0.3037 - val_accuracy: 0.8901\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.3047 - accuracy: 0.8904 - val_loss: 0.2925 - val_accuracy: 0.8929\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2918 - accuracy: 0.8951 - val_loss: 0.2896 - val_accuracy: 0.8946\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2838 - accuracy: 0.8983 - val_loss: 0.2843 - val_accuracy: 0.8968\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2744 - accuracy: 0.9008 - val_loss: 0.2774 - val_accuracy: 0.8977\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2692 - accuracy: 0.9015 - val_loss: 0.2749 - val_accuracy: 0.8992\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2592 - accuracy: 0.9061 - val_loss: 0.2706 - val_accuracy: 0.9004\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2528 - accuracy: 0.9064 - val_loss: 0.2677 - val_accuracy: 0.9029\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2476 - accuracy: 0.9090 - val_loss: 0.2652 - val_accuracy: 0.9032\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2401 - accuracy: 0.9127 - val_loss: 0.2616 - val_accuracy: 0.9042\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2358 - accuracy: 0.9143 - val_loss: 0.2664 - val_accuracy: 0.9018\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2330 - accuracy: 0.9154 - val_loss: 0.2494 - val_accuracy: 0.9065\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2246 - accuracy: 0.9178 - val_loss: 0.2536 - val_accuracy: 0.9040\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 26s 219ms/step - loss: 0.2194 - accuracy: 0.9203 - val_loss: 0.2503 - val_accuracy: 0.9090\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2147 - accuracy: 0.9218 - val_loss: 0.2484 - val_accuracy: 0.9084\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2090 - accuracy: 0.9236 - val_loss: 0.2527 - val_accuracy: 0.9091\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2029 - accuracy: 0.9257 - val_loss: 0.2513 - val_accuracy: 0.9097\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2026 - accuracy: 0.9271 - val_loss: 0.2451 - val_accuracy: 0.9093\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.1970 - accuracy: 0.9286 - val_loss: 0.2515 - val_accuracy: 0.9109\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.1923 - accuracy: 0.9287 - val_loss: 0.2454 - val_accuracy: 0.9108\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.1880 - accuracy: 0.9307 - val_loss: 0.2466 - val_accuracy: 0.9131\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2451 - accuracy: 0.9093\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.6117 - accuracy: 0.7794 - val_loss: 0.4486 - val_accuracy: 0.8410\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3962 - accuracy: 0.8582 - val_loss: 0.3828 - val_accuracy: 0.8655\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3521 - accuracy: 0.8724 - val_loss: 0.3483 - val_accuracy: 0.8776\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3239 - accuracy: 0.8820 - val_loss: 0.3402 - val_accuracy: 0.8804\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3022 - accuracy: 0.8901 - val_loss: 0.3278 - val_accuracy: 0.8821\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2876 - accuracy: 0.8944 - val_loss: 0.3267 - val_accuracy: 0.8805\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2736 - accuracy: 0.8996 - val_loss: 0.3051 - val_accuracy: 0.8911\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2633 - accuracy: 0.9030 - val_loss: 0.2976 - val_accuracy: 0.8942\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2534 - accuracy: 0.9057 - val_loss: 0.2962 - val_accuracy: 0.8929\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2439 - accuracy: 0.9099 - val_loss: 0.2995 - val_accuracy: 0.8944\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2346 - accuracy: 0.9124 - val_loss: 0.2969 - val_accuracy: 0.8973\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2258 - accuracy: 0.9162 - val_loss: 0.2986 - val_accuracy: 0.8931\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2962 - accuracy: 0.8929\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.7637 - accuracy: 0.7232 - val_loss: 0.5589 - val_accuracy: 0.7911\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4895 - accuracy: 0.8201 - val_loss: 0.4707 - val_accuracy: 0.8273\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4373 - accuracy: 0.8397 - val_loss: 0.4356 - val_accuracy: 0.8371\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4045 - accuracy: 0.8524 - val_loss: 0.4536 - val_accuracy: 0.8311\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3820 - accuracy: 0.8613 - val_loss: 0.3931 - val_accuracy: 0.8598\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 9s 37ms/step - loss: 0.3601 - accuracy: 0.8695 - val_loss: 0.3840 - val_accuracy: 0.8614\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3452 - accuracy: 0.8740 - val_loss: 0.3740 - val_accuracy: 0.8653\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3356 - accuracy: 0.8779 - val_loss: 0.3636 - val_accuracy: 0.8699\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3225 - accuracy: 0.8814 - val_loss: 0.3514 - val_accuracy: 0.8721\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3143 - accuracy: 0.8845 - val_loss: 0.3375 - val_accuracy: 0.8793\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3029 - accuracy: 0.8892 - val_loss: 0.3338 - val_accuracy: 0.8769\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2960 - accuracy: 0.8914 - val_loss: 0.3278 - val_accuracy: 0.8821\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2889 - accuracy: 0.8936 - val_loss: 0.3345 - val_accuracy: 0.8789\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2826 - accuracy: 0.8947 - val_loss: 0.3276 - val_accuracy: 0.8806\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2745 - accuracy: 0.8993 - val_loss: 0.3391 - val_accuracy: 0.8765\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2695 - accuracy: 0.8996 - val_loss: 0.3189 - val_accuracy: 0.8838\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2605 - accuracy: 0.9026 - val_loss: 0.3223 - val_accuracy: 0.8852\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2574 - accuracy: 0.9046 - val_loss: 0.3134 - val_accuracy: 0.8862\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2511 - accuracy: 0.9065 - val_loss: 0.3168 - val_accuracy: 0.8829\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 9s 35ms/step - loss: 0.2449 - accuracy: 0.9079 - val_loss: 0.3065 - val_accuracy: 0.8914\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2406 - accuracy: 0.9096 - val_loss: 0.3142 - val_accuracy: 0.8867\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2330 - accuracy: 0.9127 - val_loss: 0.2996 - val_accuracy: 0.8897\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2302 - accuracy: 0.9128 - val_loss: 0.3100 - val_accuracy: 0.8842\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2237 - accuracy: 0.9158 - val_loss: 0.3047 - val_accuracy: 0.8885\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2192 - accuracy: 0.9174 - val_loss: 0.3018 - val_accuracy: 0.8904\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2996 - accuracy: 0.8897\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.9984 - accuracy: 0.6434 - val_loss: 0.6314 - val_accuracy: 0.7501\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.5669 - accuracy: 0.7835 - val_loss: 0.5435 - val_accuracy: 0.7954\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.5026 - accuracy: 0.8132 - val_loss: 0.4889 - val_accuracy: 0.8240\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4540 - accuracy: 0.8368 - val_loss: 0.4632 - val_accuracy: 0.8321\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4229 - accuracy: 0.8492 - val_loss: 0.4547 - val_accuracy: 0.8344\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4027 - accuracy: 0.8551 - val_loss: 0.4187 - val_accuracy: 0.8510\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3849 - accuracy: 0.8636 - val_loss: 0.4067 - val_accuracy: 0.8552\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3760 - accuracy: 0.8643 - val_loss: 0.3949 - val_accuracy: 0.8590\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 9s 71ms/step - loss: 0.3563 - accuracy: 0.8730 - val_loss: 0.3763 - val_accuracy: 0.8659\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3496 - accuracy: 0.8747 - val_loss: 0.3775 - val_accuracy: 0.8651\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.3406 - accuracy: 0.8770 - val_loss: 0.3618 - val_accuracy: 0.8708\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3290 - accuracy: 0.8810 - val_loss: 0.3503 - val_accuracy: 0.8733\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3219 - accuracy: 0.8836 - val_loss: 0.3529 - val_accuracy: 0.8723\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3146 - accuracy: 0.8861 - val_loss: 0.3388 - val_accuracy: 0.8793\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3110 - accuracy: 0.8875 - val_loss: 0.3505 - val_accuracy: 0.8733\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3011 - accuracy: 0.8895 - val_loss: 0.3317 - val_accuracy: 0.8828\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.2963 - accuracy: 0.8924 - val_loss: 0.3330 - val_accuracy: 0.8801\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2929 - accuracy: 0.8933 - val_loss: 0.3274 - val_accuracy: 0.8839\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2862 - accuracy: 0.8968 - val_loss: 0.3257 - val_accuracy: 0.8838\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2858 - accuracy: 0.8954 - val_loss: 0.3194 - val_accuracy: 0.8871\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.2796 - accuracy: 0.8981 - val_loss: 0.3212 - val_accuracy: 0.8878\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 9s 71ms/step - loss: 0.2742 - accuracy: 0.9001 - val_loss: 0.3295 - val_accuracy: 0.8796\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2690 - accuracy: 0.9027 - val_loss: 0.3350 - val_accuracy: 0.8820\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3194 - accuracy: 0.8871\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.8202 - accuracy: 0.6921 - val_loss: 0.5233 - val_accuracy: 0.7947\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.5453 - accuracy: 0.7958 - val_loss: 0.4516 - val_accuracy: 0.8320\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.4843 - accuracy: 0.8195 - val_loss: 0.4188 - val_accuracy: 0.8415\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.4496 - accuracy: 0.8336 - val_loss: 0.3859 - val_accuracy: 0.8551\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.4209 - accuracy: 0.8457 - val_loss: 0.3627 - val_accuracy: 0.8661\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.4010 - accuracy: 0.8516 - val_loss: 0.3524 - val_accuracy: 0.8683\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3860 - accuracy: 0.8561 - val_loss: 0.3402 - val_accuracy: 0.8716\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3710 - accuracy: 0.8634 - val_loss: 0.3336 - val_accuracy: 0.8779\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3622 - accuracy: 0.8656 - val_loss: 0.3311 - val_accuracy: 0.8750\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3545 - accuracy: 0.8688 - val_loss: 0.3155 - val_accuracy: 0.8818\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3437 - accuracy: 0.8738 - val_loss: 0.3164 - val_accuracy: 0.8798\n",
      "Epoch 12/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3363 - accuracy: 0.8752 - val_loss: 0.3022 - val_accuracy: 0.8873\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3285 - accuracy: 0.8785 - val_loss: 0.3084 - val_accuracy: 0.8859\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3252 - accuracy: 0.8785 - val_loss: 0.2982 - val_accuracy: 0.8914\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3183 - accuracy: 0.8810 - val_loss: 0.3004 - val_accuracy: 0.8889\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3133 - accuracy: 0.8833 - val_loss: 0.2910 - val_accuracy: 0.8925\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3087 - accuracy: 0.8849 - val_loss: 0.2950 - val_accuracy: 0.8912\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3061 - accuracy: 0.8857 - val_loss: 0.2899 - val_accuracy: 0.8937\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3001 - accuracy: 0.8875 - val_loss: 0.2864 - val_accuracy: 0.8957\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2967 - accuracy: 0.8893 - val_loss: 0.2892 - val_accuracy: 0.8912\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2946 - accuracy: 0.8897 - val_loss: 0.2882 - val_accuracy: 0.8914\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2903 - accuracy: 0.8918 - val_loss: 0.2843 - val_accuracy: 0.8943\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2845 - accuracy: 0.8944 - val_loss: 0.2746 - val_accuracy: 0.8983\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2865 - accuracy: 0.8931 - val_loss: 0.2821 - val_accuracy: 0.8958\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2834 - accuracy: 0.8937 - val_loss: 0.2764 - val_accuracy: 0.8966\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2804 - accuracy: 0.8957 - val_loss: 0.2770 - val_accuracy: 0.8972\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2746 - accuracy: 0.8983\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.9579 - accuracy: 0.6464 - val_loss: 0.5684 - val_accuracy: 0.7827\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.6049 - accuracy: 0.7746 - val_loss: 0.5019 - val_accuracy: 0.8167\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.5442 - accuracy: 0.7991 - val_loss: 0.4648 - val_accuracy: 0.8274\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.5017 - accuracy: 0.8147 - val_loss: 0.4298 - val_accuracy: 0.8426\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4773 - accuracy: 0.8245 - val_loss: 0.4117 - val_accuracy: 0.8441\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4515 - accuracy: 0.8329 - val_loss: 0.3933 - val_accuracy: 0.8527\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4368 - accuracy: 0.8389 - val_loss: 0.3790 - val_accuracy: 0.8598\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4208 - accuracy: 0.8459 - val_loss: 0.3759 - val_accuracy: 0.8618\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4074 - accuracy: 0.8502 - val_loss: 0.3699 - val_accuracy: 0.8642\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3940 - accuracy: 0.8563 - val_loss: 0.3484 - val_accuracy: 0.8706\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3832 - accuracy: 0.8587 - val_loss: 0.3548 - val_accuracy: 0.8681\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3765 - accuracy: 0.8606 - val_loss: 0.3429 - val_accuracy: 0.8749\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3709 - accuracy: 0.8629 - val_loss: 0.3331 - val_accuracy: 0.8770\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3654 - accuracy: 0.8652 - val_loss: 0.3320 - val_accuracy: 0.8793\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3542 - accuracy: 0.8697 - val_loss: 0.3222 - val_accuracy: 0.8820\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3511 - accuracy: 0.8693 - val_loss: 0.3335 - val_accuracy: 0.8767\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3430 - accuracy: 0.8722 - val_loss: 0.3145 - val_accuracy: 0.8838\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3409 - accuracy: 0.8745 - val_loss: 0.3201 - val_accuracy: 0.8835\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3364 - accuracy: 0.8747 - val_loss: 0.3145 - val_accuracy: 0.8835\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3323 - accuracy: 0.8768 - val_loss: 0.3083 - val_accuracy: 0.8888\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3226 - accuracy: 0.8799 - val_loss: 0.3052 - val_accuracy: 0.8870\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3263 - accuracy: 0.8787 - val_loss: 0.3036 - val_accuracy: 0.8901\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3193 - accuracy: 0.8813 - val_loss: 0.3020 - val_accuracy: 0.8897\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3166 - accuracy: 0.8823 - val_loss: 0.2973 - val_accuracy: 0.8899\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3148 - accuracy: 0.8836 - val_loss: 0.3013 - val_accuracy: 0.8877\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3097 - accuracy: 0.8850 - val_loss: 0.2975 - val_accuracy: 0.8906\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3061 - accuracy: 0.8848 - val_loss: 0.2939 - val_accuracy: 0.8935\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3034 - accuracy: 0.8860 - val_loss: 0.2928 - val_accuracy: 0.8935\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3016 - accuracy: 0.8886 - val_loss: 0.2921 - val_accuracy: 0.8925\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2982 - accuracy: 0.8883 - val_loss: 0.2977 - val_accuracy: 0.8902\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2950 - accuracy: 0.8900 - val_loss: 0.2893 - val_accuracy: 0.8943\n",
      "Epoch 32/100\n",
      "240/240 [==============================] - 9s 35ms/step - loss: 0.2932 - accuracy: 0.8907 - val_loss: 0.2923 - val_accuracy: 0.8933\n",
      "Epoch 33/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2919 - accuracy: 0.8914 - val_loss: 0.2927 - val_accuracy: 0.8917\n",
      "Epoch 34/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2892 - accuracy: 0.8923 - val_loss: 0.2886 - val_accuracy: 0.8933\n",
      "Epoch 35/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2876 - accuracy: 0.8931 - val_loss: 0.2839 - val_accuracy: 0.8964\n",
      "Epoch 36/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2849 - accuracy: 0.8921 - val_loss: 0.2835 - val_accuracy: 0.8963\n",
      "Epoch 37/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2822 - accuracy: 0.8940 - val_loss: 0.2821 - val_accuracy: 0.8957\n",
      "Epoch 38/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2819 - accuracy: 0.8940 - val_loss: 0.2841 - val_accuracy: 0.8955\n",
      "Epoch 39/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2810 - accuracy: 0.8950 - val_loss: 0.2855 - val_accuracy: 0.8969\n",
      "Epoch 40/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2780 - accuracy: 0.8952 - val_loss: 0.2808 - val_accuracy: 0.8952\n",
      "Epoch 41/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2770 - accuracy: 0.8963 - val_loss: 0.2769 - val_accuracy: 0.8999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2770 - accuracy: 0.8970 - val_loss: 0.2812 - val_accuracy: 0.8953\n",
      "Epoch 43/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2708 - accuracy: 0.8984 - val_loss: 0.2801 - val_accuracy: 0.8984\n",
      "Epoch 44/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2706 - accuracy: 0.8979 - val_loss: 0.2747 - val_accuracy: 0.8988\n",
      "Epoch 45/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2702 - accuracy: 0.8989 - val_loss: 0.2762 - val_accuracy: 0.8999\n",
      "Epoch 46/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2694 - accuracy: 0.8984 - val_loss: 0.2738 - val_accuracy: 0.9015\n",
      "Epoch 47/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2683 - accuracy: 0.8998 - val_loss: 0.2797 - val_accuracy: 0.8987\n",
      "Epoch 48/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2665 - accuracy: 0.8990 - val_loss: 0.2763 - val_accuracy: 0.8993\n",
      "Epoch 49/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2644 - accuracy: 0.9000 - val_loss: 0.2743 - val_accuracy: 0.9019\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2738 - accuracy: 0.9015\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 1.1451 - accuracy: 0.5895 - val_loss: 0.6218 - val_accuracy: 0.7635\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.6385 - accuracy: 0.7599 - val_loss: 0.5256 - val_accuracy: 0.8000\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.5605 - accuracy: 0.7900 - val_loss: 0.4799 - val_accuracy: 0.8181\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.5231 - accuracy: 0.8067 - val_loss: 0.4500 - val_accuracy: 0.8311\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4892 - accuracy: 0.8185 - val_loss: 0.4190 - val_accuracy: 0.8425\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4638 - accuracy: 0.8280 - val_loss: 0.4049 - val_accuracy: 0.8482\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4466 - accuracy: 0.8351 - val_loss: 0.3894 - val_accuracy: 0.8550\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4259 - accuracy: 0.8421 - val_loss: 0.3728 - val_accuracy: 0.8604\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4127 - accuracy: 0.8490 - val_loss: 0.3648 - val_accuracy: 0.8615\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3992 - accuracy: 0.8530 - val_loss: 0.3545 - val_accuracy: 0.8714\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3890 - accuracy: 0.8568 - val_loss: 0.3444 - val_accuracy: 0.8721\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3835 - accuracy: 0.8595 - val_loss: 0.3350 - val_accuracy: 0.8775\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3684 - accuracy: 0.8640 - val_loss: 0.3321 - val_accuracy: 0.8790\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3608 - accuracy: 0.8666 - val_loss: 0.3236 - val_accuracy: 0.8801\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3558 - accuracy: 0.8694 - val_loss: 0.3243 - val_accuracy: 0.8815\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3522 - accuracy: 0.8706 - val_loss: 0.3175 - val_accuracy: 0.8826\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3454 - accuracy: 0.8740 - val_loss: 0.3135 - val_accuracy: 0.8858\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3426 - accuracy: 0.8734 - val_loss: 0.3088 - val_accuracy: 0.8876\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 9s 73ms/step - loss: 0.3346 - accuracy: 0.8759 - val_loss: 0.3061 - val_accuracy: 0.8878\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 9s 73ms/step - loss: 0.3333 - accuracy: 0.8771 - val_loss: 0.3038 - val_accuracy: 0.8895\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 9s 73ms/step - loss: 0.3263 - accuracy: 0.8793 - val_loss: 0.3019 - val_accuracy: 0.8904\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 9s 76ms/step - loss: 0.3265 - accuracy: 0.8779 - val_loss: 0.2976 - val_accuracy: 0.8899\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 9s 71ms/step - loss: 0.3232 - accuracy: 0.8801 - val_loss: 0.2947 - val_accuracy: 0.8941\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3178 - accuracy: 0.8827 - val_loss: 0.2940 - val_accuracy: 0.8928\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3161 - accuracy: 0.8829 - val_loss: 0.2943 - val_accuracy: 0.8907\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3129 - accuracy: 0.8842 - val_loss: 0.2903 - val_accuracy: 0.8940\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3106 - accuracy: 0.8861 - val_loss: 0.2885 - val_accuracy: 0.8936\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3056 - accuracy: 0.8862 - val_loss: 0.2845 - val_accuracy: 0.8968\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3030 - accuracy: 0.8888 - val_loss: 0.2850 - val_accuracy: 0.8951\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3016 - accuracy: 0.8891 - val_loss: 0.2861 - val_accuracy: 0.8951\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2999 - accuracy: 0.8904 - val_loss: 0.2828 - val_accuracy: 0.8954\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.2956 - accuracy: 0.8909 - val_loss: 0.2823 - val_accuracy: 0.8973\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2946 - accuracy: 0.8899 - val_loss: 0.2785 - val_accuracy: 0.8975\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.2900 - accuracy: 0.8924 - val_loss: 0.2799 - val_accuracy: 0.8961\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.2904 - accuracy: 0.8914 - val_loss: 0.2770 - val_accuracy: 0.8974\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2878 - accuracy: 0.8926 - val_loss: 0.2775 - val_accuracy: 0.8963\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2838 - accuracy: 0.8954 - val_loss: 0.2763 - val_accuracy: 0.8978\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 8s 68ms/step - loss: 0.2852 - accuracy: 0.8939 - val_loss: 0.2724 - val_accuracy: 0.8993\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2827 - accuracy: 0.8944 - val_loss: 0.2733 - val_accuracy: 0.9004\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2771 - accuracy: 0.8963 - val_loss: 0.2737 - val_accuracy: 0.8984\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.2761 - accuracy: 0.8978 - val_loss: 0.2747 - val_accuracy: 0.8973\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2724 - accuracy: 0.8993\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.7129 - accuracy: 0.7566 - val_loss: 0.4994 - val_accuracy: 0.8220\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.4415 - accuracy: 0.8416 - val_loss: 0.4342 - val_accuracy: 0.8455\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3955 - accuracy: 0.8600 - val_loss: 0.3973 - val_accuracy: 0.8594\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3688 - accuracy: 0.8684 - val_loss: 0.4029 - val_accuracy: 0.8537\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3501 - accuracy: 0.8747 - val_loss: 0.3732 - val_accuracy: 0.8700\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3351 - accuracy: 0.8800 - val_loss: 0.3580 - val_accuracy: 0.8719\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3234 - accuracy: 0.8832 - val_loss: 0.3620 - val_accuracy: 0.8673\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3100 - accuracy: 0.8882 - val_loss: 0.3467 - val_accuracy: 0.8761\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3010 - accuracy: 0.8914 - val_loss: 0.3399 - val_accuracy: 0.8784\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2914 - accuracy: 0.8935 - val_loss: 0.3239 - val_accuracy: 0.8848\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2829 - accuracy: 0.8976 - val_loss: 0.3256 - val_accuracy: 0.8829\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2751 - accuracy: 0.9003 - val_loss: 0.3143 - val_accuracy: 0.8889\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2669 - accuracy: 0.9029 - val_loss: 0.3131 - val_accuracy: 0.8865\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2599 - accuracy: 0.9061 - val_loss: 0.3224 - val_accuracy: 0.8841\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2554 - accuracy: 0.9080 - val_loss: 0.3003 - val_accuracy: 0.8944\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2484 - accuracy: 0.9094 - val_loss: 0.3021 - val_accuracy: 0.8948\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2424 - accuracy: 0.9109 - val_loss: 0.3196 - val_accuracy: 0.8884\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2350 - accuracy: 0.9135 - val_loss: 0.3096 - val_accuracy: 0.8878\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3003 - accuracy: 0.8944\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.9354 - accuracy: 0.6951 - val_loss: 0.6239 - val_accuracy: 0.7681\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.5599 - accuracy: 0.7934 - val_loss: 0.5336 - val_accuracy: 0.8028\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4975 - accuracy: 0.8170 - val_loss: 0.4896 - val_accuracy: 0.8207\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4633 - accuracy: 0.8326 - val_loss: 0.4975 - val_accuracy: 0.8189\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4335 - accuracy: 0.8438 - val_loss: 0.4493 - val_accuracy: 0.8348\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4155 - accuracy: 0.8509 - val_loss: 0.4302 - val_accuracy: 0.8462\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3961 - accuracy: 0.8584 - val_loss: 0.4131 - val_accuracy: 0.8502\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3811 - accuracy: 0.8644 - val_loss: 0.4042 - val_accuracy: 0.8523\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3684 - accuracy: 0.8694 - val_loss: 0.3840 - val_accuracy: 0.8615\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3593 - accuracy: 0.8718 - val_loss: 0.3767 - val_accuracy: 0.8614\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3461 - accuracy: 0.8762 - val_loss: 0.3647 - val_accuracy: 0.8686\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3398 - accuracy: 0.8770 - val_loss: 0.3691 - val_accuracy: 0.8672\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3325 - accuracy: 0.8796 - val_loss: 0.3609 - val_accuracy: 0.8694\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3238 - accuracy: 0.8837 - val_loss: 0.3524 - val_accuracy: 0.8709\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3172 - accuracy: 0.8856 - val_loss: 0.3577 - val_accuracy: 0.8687\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3155 - accuracy: 0.8859 - val_loss: 0.3411 - val_accuracy: 0.8762\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3057 - accuracy: 0.8897 - val_loss: 0.3365 - val_accuracy: 0.8777\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3024 - accuracy: 0.8907 - val_loss: 0.3424 - val_accuracy: 0.8730\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2961 - accuracy: 0.8923 - val_loss: 0.3376 - val_accuracy: 0.8725\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2907 - accuracy: 0.8943 - val_loss: 0.3408 - val_accuracy: 0.8748\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3365 - accuracy: 0.8777\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 1.1719 - accuracy: 0.6383 - val_loss: 0.7181 - val_accuracy: 0.7417\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.6307 - accuracy: 0.7689 - val_loss: 0.5972 - val_accuracy: 0.7785\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.5330 - accuracy: 0.8048 - val_loss: 0.5144 - val_accuracy: 0.8128\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4849 - accuracy: 0.8227 - val_loss: 0.4858 - val_accuracy: 0.8228\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.4606 - accuracy: 0.8323 - val_loss: 0.4636 - val_accuracy: 0.8286\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4454 - accuracy: 0.8391 - val_loss: 0.4557 - val_accuracy: 0.8351\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4280 - accuracy: 0.8450 - val_loss: 0.4647 - val_accuracy: 0.8301\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4164 - accuracy: 0.8490 - val_loss: 0.4279 - val_accuracy: 0.8435\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4049 - accuracy: 0.8525 - val_loss: 0.4396 - val_accuracy: 0.8393\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3978 - accuracy: 0.8555 - val_loss: 0.4184 - val_accuracy: 0.8487\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 9s 71ms/step - loss: 0.3853 - accuracy: 0.8601 - val_loss: 0.4123 - val_accuracy: 0.8495\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 9s 72ms/step - loss: 0.3772 - accuracy: 0.8644 - val_loss: 0.4072 - val_accuracy: 0.8508\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3701 - accuracy: 0.8655 - val_loss: 0.3936 - val_accuracy: 0.8598\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3614 - accuracy: 0.8693 - val_loss: 0.3948 - val_accuracy: 0.8572\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3561 - accuracy: 0.8713 - val_loss: 0.3849 - val_accuracy: 0.8603\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3508 - accuracy: 0.8728 - val_loss: 0.3802 - val_accuracy: 0.8632\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3471 - accuracy: 0.8742 - val_loss: 0.3766 - val_accuracy: 0.8613\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3395 - accuracy: 0.8776 - val_loss: 0.3687 - val_accuracy: 0.8656\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.3323 - accuracy: 0.8792 - val_loss: 0.3665 - val_accuracy: 0.8656\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3331 - accuracy: 0.8801 - val_loss: 0.3715 - val_accuracy: 0.8640\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3256 - accuracy: 0.8817 - val_loss: 0.3621 - val_accuracy: 0.8679\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3214 - accuracy: 0.8838 - val_loss: 0.3610 - val_accuracy: 0.8676\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3186 - accuracy: 0.8845 - val_loss: 0.3608 - val_accuracy: 0.8685\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3142 - accuracy: 0.8863 - val_loss: 0.3520 - val_accuracy: 0.8711\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3121 - accuracy: 0.8877 - val_loss: 0.3509 - val_accuracy: 0.8727\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3078 - accuracy: 0.8895 - val_loss: 0.3478 - val_accuracy: 0.8731\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3055 - accuracy: 0.8891 - val_loss: 0.3474 - val_accuracy: 0.8737\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3008 - accuracy: 0.8907 - val_loss: 0.3465 - val_accuracy: 0.8721\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2976 - accuracy: 0.8921 - val_loss: 0.3426 - val_accuracy: 0.8737\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.2934 - accuracy: 0.8932 - val_loss: 0.3394 - val_accuracy: 0.8768\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2916 - accuracy: 0.8954 - val_loss: 0.3358 - val_accuracy: 0.8766\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2899 - accuracy: 0.8938 - val_loss: 0.3348 - val_accuracy: 0.8785\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2848 - accuracy: 0.8965 - val_loss: 0.3341 - val_accuracy: 0.8768\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 9s 72ms/step - loss: 0.2841 - accuracy: 0.8966 - val_loss: 0.3328 - val_accuracy: 0.8787\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 9s 71ms/step - loss: 0.2819 - accuracy: 0.8970 - val_loss: 0.3331 - val_accuracy: 0.8797\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.2782 - accuracy: 0.8999 - val_loss: 0.3314 - val_accuracy: 0.8809\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 9s 71ms/step - loss: 0.2794 - accuracy: 0.8983 - val_loss: 0.3268 - val_accuracy: 0.8819\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.2721 - accuracy: 0.9015 - val_loss: 0.3276 - val_accuracy: 0.8809\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 9s 74ms/step - loss: 0.2702 - accuracy: 0.9019 - val_loss: 0.3306 - val_accuracy: 0.8794\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 9s 74ms/step - loss: 0.2703 - accuracy: 0.9015 - val_loss: 0.3315 - val_accuracy: 0.8813\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3268 - accuracy: 0.8819\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.9570 - accuracy: 0.6506 - val_loss: 0.5640 - val_accuracy: 0.7847\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.6001 - accuracy: 0.7745 - val_loss: 0.4890 - val_accuracy: 0.8199\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.5293 - accuracy: 0.8037 - val_loss: 0.4493 - val_accuracy: 0.8302\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.4891 - accuracy: 0.8193 - val_loss: 0.4130 - val_accuracy: 0.8459\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.4608 - accuracy: 0.8315 - val_loss: 0.4015 - val_accuracy: 0.8524\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.4370 - accuracy: 0.8395 - val_loss: 0.3834 - val_accuracy: 0.8578\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.4212 - accuracy: 0.8444 - val_loss: 0.3643 - val_accuracy: 0.8662\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.4083 - accuracy: 0.8498 - val_loss: 0.3541 - val_accuracy: 0.8690\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3936 - accuracy: 0.8543 - val_loss: 0.3414 - val_accuracy: 0.8767\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3856 - accuracy: 0.8573 - val_loss: 0.3388 - val_accuracy: 0.8746\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3742 - accuracy: 0.8608 - val_loss: 0.3275 - val_accuracy: 0.8795\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3657 - accuracy: 0.8649 - val_loss: 0.3243 - val_accuracy: 0.8797\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3568 - accuracy: 0.8692 - val_loss: 0.3208 - val_accuracy: 0.8813\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3486 - accuracy: 0.8700 - val_loss: 0.3165 - val_accuracy: 0.8827\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3417 - accuracy: 0.8742 - val_loss: 0.3106 - val_accuracy: 0.8852\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3358 - accuracy: 0.8752 - val_loss: 0.3141 - val_accuracy: 0.8834\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3346 - accuracy: 0.8765 - val_loss: 0.3001 - val_accuracy: 0.8869\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3262 - accuracy: 0.8789 - val_loss: 0.2988 - val_accuracy: 0.8900\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.3238 - accuracy: 0.8801 - val_loss: 0.2974 - val_accuracy: 0.8897\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3206 - accuracy: 0.8813 - val_loss: 0.2913 - val_accuracy: 0.8915\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3157 - accuracy: 0.8826 - val_loss: 0.2929 - val_accuracy: 0.8883\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3138 - accuracy: 0.8840 - val_loss: 0.2906 - val_accuracy: 0.8932\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3093 - accuracy: 0.8856 - val_loss: 0.2831 - val_accuracy: 0.8958\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3064 - accuracy: 0.8869 - val_loss: 0.2809 - val_accuracy: 0.8955\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3035 - accuracy: 0.8890 - val_loss: 0.2919 - val_accuracy: 0.8928\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.3006 - accuracy: 0.8891 - val_loss: 0.2793 - val_accuracy: 0.8971\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2952 - accuracy: 0.8910 - val_loss: 0.2826 - val_accuracy: 0.8949\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2969 - accuracy: 0.8902 - val_loss: 0.2770 - val_accuracy: 0.8984\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2966 - accuracy: 0.8904 - val_loss: 0.2727 - val_accuracy: 0.8989\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2896 - accuracy: 0.8931 - val_loss: 0.2779 - val_accuracy: 0.8972\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2880 - accuracy: 0.8924 - val_loss: 0.2723 - val_accuracy: 0.8996\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2832 - accuracy: 0.8968 - val_loss: 0.2772 - val_accuracy: 0.9002\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2814 - accuracy: 0.8950 - val_loss: 0.2723 - val_accuracy: 0.9010\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2807 - accuracy: 0.8959 - val_loss: 0.2707 - val_accuracy: 0.9003\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2773 - accuracy: 0.8970 - val_loss: 0.2684 - val_accuracy: 0.9011\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2790 - accuracy: 0.8954 - val_loss: 0.2689 - val_accuracy: 0.9001\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2731 - accuracy: 0.8983 - val_loss: 0.2689 - val_accuracy: 0.9001\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 10s 17ms/step - loss: 0.2717 - accuracy: 0.8982 - val_loss: 0.2654 - val_accuracy: 0.9029\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2711 - accuracy: 0.8999 - val_loss: 0.2674 - val_accuracy: 0.9019\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2694 - accuracy: 0.8994 - val_loss: 0.2595 - val_accuracy: 0.9055\n",
      "Epoch 41/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2687 - accuracy: 0.8991 - val_loss: 0.2641 - val_accuracy: 0.9039\n",
      "Epoch 42/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2678 - accuracy: 0.9004 - val_loss: 0.2637 - val_accuracy: 0.9024\n",
      "Epoch 43/100\n",
      "600/600 [==============================] - 10s 16ms/step - loss: 0.2686 - accuracy: 0.9002 - val_loss: 0.2658 - val_accuracy: 0.9013\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2595 - accuracy: 0.9055\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 1.1971 - accuracy: 0.5580 - val_loss: 0.6507 - val_accuracy: 0.7500\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.6852 - accuracy: 0.7456 - val_loss: 0.5564 - val_accuracy: 0.7860\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.6013 - accuracy: 0.7754 - val_loss: 0.5149 - val_accuracy: 0.8022\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.5549 - accuracy: 0.7948 - val_loss: 0.4759 - val_accuracy: 0.8198\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.5255 - accuracy: 0.8063 - val_loss: 0.4502 - val_accuracy: 0.8320\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4985 - accuracy: 0.8148 - val_loss: 0.4305 - val_accuracy: 0.8393\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.4783 - accuracy: 0.8236 - val_loss: 0.4221 - val_accuracy: 0.8445\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4596 - accuracy: 0.8307 - val_loss: 0.4074 - val_accuracy: 0.8494\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.4444 - accuracy: 0.8377 - val_loss: 0.3901 - val_accuracy: 0.8561\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4352 - accuracy: 0.8410 - val_loss: 0.3838 - val_accuracy: 0.8551\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4249 - accuracy: 0.8433 - val_loss: 0.3756 - val_accuracy: 0.8609\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.4137 - accuracy: 0.8470 - val_loss: 0.3687 - val_accuracy: 0.8639\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.4048 - accuracy: 0.8503 - val_loss: 0.3632 - val_accuracy: 0.8644\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3990 - accuracy: 0.8529 - val_loss: 0.3589 - val_accuracy: 0.8647\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3903 - accuracy: 0.8556 - val_loss: 0.3503 - val_accuracy: 0.8699\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3854 - accuracy: 0.8570 - val_loss: 0.3465 - val_accuracy: 0.8709\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3800 - accuracy: 0.8587 - val_loss: 0.3402 - val_accuracy: 0.8744\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3730 - accuracy: 0.8622 - val_loss: 0.3378 - val_accuracy: 0.8733\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3713 - accuracy: 0.8630 - val_loss: 0.3333 - val_accuracy: 0.8763\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3646 - accuracy: 0.8650 - val_loss: 0.3290 - val_accuracy: 0.8787\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3584 - accuracy: 0.8664 - val_loss: 0.3273 - val_accuracy: 0.8806\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3551 - accuracy: 0.8690 - val_loss: 0.3228 - val_accuracy: 0.8799\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3520 - accuracy: 0.8691 - val_loss: 0.3212 - val_accuracy: 0.8826\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3464 - accuracy: 0.8713 - val_loss: 0.3191 - val_accuracy: 0.8831\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3437 - accuracy: 0.8722 - val_loss: 0.3154 - val_accuracy: 0.8842\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3409 - accuracy: 0.8738 - val_loss: 0.3118 - val_accuracy: 0.8849\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3381 - accuracy: 0.8738 - val_loss: 0.3093 - val_accuracy: 0.8863\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3316 - accuracy: 0.8757 - val_loss: 0.3054 - val_accuracy: 0.8870\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3305 - accuracy: 0.8792 - val_loss: 0.3025 - val_accuracy: 0.8903\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3284 - accuracy: 0.8790 - val_loss: 0.2998 - val_accuracy: 0.8892\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3234 - accuracy: 0.8803 - val_loss: 0.3005 - val_accuracy: 0.8885\n",
      "Epoch 32/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3213 - accuracy: 0.8802 - val_loss: 0.2987 - val_accuracy: 0.8914\n",
      "Epoch 33/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3199 - accuracy: 0.8814 - val_loss: 0.2944 - val_accuracy: 0.8907\n",
      "Epoch 34/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3132 - accuracy: 0.8832 - val_loss: 0.2944 - val_accuracy: 0.8912\n",
      "Epoch 35/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3117 - accuracy: 0.8850 - val_loss: 0.2942 - val_accuracy: 0.8920\n",
      "Epoch 36/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.3102 - accuracy: 0.8857 - val_loss: 0.2921 - val_accuracy: 0.8943\n",
      "Epoch 37/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3104 - accuracy: 0.8845 - val_loss: 0.2913 - val_accuracy: 0.8923\n",
      "Epoch 38/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3070 - accuracy: 0.8860 - val_loss: 0.2903 - val_accuracy: 0.8940\n",
      "Epoch 39/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3043 - accuracy: 0.8874 - val_loss: 0.2915 - val_accuracy: 0.8943\n",
      "Epoch 40/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2997 - accuracy: 0.8894 - val_loss: 0.2865 - val_accuracy: 0.8949\n",
      "Epoch 41/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3009 - accuracy: 0.8892 - val_loss: 0.2829 - val_accuracy: 0.8959\n",
      "Epoch 42/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.3004 - accuracy: 0.8887 - val_loss: 0.2842 - val_accuracy: 0.8975\n",
      "Epoch 43/100\n",
      "240/240 [==============================] - 9s 36ms/step - loss: 0.2962 - accuracy: 0.8903 - val_loss: 0.2821 - val_accuracy: 0.8975\n",
      "Epoch 44/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2939 - accuracy: 0.8908 - val_loss: 0.2800 - val_accuracy: 0.8988\n",
      "Epoch 45/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2917 - accuracy: 0.8906 - val_loss: 0.2810 - val_accuracy: 0.8989\n",
      "Epoch 46/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2929 - accuracy: 0.8916 - val_loss: 0.2774 - val_accuracy: 0.8997\n",
      "Epoch 47/100\n",
      "240/240 [==============================] - 9s 35ms/step - loss: 0.2868 - accuracy: 0.8933 - val_loss: 0.2776 - val_accuracy: 0.8984\n",
      "Epoch 48/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2868 - accuracy: 0.8939 - val_loss: 0.2768 - val_accuracy: 0.8990\n",
      "Epoch 49/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2820 - accuracy: 0.8977 - val_loss: 0.2790 - val_accuracy: 0.8989\n",
      "Epoch 50/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2842 - accuracy: 0.8945 - val_loss: 0.2793 - val_accuracy: 0.8971\n",
      "Epoch 51/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2822 - accuracy: 0.8957 - val_loss: 0.2750 - val_accuracy: 0.8997\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2809 - accuracy: 0.8947 - val_loss: 0.2760 - val_accuracy: 0.8995\n",
      "Epoch 53/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2825 - accuracy: 0.8960 - val_loss: 0.2744 - val_accuracy: 0.8994\n",
      "Epoch 54/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2831 - accuracy: 0.8952 - val_loss: 0.2743 - val_accuracy: 0.9021\n",
      "Epoch 55/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2770 - accuracy: 0.8976 - val_loss: 0.2720 - val_accuracy: 0.9012\n",
      "Epoch 56/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2770 - accuracy: 0.8970 - val_loss: 0.2693 - val_accuracy: 0.9034\n",
      "Epoch 57/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2740 - accuracy: 0.8979 - val_loss: 0.2734 - val_accuracy: 0.9009\n",
      "Epoch 58/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2726 - accuracy: 0.8996 - val_loss: 0.2713 - val_accuracy: 0.9020\n",
      "Epoch 59/100\n",
      "240/240 [==============================] - 8s 35ms/step - loss: 0.2717 - accuracy: 0.8990 - val_loss: 0.2706 - val_accuracy: 0.9019\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2693 - accuracy: 0.9034\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 1.4345 - accuracy: 0.4807 - val_loss: 0.7498 - val_accuracy: 0.7295\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.7827 - accuracy: 0.7123 - val_loss: 0.6115 - val_accuracy: 0.7684\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.6764 - accuracy: 0.7509 - val_loss: 0.5688 - val_accuracy: 0.7812\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.6240 - accuracy: 0.7678 - val_loss: 0.5314 - val_accuracy: 0.7955\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.5873 - accuracy: 0.7834 - val_loss: 0.5024 - val_accuracy: 0.8104\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.5551 - accuracy: 0.7944 - val_loss: 0.4861 - val_accuracy: 0.8193\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.5380 - accuracy: 0.8008 - val_loss: 0.4672 - val_accuracy: 0.8259\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.5205 - accuracy: 0.8072 - val_loss: 0.4510 - val_accuracy: 0.8328\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.5056 - accuracy: 0.8139 - val_loss: 0.4422 - val_accuracy: 0.8342\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4936 - accuracy: 0.8187 - val_loss: 0.4314 - val_accuracy: 0.8413\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4785 - accuracy: 0.8230 - val_loss: 0.4165 - val_accuracy: 0.8462\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4703 - accuracy: 0.8270 - val_loss: 0.4089 - val_accuracy: 0.8504\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4619 - accuracy: 0.8293 - val_loss: 0.4039 - val_accuracy: 0.8531\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4544 - accuracy: 0.8330 - val_loss: 0.3986 - val_accuracy: 0.8541\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4457 - accuracy: 0.8357 - val_loss: 0.3914 - val_accuracy: 0.8567\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4385 - accuracy: 0.8378 - val_loss: 0.3883 - val_accuracy: 0.8571\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4261 - accuracy: 0.8432 - val_loss: 0.3810 - val_accuracy: 0.8594\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4246 - accuracy: 0.8441 - val_loss: 0.3744 - val_accuracy: 0.8638\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4218 - accuracy: 0.8429 - val_loss: 0.3699 - val_accuracy: 0.8647\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4131 - accuracy: 0.8489 - val_loss: 0.3677 - val_accuracy: 0.8651\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4090 - accuracy: 0.8489 - val_loss: 0.3636 - val_accuracy: 0.8637\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.4009 - accuracy: 0.8528 - val_loss: 0.3591 - val_accuracy: 0.8681\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.4017 - accuracy: 0.8519 - val_loss: 0.3555 - val_accuracy: 0.8681\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3933 - accuracy: 0.8543 - val_loss: 0.3522 - val_accuracy: 0.8713\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3913 - accuracy: 0.8554 - val_loss: 0.3493 - val_accuracy: 0.8704\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3861 - accuracy: 0.8580 - val_loss: 0.3450 - val_accuracy: 0.8733\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3809 - accuracy: 0.8600 - val_loss: 0.3441 - val_accuracy: 0.8733\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3775 - accuracy: 0.8609 - val_loss: 0.3411 - val_accuracy: 0.8745\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3757 - accuracy: 0.8622 - val_loss: 0.3369 - val_accuracy: 0.8763\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3695 - accuracy: 0.8637 - val_loss: 0.3348 - val_accuracy: 0.8760\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3683 - accuracy: 0.8630 - val_loss: 0.3329 - val_accuracy: 0.8778\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3655 - accuracy: 0.8654 - val_loss: 0.3300 - val_accuracy: 0.8786\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3607 - accuracy: 0.8657 - val_loss: 0.3280 - val_accuracy: 0.8794\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3586 - accuracy: 0.8675 - val_loss: 0.3278 - val_accuracy: 0.8778\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.3548 - accuracy: 0.8679 - val_loss: 0.3212 - val_accuracy: 0.8816\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3512 - accuracy: 0.8705 - val_loss: 0.3219 - val_accuracy: 0.8797\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3490 - accuracy: 0.8702 - val_loss: 0.3180 - val_accuracy: 0.8826\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3479 - accuracy: 0.8708 - val_loss: 0.3192 - val_accuracy: 0.8818\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3451 - accuracy: 0.8717 - val_loss: 0.3172 - val_accuracy: 0.8817\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3444 - accuracy: 0.8725 - val_loss: 0.3118 - val_accuracy: 0.8844\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3381 - accuracy: 0.8761 - val_loss: 0.3108 - val_accuracy: 0.8855\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 8s 71ms/step - loss: 0.3364 - accuracy: 0.8753 - val_loss: 0.3082 - val_accuracy: 0.8867\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3360 - accuracy: 0.8753 - val_loss: 0.3072 - val_accuracy: 0.8887\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3315 - accuracy: 0.8764 - val_loss: 0.3047 - val_accuracy: 0.8885\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3302 - accuracy: 0.8773 - val_loss: 0.3060 - val_accuracy: 0.8858\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3296 - accuracy: 0.8772 - val_loss: 0.3047 - val_accuracy: 0.8871\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3272 - accuracy: 0.8793 - val_loss: 0.3002 - val_accuracy: 0.8897\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3251 - accuracy: 0.8794 - val_loss: 0.3008 - val_accuracy: 0.8871\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3226 - accuracy: 0.8792 - val_loss: 0.3002 - val_accuracy: 0.8871\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3192 - accuracy: 0.8820 - val_loss: 0.2986 - val_accuracy: 0.8888\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3207 - accuracy: 0.8813 - val_loss: 0.2985 - val_accuracy: 0.8899\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3170 - accuracy: 0.8823 - val_loss: 0.2953 - val_accuracy: 0.8904\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3177 - accuracy: 0.8821 - val_loss: 0.2940 - val_accuracy: 0.8912\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3147 - accuracy: 0.8840 - val_loss: 0.2953 - val_accuracy: 0.8893\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3122 - accuracy: 0.8837 - val_loss: 0.2950 - val_accuracy: 0.8907\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3099 - accuracy: 0.8853 - val_loss: 0.2913 - val_accuracy: 0.8908\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3088 - accuracy: 0.8856 - val_loss: 0.2907 - val_accuracy: 0.8921\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3101 - accuracy: 0.8848 - val_loss: 0.2882 - val_accuracy: 0.8917\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3054 - accuracy: 0.8869 - val_loss: 0.2918 - val_accuracy: 0.8919\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3043 - accuracy: 0.8866 - val_loss: 0.2864 - val_accuracy: 0.8945\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3033 - accuracy: 0.8878 - val_loss: 0.2840 - val_accuracy: 0.8946\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 8s 70ms/step - loss: 0.3019 - accuracy: 0.8892 - val_loss: 0.2861 - val_accuracy: 0.8937\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3007 - accuracy: 0.8881 - val_loss: 0.2871 - val_accuracy: 0.8914\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 8s 69ms/step - loss: 0.3003 - accuracy: 0.8889 - val_loss: 0.2849 - val_accuracy: 0.8949\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2840 - accuracy: 0.8946\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.4914 - accuracy: 0.8233 - val_loss: 0.3723 - val_accuracy: 0.8658\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.3360 - accuracy: 0.8774 - val_loss: 0.3280 - val_accuracy: 0.8780\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2867 - accuracy: 0.8955 - val_loss: 0.3077 - val_accuracy: 0.8861\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.2595 - accuracy: 0.9041 - val_loss: 0.2901 - val_accuracy: 0.8927\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2339 - accuracy: 0.9119 - val_loss: 0.2818 - val_accuracy: 0.8960\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2153 - accuracy: 0.9198 - val_loss: 0.2704 - val_accuracy: 0.9022\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1962 - accuracy: 0.9268 - val_loss: 0.2712 - val_accuracy: 0.9025\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.1796 - accuracy: 0.9327 - val_loss: 0.2936 - val_accuracy: 0.8971\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 33s 56ms/step - loss: 0.1633 - accuracy: 0.9387 - val_loss: 0.2842 - val_accuracy: 0.9052\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2704 - accuracy: 0.9022\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.6176 - accuracy: 0.7783 - val_loss: 0.4841 - val_accuracy: 0.8141\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.4117 - accuracy: 0.8523 - val_loss: 0.3861 - val_accuracy: 0.8611\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3603 - accuracy: 0.8705 - val_loss: 0.3645 - val_accuracy: 0.8669\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3262 - accuracy: 0.8813 - val_loss: 0.3405 - val_accuracy: 0.8796\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 28s 117ms/step - loss: 0.3008 - accuracy: 0.8898 - val_loss: 0.3102 - val_accuracy: 0.8888\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 28s 118ms/step - loss: 0.2785 - accuracy: 0.8964 - val_loss: 0.3187 - val_accuracy: 0.8849\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2631 - accuracy: 0.9036 - val_loss: 0.3030 - val_accuracy: 0.8932\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2468 - accuracy: 0.9086 - val_loss: 0.2942 - val_accuracy: 0.8953\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2351 - accuracy: 0.9127 - val_loss: 0.3088 - val_accuracy: 0.8895\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2233 - accuracy: 0.9176 - val_loss: 0.2953 - val_accuracy: 0.8964\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2066 - accuracy: 0.9230 - val_loss: 0.2939 - val_accuracy: 0.8974\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1988 - accuracy: 0.9259 - val_loss: 0.2831 - val_accuracy: 0.8967\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.1900 - accuracy: 0.9286 - val_loss: 0.2923 - val_accuracy: 0.9016\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.1754 - accuracy: 0.9345 - val_loss: 0.2869 - val_accuracy: 0.9003\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1647 - accuracy: 0.9384 - val_loss: 0.2863 - val_accuracy: 0.9008\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2831 - accuracy: 0.8967\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.6749 - accuracy: 0.7571 - val_loss: 0.4881 - val_accuracy: 0.8208\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.4160 - accuracy: 0.8519 - val_loss: 0.4041 - val_accuracy: 0.8544\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.3645 - accuracy: 0.8690 - val_loss: 0.3864 - val_accuracy: 0.8598\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.3298 - accuracy: 0.8811 - val_loss: 0.3475 - val_accuracy: 0.8755\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.3039 - accuracy: 0.8892 - val_loss: 0.3497 - val_accuracy: 0.8718\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2854 - accuracy: 0.8951 - val_loss: 0.3184 - val_accuracy: 0.8841\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.2745 - accuracy: 0.8983 - val_loss: 0.3046 - val_accuracy: 0.8911\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2549 - accuracy: 0.9058 - val_loss: 0.3001 - val_accuracy: 0.8899\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.2449 - accuracy: 0.9098 - val_loss: 0.3027 - val_accuracy: 0.8898\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2318 - accuracy: 0.9154 - val_loss: 0.2837 - val_accuracy: 0.8959\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2225 - accuracy: 0.9174 - val_loss: 0.2906 - val_accuracy: 0.8960\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2124 - accuracy: 0.9220 - val_loss: 0.2861 - val_accuracy: 0.8980\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2052 - accuracy: 0.9250 - val_loss: 0.2920 - val_accuracy: 0.8981\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2837 - accuracy: 0.8959\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.5945 - accuracy: 0.7855 - val_loss: 0.4003 - val_accuracy: 0.8619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.3925 - accuracy: 0.8600 - val_loss: 0.3472 - val_accuracy: 0.8767\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.3404 - accuracy: 0.8773 - val_loss: 0.3065 - val_accuracy: 0.8894\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.3125 - accuracy: 0.8882 - val_loss: 0.2984 - val_accuracy: 0.8886\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2900 - accuracy: 0.8942 - val_loss: 0.2966 - val_accuracy: 0.8893\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2738 - accuracy: 0.8992 - val_loss: 0.2795 - val_accuracy: 0.8965\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2582 - accuracy: 0.9061 - val_loss: 0.2802 - val_accuracy: 0.8952\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2475 - accuracy: 0.9090 - val_loss: 0.2757 - val_accuracy: 0.9016\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2334 - accuracy: 0.9136 - val_loss: 0.2819 - val_accuracy: 0.8987\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2278 - accuracy: 0.9168 - val_loss: 0.2761 - val_accuracy: 0.8998\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2157 - accuracy: 0.9183 - val_loss: 0.3059 - val_accuracy: 0.8938\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2757 - accuracy: 0.9016\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.7085 - accuracy: 0.7441 - val_loss: 0.4517 - val_accuracy: 0.8361\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.4426 - accuracy: 0.8412 - val_loss: 0.3815 - val_accuracy: 0.8623\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.3836 - accuracy: 0.8612 - val_loss: 0.3444 - val_accuracy: 0.8763\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.3509 - accuracy: 0.8732 - val_loss: 0.3227 - val_accuracy: 0.8855\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.3252 - accuracy: 0.8817 - val_loss: 0.3160 - val_accuracy: 0.8856\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.3064 - accuracy: 0.8880 - val_loss: 0.2990 - val_accuracy: 0.8918\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2905 - accuracy: 0.8943 - val_loss: 0.2908 - val_accuracy: 0.8950\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2775 - accuracy: 0.8979 - val_loss: 0.2870 - val_accuracy: 0.8980\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.2654 - accuracy: 0.9021 - val_loss: 0.2853 - val_accuracy: 0.8989\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2526 - accuracy: 0.9072 - val_loss: 0.2909 - val_accuracy: 0.8957\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2438 - accuracy: 0.9102 - val_loss: 0.2868 - val_accuracy: 0.9006\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.2365 - accuracy: 0.9129 - val_loss: 0.2714 - val_accuracy: 0.9051\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.2245 - accuracy: 0.9169 - val_loss: 0.2730 - val_accuracy: 0.9064\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.2198 - accuracy: 0.9191 - val_loss: 0.2657 - val_accuracy: 0.9080\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.2094 - accuracy: 0.9219 - val_loss: 0.2658 - val_accuracy: 0.9086\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2058 - accuracy: 0.9227 - val_loss: 0.2789 - val_accuracy: 0.9067\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 27s 113ms/step - loss: 0.1969 - accuracy: 0.9268 - val_loss: 0.2659 - val_accuracy: 0.9096\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2657 - accuracy: 0.9080\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.8695 - accuracy: 0.6822 - val_loss: 0.5205 - val_accuracy: 0.8040\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.5225 - accuracy: 0.8104 - val_loss: 0.4187 - val_accuracy: 0.8493\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.4347 - accuracy: 0.8431 - val_loss: 0.3812 - val_accuracy: 0.8605\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3938 - accuracy: 0.8594 - val_loss: 0.3643 - val_accuracy: 0.8666\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.3682 - accuracy: 0.8679 - val_loss: 0.3337 - val_accuracy: 0.8822\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.3426 - accuracy: 0.8768 - val_loss: 0.3227 - val_accuracy: 0.8809\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3244 - accuracy: 0.8830 - val_loss: 0.3116 - val_accuracy: 0.8869\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.3114 - accuracy: 0.8877 - val_loss: 0.2993 - val_accuracy: 0.8946\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2966 - accuracy: 0.8921 - val_loss: 0.2949 - val_accuracy: 0.8909\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2868 - accuracy: 0.8964 - val_loss: 0.2838 - val_accuracy: 0.8998\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2785 - accuracy: 0.8985 - val_loss: 0.2784 - val_accuracy: 0.8983\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2697 - accuracy: 0.9036 - val_loss: 0.2725 - val_accuracy: 0.9009\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2587 - accuracy: 0.9067 - val_loss: 0.2728 - val_accuracy: 0.9013\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2522 - accuracy: 0.9085 - val_loss: 0.2651 - val_accuracy: 0.9050\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2462 - accuracy: 0.9099 - val_loss: 0.2707 - val_accuracy: 0.9034\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.2349 - accuracy: 0.9142 - val_loss: 0.2699 - val_accuracy: 0.9044\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2323 - accuracy: 0.9140 - val_loss: 0.2601 - val_accuracy: 0.9057\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.2260 - accuracy: 0.9174 - val_loss: 0.2695 - val_accuracy: 0.9035\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2181 - accuracy: 0.9192 - val_loss: 0.2634 - val_accuracy: 0.9057\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2121 - accuracy: 0.9225 - val_loss: 0.2569 - val_accuracy: 0.9093\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.2060 - accuracy: 0.9241 - val_loss: 0.2597 - val_accuracy: 0.9074\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.2015 - accuracy: 0.9254 - val_loss: 0.2585 - val_accuracy: 0.9096\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.1967 - accuracy: 0.9261 - val_loss: 0.2654 - val_accuracy: 0.9095\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2569 - accuracy: 0.9093\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.5789 - accuracy: 0.7880 - val_loss: 0.4458 - val_accuracy: 0.8404\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.3936 - accuracy: 0.8609 - val_loss: 0.3778 - val_accuracy: 0.8649\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.3404 - accuracy: 0.8786 - val_loss: 0.3513 - val_accuracy: 0.8750\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.3072 - accuracy: 0.8890 - val_loss: 0.3275 - val_accuracy: 0.8816\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2815 - accuracy: 0.8980 - val_loss: 0.3504 - val_accuracy: 0.8746\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2618 - accuracy: 0.9040 - val_loss: 0.2997 - val_accuracy: 0.8943\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2432 - accuracy: 0.9115 - val_loss: 0.3044 - val_accuracy: 0.8891\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2287 - accuracy: 0.9158 - val_loss: 0.2765 - val_accuracy: 0.8997\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2137 - accuracy: 0.9215 - val_loss: 0.2770 - val_accuracy: 0.8992\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.1997 - accuracy: 0.9258 - val_loss: 0.2743 - val_accuracy: 0.9024\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1866 - accuracy: 0.9305 - val_loss: 0.2875 - val_accuracy: 0.9001\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.1759 - accuracy: 0.9349 - val_loss: 0.2722 - val_accuracy: 0.9042\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.1657 - accuracy: 0.9388 - val_loss: 0.2767 - val_accuracy: 0.9043\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1556 - accuracy: 0.9426 - val_loss: 0.2747 - val_accuracy: 0.9057\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1431 - accuracy: 0.9472 - val_loss: 0.2852 - val_accuracy: 0.9056\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2722 - accuracy: 0.9042\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.6357 - accuracy: 0.7823 - val_loss: 0.4857 - val_accuracy: 0.8261\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.4223 - accuracy: 0.8523 - val_loss: 0.4493 - val_accuracy: 0.8452\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 27s 115ms/step - loss: 0.3703 - accuracy: 0.8694 - val_loss: 0.3768 - val_accuracy: 0.8683\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.3361 - accuracy: 0.8808 - val_loss: 0.3603 - val_accuracy: 0.8712\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3157 - accuracy: 0.8872 - val_loss: 0.3380 - val_accuracy: 0.8774\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2939 - accuracy: 0.8953 - val_loss: 0.3418 - val_accuracy: 0.8761\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2790 - accuracy: 0.9006 - val_loss: 0.3276 - val_accuracy: 0.8798\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2656 - accuracy: 0.9048 - val_loss: 0.3177 - val_accuracy: 0.8878\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2528 - accuracy: 0.9082 - val_loss: 0.3104 - val_accuracy: 0.8913\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2396 - accuracy: 0.9128 - val_loss: 0.3121 - val_accuracy: 0.8866\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2310 - accuracy: 0.9158 - val_loss: 0.2981 - val_accuracy: 0.8970\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2203 - accuracy: 0.9192 - val_loss: 0.2999 - val_accuracy: 0.8962\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2093 - accuracy: 0.9234 - val_loss: 0.2872 - val_accuracy: 0.8989\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1986 - accuracy: 0.9269 - val_loss: 0.2796 - val_accuracy: 0.9016\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1916 - accuracy: 0.9297 - val_loss: 0.2808 - val_accuracy: 0.9021\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 27s 115ms/step - loss: 0.1841 - accuracy: 0.9319 - val_loss: 0.2985 - val_accuracy: 0.8950\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.1740 - accuracy: 0.9373 - val_loss: 0.2963 - val_accuracy: 0.9012\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2796 - accuracy: 0.9016\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.7896 - accuracy: 0.7173 - val_loss: 0.5548 - val_accuracy: 0.7960\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.4989 - accuracy: 0.8190 - val_loss: 0.4795 - val_accuracy: 0.8268\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.4450 - accuracy: 0.8414 - val_loss: 0.4456 - val_accuracy: 0.8372\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.4065 - accuracy: 0.8547 - val_loss: 0.4254 - val_accuracy: 0.8500\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3857 - accuracy: 0.8618 - val_loss: 0.3912 - val_accuracy: 0.8596\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3666 - accuracy: 0.8697 - val_loss: 0.3861 - val_accuracy: 0.8627\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3492 - accuracy: 0.8748 - val_loss: 0.3676 - val_accuracy: 0.8692\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3305 - accuracy: 0.8824 - val_loss: 0.3600 - val_accuracy: 0.8719\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3169 - accuracy: 0.8870 - val_loss: 0.3403 - val_accuracy: 0.8796\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3064 - accuracy: 0.8896 - val_loss: 0.3330 - val_accuracy: 0.8798\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2953 - accuracy: 0.8944 - val_loss: 0.3438 - val_accuracy: 0.8772\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2871 - accuracy: 0.8973 - val_loss: 0.3267 - val_accuracy: 0.8832\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2794 - accuracy: 0.8993 - val_loss: 0.3284 - val_accuracy: 0.8829\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.2719 - accuracy: 0.9029 - val_loss: 0.3121 - val_accuracy: 0.8897\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2665 - accuracy: 0.9040 - val_loss: 0.3155 - val_accuracy: 0.8856\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2522 - accuracy: 0.9090 - val_loss: 0.3074 - val_accuracy: 0.8879\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2457 - accuracy: 0.9119 - val_loss: 0.2977 - val_accuracy: 0.8918\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2439 - accuracy: 0.9128 - val_loss: 0.3067 - val_accuracy: 0.8879\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2410 - accuracy: 0.9128 - val_loss: 0.3173 - val_accuracy: 0.8846\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2333 - accuracy: 0.9148 - val_loss: 0.2954 - val_accuracy: 0.8958\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2268 - accuracy: 0.9174 - val_loss: 0.3018 - val_accuracy: 0.8912\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2244 - accuracy: 0.9176 - val_loss: 0.2858 - val_accuracy: 0.8978\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 26s 218ms/step - loss: 0.2161 - accuracy: 0.9220 - val_loss: 0.2929 - val_accuracy: 0.8955\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2107 - accuracy: 0.9239 - val_loss: 0.2898 - val_accuracy: 0.8967\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.2077 - accuracy: 0.9252 - val_loss: 0.2958 - val_accuracy: 0.8945\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2858 - accuracy: 0.8978\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.7096 - accuracy: 0.7435 - val_loss: 0.4745 - val_accuracy: 0.8298\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 33s 55ms/step - loss: 0.4699 - accuracy: 0.8310 - val_loss: 0.3827 - val_accuracy: 0.8618\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.4031 - accuracy: 0.8552 - val_loss: 0.3641 - val_accuracy: 0.8675\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.3642 - accuracy: 0.8693 - val_loss: 0.3253 - val_accuracy: 0.8847\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.3401 - accuracy: 0.8775 - val_loss: 0.3182 - val_accuracy: 0.8860\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.3180 - accuracy: 0.8851 - val_loss: 0.3011 - val_accuracy: 0.8898\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.3021 - accuracy: 0.8906 - val_loss: 0.2917 - val_accuracy: 0.8953\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.2859 - accuracy: 0.8958 - val_loss: 0.2895 - val_accuracy: 0.8959\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2758 - accuracy: 0.8998 - val_loss: 0.2811 - val_accuracy: 0.8995\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2654 - accuracy: 0.9035 - val_loss: 0.2824 - val_accuracy: 0.8981\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2548 - accuracy: 0.9070 - val_loss: 0.2697 - val_accuracy: 0.9030\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2449 - accuracy: 0.9111 - val_loss: 0.2690 - val_accuracy: 0.9032\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2366 - accuracy: 0.9138 - val_loss: 0.2587 - val_accuracy: 0.9093\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2297 - accuracy: 0.9160 - val_loss: 0.2656 - val_accuracy: 0.9039\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2206 - accuracy: 0.9187 - val_loss: 0.2653 - val_accuracy: 0.9059\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.2152 - accuracy: 0.9197 - val_loss: 0.2519 - val_accuracy: 0.9100\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 33s 54ms/step - loss: 0.2051 - accuracy: 0.9251 - val_loss: 0.2537 - val_accuracy: 0.9097\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1989 - accuracy: 0.9276 - val_loss: 0.2514 - val_accuracy: 0.9112\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.1937 - accuracy: 0.9277 - val_loss: 0.2552 - val_accuracy: 0.9127\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 33s 55ms/step - loss: 0.1880 - accuracy: 0.9300 - val_loss: 0.2570 - val_accuracy: 0.9133\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 32s 54ms/step - loss: 0.1823 - accuracy: 0.9320 - val_loss: 0.2594 - val_accuracy: 0.9092\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2514 - accuracy: 0.9112\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.8056 - accuracy: 0.7154 - val_loss: 0.4880 - val_accuracy: 0.8208\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.4921 - accuracy: 0.8242 - val_loss: 0.4075 - val_accuracy: 0.8541\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.4202 - accuracy: 0.8492 - val_loss: 0.3666 - val_accuracy: 0.8693\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3776 - accuracy: 0.8654 - val_loss: 0.3363 - val_accuracy: 0.8778\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.3513 - accuracy: 0.8749 - val_loss: 0.3202 - val_accuracy: 0.8844\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 28s 116ms/step - loss: 0.3331 - accuracy: 0.8799 - val_loss: 0.3066 - val_accuracy: 0.8888\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.3145 - accuracy: 0.8869 - val_loss: 0.2941 - val_accuracy: 0.8951\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.3021 - accuracy: 0.8912 - val_loss: 0.2920 - val_accuracy: 0.8952\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2897 - accuracy: 0.8963 - val_loss: 0.2798 - val_accuracy: 0.8975\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2794 - accuracy: 0.8990 - val_loss: 0.2762 - val_accuracy: 0.9010\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2691 - accuracy: 0.9026 - val_loss: 0.2764 - val_accuracy: 0.9016\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2590 - accuracy: 0.9060 - val_loss: 0.2629 - val_accuracy: 0.9038\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2501 - accuracy: 0.9094 - val_loss: 0.2663 - val_accuracy: 0.9058\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2418 - accuracy: 0.9123 - val_loss: 0.2617 - val_accuracy: 0.9071\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 28s 115ms/step - loss: 0.2357 - accuracy: 0.9144 - val_loss: 0.2555 - val_accuracy: 0.9072\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2290 - accuracy: 0.9169 - val_loss: 0.2563 - val_accuracy: 0.9074\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2215 - accuracy: 0.9186 - val_loss: 0.2608 - val_accuracy: 0.9044\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 27s 114ms/step - loss: 0.2187 - accuracy: 0.9206 - val_loss: 0.2563 - val_accuracy: 0.9076\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2555 - accuracy: 0.9072\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.9344 - accuracy: 0.6676 - val_loss: 0.5466 - val_accuracy: 0.7977\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.5611 - accuracy: 0.7954 - val_loss: 0.4569 - val_accuracy: 0.8343\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.4805 - accuracy: 0.8277 - val_loss: 0.4082 - val_accuracy: 0.8533\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.4395 - accuracy: 0.8458 - val_loss: 0.3855 - val_accuracy: 0.8617\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.4058 - accuracy: 0.8550 - val_loss: 0.3712 - val_accuracy: 0.8672\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3823 - accuracy: 0.8632 - val_loss: 0.3489 - val_accuracy: 0.8733\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3631 - accuracy: 0.8699 - val_loss: 0.3399 - val_accuracy: 0.8770\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.3472 - accuracy: 0.8757 - val_loss: 0.3265 - val_accuracy: 0.8846\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3326 - accuracy: 0.8795 - val_loss: 0.3122 - val_accuracy: 0.8880\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.3205 - accuracy: 0.8851 - val_loss: 0.3053 - val_accuracy: 0.8901\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 26s 216ms/step - loss: 0.3086 - accuracy: 0.8887 - val_loss: 0.3028 - val_accuracy: 0.8906\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.3005 - accuracy: 0.8917 - val_loss: 0.2906 - val_accuracy: 0.8953\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2911 - accuracy: 0.8950 - val_loss: 0.2858 - val_accuracy: 0.8967\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2816 - accuracy: 0.8978 - val_loss: 0.2796 - val_accuracy: 0.8977\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 26s 217ms/step - loss: 0.2742 - accuracy: 0.9004 - val_loss: 0.2829 - val_accuracy: 0.8981\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2709 - accuracy: 0.9012 - val_loss: 0.2754 - val_accuracy: 0.8996\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 26s 215ms/step - loss: 0.2624 - accuracy: 0.9041 - val_loss: 0.2733 - val_accuracy: 0.8984\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2555 - accuracy: 0.9071 - val_loss: 0.2701 - val_accuracy: 0.8998\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2532 - accuracy: 0.9084 - val_loss: 0.2675 - val_accuracy: 0.9026\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2436 - accuracy: 0.9113 - val_loss: 0.2630 - val_accuracy: 0.9046\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2403 - accuracy: 0.9132 - val_loss: 0.2613 - val_accuracy: 0.9045\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2360 - accuracy: 0.9142 - val_loss: 0.2602 - val_accuracy: 0.9069\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2296 - accuracy: 0.9155 - val_loss: 0.2596 - val_accuracy: 0.9062\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2265 - accuracy: 0.9171 - val_loss: 0.2632 - val_accuracy: 0.9081\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2232 - accuracy: 0.9187 - val_loss: 0.2682 - val_accuracy: 0.9075\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.2169 - accuracy: 0.9204 - val_loss: 0.2556 - val_accuracy: 0.9086\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2105 - accuracy: 0.9230 - val_loss: 0.2522 - val_accuracy: 0.9088\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2109 - accuracy: 0.9227 - val_loss: 0.2475 - val_accuracy: 0.9113\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 26s 213ms/step - loss: 0.2023 - accuracy: 0.9255 - val_loss: 0.2532 - val_accuracy: 0.9091\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 26s 214ms/step - loss: 0.1994 - accuracy: 0.9266 - val_loss: 0.2485 - val_accuracy: 0.9103\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 25s 212ms/step - loss: 0.1981 - accuracy: 0.9266 - val_loss: 0.2566 - val_accuracy: 0.9083\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.2475 - accuracy: 0.9113\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.7003 - accuracy: 0.7388 - val_loss: 0.5324 - val_accuracy: 0.7923\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4490 - accuracy: 0.8367 - val_loss: 0.4194 - val_accuracy: 0.8484\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3847 - accuracy: 0.8617 - val_loss: 0.3954 - val_accuracy: 0.8593\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3508 - accuracy: 0.8744 - val_loss: 0.3703 - val_accuracy: 0.8667\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3268 - accuracy: 0.8820 - val_loss: 0.3560 - val_accuracy: 0.8748\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3117 - accuracy: 0.8867 - val_loss: 0.3386 - val_accuracy: 0.8771\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2969 - accuracy: 0.8917 - val_loss: 0.3298 - val_accuracy: 0.8817\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2823 - accuracy: 0.8976 - val_loss: 0.3187 - val_accuracy: 0.8855\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2730 - accuracy: 0.8993 - val_loss: 0.3266 - val_accuracy: 0.8838\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2631 - accuracy: 0.9023 - val_loss: 0.3090 - val_accuracy: 0.8875\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2537 - accuracy: 0.9067 - val_loss: 0.3172 - val_accuracy: 0.8892\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2462 - accuracy: 0.9093 - val_loss: 0.3020 - val_accuracy: 0.8912\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2364 - accuracy: 0.9130 - val_loss: 0.3206 - val_accuracy: 0.8852\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2311 - accuracy: 0.9141 - val_loss: 0.3053 - val_accuracy: 0.8913\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2219 - accuracy: 0.9184 - val_loss: 0.3112 - val_accuracy: 0.8921\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3020 - accuracy: 0.8912\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.8194 - accuracy: 0.7048 - val_loss: 0.5816 - val_accuracy: 0.7797\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.5139 - accuracy: 0.8103 - val_loss: 0.4989 - val_accuracy: 0.8190\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4521 - accuracy: 0.8372 - val_loss: 0.4571 - val_accuracy: 0.8305\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4123 - accuracy: 0.8507 - val_loss: 0.4249 - val_accuracy: 0.8454\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3852 - accuracy: 0.8598 - val_loss: 0.4073 - val_accuracy: 0.8535\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3640 - accuracy: 0.8680 - val_loss: 0.3820 - val_accuracy: 0.8613\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3506 - accuracy: 0.8726 - val_loss: 0.3709 - val_accuracy: 0.8659\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3363 - accuracy: 0.8775 - val_loss: 0.3710 - val_accuracy: 0.8687\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3249 - accuracy: 0.8814 - val_loss: 0.3497 - val_accuracy: 0.8727\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3130 - accuracy: 0.8862 - val_loss: 0.3553 - val_accuracy: 0.8701\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3076 - accuracy: 0.8874 - val_loss: 0.3340 - val_accuracy: 0.8804\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3002 - accuracy: 0.8910 - val_loss: 0.3298 - val_accuracy: 0.8800\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2916 - accuracy: 0.8940 - val_loss: 0.3244 - val_accuracy: 0.8819\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2841 - accuracy: 0.8970 - val_loss: 0.3277 - val_accuracy: 0.8834\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 9s 40ms/step - loss: 0.2773 - accuracy: 0.8985 - val_loss: 0.3282 - val_accuracy: 0.8821\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2698 - accuracy: 0.9008 - val_loss: 0.3165 - val_accuracy: 0.8840\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2649 - accuracy: 0.9030 - val_loss: 0.3173 - val_accuracy: 0.8853\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2614 - accuracy: 0.9044 - val_loss: 0.3146 - val_accuracy: 0.8879\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2553 - accuracy: 0.9075 - val_loss: 0.3181 - val_accuracy: 0.8839\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2505 - accuracy: 0.9083 - val_loss: 0.3296 - val_accuracy: 0.8833\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2449 - accuracy: 0.9104 - val_loss: 0.3068 - val_accuracy: 0.8899\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2423 - accuracy: 0.9119 - val_loss: 0.3092 - val_accuracy: 0.8882\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2362 - accuracy: 0.9137 - val_loss: 0.3106 - val_accuracy: 0.8895\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2333 - accuracy: 0.9144 - val_loss: 0.3018 - val_accuracy: 0.8937\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2288 - accuracy: 0.9165 - val_loss: 0.3087 - val_accuracy: 0.8907\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2255 - accuracy: 0.9171 - val_loss: 0.3014 - val_accuracy: 0.8938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2221 - accuracy: 0.9187 - val_loss: 0.3092 - val_accuracy: 0.8883\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2196 - accuracy: 0.9187 - val_loss: 0.3056 - val_accuracy: 0.8921\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 10s 42ms/step - loss: 0.2154 - accuracy: 0.9201 - val_loss: 0.3104 - val_accuracy: 0.8886\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3014 - accuracy: 0.8938\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.9354 - accuracy: 0.6776 - val_loss: 0.6267 - val_accuracy: 0.7699\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.5611 - accuracy: 0.7904 - val_loss: 0.5443 - val_accuracy: 0.7993\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4917 - accuracy: 0.8201 - val_loss: 0.4951 - val_accuracy: 0.8204\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4535 - accuracy: 0.8375 - val_loss: 0.4638 - val_accuracy: 0.8336\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 9s 78ms/step - loss: 0.4250 - accuracy: 0.8480 - val_loss: 0.4450 - val_accuracy: 0.8429\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4025 - accuracy: 0.8554 - val_loss: 0.4103 - val_accuracy: 0.8532\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3819 - accuracy: 0.8619 - val_loss: 0.4122 - val_accuracy: 0.8509\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3747 - accuracy: 0.8647 - val_loss: 0.3922 - val_accuracy: 0.8610\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3568 - accuracy: 0.8707 - val_loss: 0.3825 - val_accuracy: 0.8612\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 9s 78ms/step - loss: 0.3441 - accuracy: 0.8753 - val_loss: 0.3673 - val_accuracy: 0.8726\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3370 - accuracy: 0.8782 - val_loss: 0.3694 - val_accuracy: 0.8703\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3244 - accuracy: 0.8826 - val_loss: 0.3561 - val_accuracy: 0.8742\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3176 - accuracy: 0.8859 - val_loss: 0.3520 - val_accuracy: 0.8760\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3104 - accuracy: 0.8877 - val_loss: 0.3425 - val_accuracy: 0.8762\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3050 - accuracy: 0.8885 - val_loss: 0.3408 - val_accuracy: 0.8784\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3009 - accuracy: 0.8905 - val_loss: 0.3429 - val_accuracy: 0.8795\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2934 - accuracy: 0.8927 - val_loss: 0.3311 - val_accuracy: 0.8823\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.2885 - accuracy: 0.8957 - val_loss: 0.3389 - val_accuracy: 0.8796\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2863 - accuracy: 0.8956 - val_loss: 0.3250 - val_accuracy: 0.8844\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.2820 - accuracy: 0.8963 - val_loss: 0.3377 - val_accuracy: 0.8832\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2765 - accuracy: 0.8983 - val_loss: 0.3243 - val_accuracy: 0.8847\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.2731 - accuracy: 0.9000 - val_loss: 0.3162 - val_accuracy: 0.8885\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.2686 - accuracy: 0.9017 - val_loss: 0.3192 - val_accuracy: 0.8877\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.2643 - accuracy: 0.9029 - val_loss: 0.3231 - val_accuracy: 0.8861\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.2626 - accuracy: 0.9039 - val_loss: 0.3218 - val_accuracy: 0.8874\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3162 - accuracy: 0.8885\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.8358 - accuracy: 0.6847 - val_loss: 0.5415 - val_accuracy: 0.7919\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.5630 - accuracy: 0.7913 - val_loss: 0.4646 - val_accuracy: 0.8264\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4958 - accuracy: 0.8187 - val_loss: 0.4201 - val_accuracy: 0.8474\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.4535 - accuracy: 0.8345 - val_loss: 0.3880 - val_accuracy: 0.8589\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4259 - accuracy: 0.8443 - val_loss: 0.3718 - val_accuracy: 0.8618\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4061 - accuracy: 0.8509 - val_loss: 0.3565 - val_accuracy: 0.8644\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3891 - accuracy: 0.8571 - val_loss: 0.3444 - val_accuracy: 0.8710\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3776 - accuracy: 0.8618 - val_loss: 0.3363 - val_accuracy: 0.8759\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3678 - accuracy: 0.8649 - val_loss: 0.3266 - val_accuracy: 0.8809\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3557 - accuracy: 0.8689 - val_loss: 0.3180 - val_accuracy: 0.8819\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3454 - accuracy: 0.8739 - val_loss: 0.3198 - val_accuracy: 0.8838\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3414 - accuracy: 0.8736 - val_loss: 0.3183 - val_accuracy: 0.8810\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3313 - accuracy: 0.8775 - val_loss: 0.3138 - val_accuracy: 0.8864\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3255 - accuracy: 0.8801 - val_loss: 0.3071 - val_accuracy: 0.8883\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.3240 - accuracy: 0.8817 - val_loss: 0.3024 - val_accuracy: 0.8919\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3173 - accuracy: 0.8828 - val_loss: 0.2995 - val_accuracy: 0.8889\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3146 - accuracy: 0.8838 - val_loss: 0.3127 - val_accuracy: 0.8835\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3098 - accuracy: 0.8853 - val_loss: 0.3113 - val_accuracy: 0.8848\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3095 - accuracy: 0.8853 - val_loss: 0.3026 - val_accuracy: 0.8917\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2995 - accuracy: 0.8889\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.9920 - accuracy: 0.6336 - val_loss: 0.5683 - val_accuracy: 0.7765\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.5956 - accuracy: 0.7749 - val_loss: 0.4749 - val_accuracy: 0.8275\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.5199 - accuracy: 0.8096 - val_loss: 0.4348 - val_accuracy: 0.8386\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4773 - accuracy: 0.8265 - val_loss: 0.4014 - val_accuracy: 0.8530\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4476 - accuracy: 0.8390 - val_loss: 0.3760 - val_accuracy: 0.8620\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4221 - accuracy: 0.8464 - val_loss: 0.3606 - val_accuracy: 0.8663\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4071 - accuracy: 0.8522 - val_loss: 0.3549 - val_accuracy: 0.8672\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3901 - accuracy: 0.8585 - val_loss: 0.3416 - val_accuracy: 0.8755\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3785 - accuracy: 0.8621 - val_loss: 0.3358 - val_accuracy: 0.8777\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3729 - accuracy: 0.8630 - val_loss: 0.3325 - val_accuracy: 0.8795\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3616 - accuracy: 0.8674 - val_loss: 0.3253 - val_accuracy: 0.8819\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3562 - accuracy: 0.8695 - val_loss: 0.3183 - val_accuracy: 0.8837\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3517 - accuracy: 0.8719 - val_loss: 0.3139 - val_accuracy: 0.8862\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3416 - accuracy: 0.8750 - val_loss: 0.3097 - val_accuracy: 0.8884\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3360 - accuracy: 0.8773 - val_loss: 0.3099 - val_accuracy: 0.8884\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3331 - accuracy: 0.8763 - val_loss: 0.3088 - val_accuracy: 0.8900\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3289 - accuracy: 0.8785 - val_loss: 0.3078 - val_accuracy: 0.8915\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3211 - accuracy: 0.8831 - val_loss: 0.2979 - val_accuracy: 0.8919\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3206 - accuracy: 0.8810 - val_loss: 0.3045 - val_accuracy: 0.8880\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3143 - accuracy: 0.8837 - val_loss: 0.2946 - val_accuracy: 0.8939\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3118 - accuracy: 0.8854 - val_loss: 0.2927 - val_accuracy: 0.8950\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3088 - accuracy: 0.8864 - val_loss: 0.2922 - val_accuracy: 0.8943\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3050 - accuracy: 0.8866 - val_loss: 0.2897 - val_accuracy: 0.8938\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3048 - accuracy: 0.8880 - val_loss: 0.2907 - val_accuracy: 0.8971\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3013 - accuracy: 0.8887 - val_loss: 0.2977 - val_accuracy: 0.8917\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3006 - accuracy: 0.8884 - val_loss: 0.2852 - val_accuracy: 0.8987\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2973 - accuracy: 0.8899 - val_loss: 0.2871 - val_accuracy: 0.8946\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2963 - accuracy: 0.8906 - val_loss: 0.2897 - val_accuracy: 0.8971\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2914 - accuracy: 0.8915 - val_loss: 0.2807 - val_accuracy: 0.8991\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2905 - accuracy: 0.8929 - val_loss: 0.2815 - val_accuracy: 0.8990\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2881 - accuracy: 0.8939 - val_loss: 0.2762 - val_accuracy: 0.9005\n",
      "Epoch 32/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2836 - accuracy: 0.8950 - val_loss: 0.2744 - val_accuracy: 0.9024\n",
      "Epoch 33/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2824 - accuracy: 0.8958 - val_loss: 0.2778 - val_accuracy: 0.8994\n",
      "Epoch 34/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2806 - accuracy: 0.8959 - val_loss: 0.2758 - val_accuracy: 0.9000\n",
      "Epoch 35/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2793 - accuracy: 0.8961 - val_loss: 0.2747 - val_accuracy: 0.9012\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2744 - accuracy: 0.9024\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 1.2195 - accuracy: 0.5474 - val_loss: 0.6596 - val_accuracy: 0.7500\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.6890 - accuracy: 0.7387 - val_loss: 0.5625 - val_accuracy: 0.7876\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.6063 - accuracy: 0.7727 - val_loss: 0.5027 - val_accuracy: 0.8138\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 10s 85ms/step - loss: 0.5567 - accuracy: 0.7957 - val_loss: 0.4735 - val_accuracy: 0.8281\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.5188 - accuracy: 0.8102 - val_loss: 0.4422 - val_accuracy: 0.8384\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.4876 - accuracy: 0.8238 - val_loss: 0.4193 - val_accuracy: 0.8470\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.4625 - accuracy: 0.8339 - val_loss: 0.4007 - val_accuracy: 0.8551\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4442 - accuracy: 0.8383 - val_loss: 0.3915 - val_accuracy: 0.8567\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.4278 - accuracy: 0.8462 - val_loss: 0.3755 - val_accuracy: 0.8637\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.4159 - accuracy: 0.8488 - val_loss: 0.3683 - val_accuracy: 0.8660\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 10s 82ms/step - loss: 0.4080 - accuracy: 0.8521 - val_loss: 0.3664 - val_accuracy: 0.8697\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3986 - accuracy: 0.8554 - val_loss: 0.3574 - val_accuracy: 0.8729\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3892 - accuracy: 0.8585 - val_loss: 0.3482 - val_accuracy: 0.8745\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3821 - accuracy: 0.8616 - val_loss: 0.3509 - val_accuracy: 0.8756\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3727 - accuracy: 0.8626 - val_loss: 0.3381 - val_accuracy: 0.8773\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3686 - accuracy: 0.8666 - val_loss: 0.3318 - val_accuracy: 0.8807\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3632 - accuracy: 0.8661 - val_loss: 0.3364 - val_accuracy: 0.8787\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3586 - accuracy: 0.8699 - val_loss: 0.3294 - val_accuracy: 0.8793\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3530 - accuracy: 0.8708 - val_loss: 0.3288 - val_accuracy: 0.8820\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3487 - accuracy: 0.8721 - val_loss: 0.3196 - val_accuracy: 0.8845\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3433 - accuracy: 0.8743 - val_loss: 0.3210 - val_accuracy: 0.8852\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3407 - accuracy: 0.8758 - val_loss: 0.3187 - val_accuracy: 0.8827\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3349 - accuracy: 0.8764 - val_loss: 0.3195 - val_accuracy: 0.8842\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3327 - accuracy: 0.8777 - val_loss: 0.3199 - val_accuracy: 0.8844\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3307 - accuracy: 0.8784 - val_loss: 0.3098 - val_accuracy: 0.8890\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3286 - accuracy: 0.8794 - val_loss: 0.3091 - val_accuracy: 0.8874\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3240 - accuracy: 0.8792 - val_loss: 0.3126 - val_accuracy: 0.8855\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3203 - accuracy: 0.8824 - val_loss: 0.3049 - val_accuracy: 0.8876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3224 - accuracy: 0.8818 - val_loss: 0.3095 - val_accuracy: 0.8879\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3173 - accuracy: 0.8820 - val_loss: 0.2968 - val_accuracy: 0.8902\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3156 - accuracy: 0.8838 - val_loss: 0.3012 - val_accuracy: 0.8888\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3108 - accuracy: 0.8859 - val_loss: 0.3018 - val_accuracy: 0.8878\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3122 - accuracy: 0.8847 - val_loss: 0.2944 - val_accuracy: 0.8915\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3093 - accuracy: 0.8849 - val_loss: 0.2973 - val_accuracy: 0.8916\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3048 - accuracy: 0.8878 - val_loss: 0.2936 - val_accuracy: 0.8906\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3061 - accuracy: 0.8871 - val_loss: 0.2926 - val_accuracy: 0.8926\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3006 - accuracy: 0.8886 - val_loss: 0.2895 - val_accuracy: 0.8931\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.2990 - accuracy: 0.8886 - val_loss: 0.2877 - val_accuracy: 0.8937\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2983 - accuracy: 0.8883 - val_loss: 0.2896 - val_accuracy: 0.8931\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2978 - accuracy: 0.8897 - val_loss: 0.2878 - val_accuracy: 0.8935\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.2941 - accuracy: 0.8911 - val_loss: 0.2925 - val_accuracy: 0.8906\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2877 - accuracy: 0.8937\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.7765 - accuracy: 0.7244 - val_loss: 0.5686 - val_accuracy: 0.7877\n",
      "Epoch 2/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.5056 - accuracy: 0.8134 - val_loss: 0.4846 - val_accuracy: 0.8228\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4473 - accuracy: 0.8364 - val_loss: 0.4410 - val_accuracy: 0.8389\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4107 - accuracy: 0.8503 - val_loss: 0.4347 - val_accuracy: 0.8416\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3834 - accuracy: 0.8616 - val_loss: 0.3956 - val_accuracy: 0.8586\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3637 - accuracy: 0.8679 - val_loss: 0.3918 - val_accuracy: 0.8606\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3486 - accuracy: 0.8735 - val_loss: 0.3671 - val_accuracy: 0.8676\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3353 - accuracy: 0.8781 - val_loss: 0.3626 - val_accuracy: 0.8707\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3223 - accuracy: 0.8830 - val_loss: 0.3529 - val_accuracy: 0.8733\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3123 - accuracy: 0.8860 - val_loss: 0.3557 - val_accuracy: 0.8736\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3041 - accuracy: 0.8890 - val_loss: 0.3538 - val_accuracy: 0.8708\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2978 - accuracy: 0.8907 - val_loss: 0.3432 - val_accuracy: 0.8763\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2901 - accuracy: 0.8943 - val_loss: 0.3296 - val_accuracy: 0.8840\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2835 - accuracy: 0.8960 - val_loss: 0.3367 - val_accuracy: 0.8806\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2758 - accuracy: 0.8991 - val_loss: 0.3284 - val_accuracy: 0.8804\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.2720 - accuracy: 0.9009 - val_loss: 0.3222 - val_accuracy: 0.8838\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2649 - accuracy: 0.9043 - val_loss: 0.3175 - val_accuracy: 0.8881\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2609 - accuracy: 0.9048 - val_loss: 0.3197 - val_accuracy: 0.8855\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2527 - accuracy: 0.9072 - val_loss: 0.3100 - val_accuracy: 0.8898\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2495 - accuracy: 0.9082 - val_loss: 0.3090 - val_accuracy: 0.8904\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2446 - accuracy: 0.9105 - val_loss: 0.3145 - val_accuracy: 0.8858\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2417 - accuracy: 0.9116 - val_loss: 0.3110 - val_accuracy: 0.8904\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2367 - accuracy: 0.9132 - val_loss: 0.2999 - val_accuracy: 0.8917\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2317 - accuracy: 0.9158 - val_loss: 0.3174 - val_accuracy: 0.8878\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2291 - accuracy: 0.9161 - val_loss: 0.3101 - val_accuracy: 0.8882\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.2242 - accuracy: 0.9176 - val_loss: 0.3073 - val_accuracy: 0.8920\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2999 - accuracy: 0.8917\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.9640 - accuracy: 0.6713 - val_loss: 0.6337 - val_accuracy: 0.7684\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.5716 - accuracy: 0.7912 - val_loss: 0.5598 - val_accuracy: 0.7925\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.5060 - accuracy: 0.8175 - val_loss: 0.5374 - val_accuracy: 0.8020\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4706 - accuracy: 0.8321 - val_loss: 0.4810 - val_accuracy: 0.8253\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4423 - accuracy: 0.8426 - val_loss: 0.4625 - val_accuracy: 0.8349\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4213 - accuracy: 0.8505 - val_loss: 0.4279 - val_accuracy: 0.8508\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4013 - accuracy: 0.8573 - val_loss: 0.4128 - val_accuracy: 0.8555\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3878 - accuracy: 0.8623 - val_loss: 0.4044 - val_accuracy: 0.8575\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3748 - accuracy: 0.8666 - val_loss: 0.3947 - val_accuracy: 0.8609\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3625 - accuracy: 0.8713 - val_loss: 0.3794 - val_accuracy: 0.8677\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3485 - accuracy: 0.8752 - val_loss: 0.3636 - val_accuracy: 0.8705\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3422 - accuracy: 0.8776 - val_loss: 0.3568 - val_accuracy: 0.8753\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3334 - accuracy: 0.8813 - val_loss: 0.3594 - val_accuracy: 0.8725\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3248 - accuracy: 0.8835 - val_loss: 0.3499 - val_accuracy: 0.8763\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3177 - accuracy: 0.8870 - val_loss: 0.3454 - val_accuracy: 0.8771\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3131 - accuracy: 0.8879 - val_loss: 0.3437 - val_accuracy: 0.8792\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3086 - accuracy: 0.8893 - val_loss: 0.3386 - val_accuracy: 0.8808\n",
      "Epoch 18/100\n",
      "240/240 [==============================] - 10s 42ms/step - loss: 0.3020 - accuracy: 0.8904 - val_loss: 0.3304 - val_accuracy: 0.8854\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2974 - accuracy: 0.8936 - val_loss: 0.3364 - val_accuracy: 0.8788\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2919 - accuracy: 0.8956 - val_loss: 0.3273 - val_accuracy: 0.8825\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2881 - accuracy: 0.8962 - val_loss: 0.3297 - val_accuracy: 0.8819\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2838 - accuracy: 0.8972 - val_loss: 0.3206 - val_accuracy: 0.8885\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2773 - accuracy: 0.9000 - val_loss: 0.3284 - val_accuracy: 0.8841\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2747 - accuracy: 0.9016 - val_loss: 0.3263 - val_accuracy: 0.8852\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2731 - accuracy: 0.9007 - val_loss: 0.3177 - val_accuracy: 0.8876\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2695 - accuracy: 0.9023 - val_loss: 0.3189 - val_accuracy: 0.8858\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2647 - accuracy: 0.9048 - val_loss: 0.3105 - val_accuracy: 0.8917\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2625 - accuracy: 0.9042 - val_loss: 0.3112 - val_accuracy: 0.8908\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2581 - accuracy: 0.9062 - val_loss: 0.3115 - val_accuracy: 0.8889\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2580 - accuracy: 0.9053 - val_loss: 0.3110 - val_accuracy: 0.8924\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3105 - accuracy: 0.8917\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 1.2066 - accuracy: 0.6279 - val_loss: 0.7185 - val_accuracy: 0.7288\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.6308 - accuracy: 0.7608 - val_loss: 0.5929 - val_accuracy: 0.7771\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.5530 - accuracy: 0.7921 - val_loss: 0.5774 - val_accuracy: 0.7823\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.5076 - accuracy: 0.8133 - val_loss: 0.5176 - val_accuracy: 0.8073\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4743 - accuracy: 0.8281 - val_loss: 0.4799 - val_accuracy: 0.8280\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4549 - accuracy: 0.8348 - val_loss: 0.4606 - val_accuracy: 0.8357\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4336 - accuracy: 0.8452 - val_loss: 0.4668 - val_accuracy: 0.8240\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4216 - accuracy: 0.8494 - val_loss: 0.4335 - val_accuracy: 0.8491\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4039 - accuracy: 0.8572 - val_loss: 0.4162 - val_accuracy: 0.8540\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3927 - accuracy: 0.8615 - val_loss: 0.4053 - val_accuracy: 0.8560\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3848 - accuracy: 0.8629 - val_loss: 0.3980 - val_accuracy: 0.8599\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3765 - accuracy: 0.8657 - val_loss: 0.3954 - val_accuracy: 0.8606\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3682 - accuracy: 0.8678 - val_loss: 0.3910 - val_accuracy: 0.8611\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3613 - accuracy: 0.8710 - val_loss: 0.3771 - val_accuracy: 0.8658\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3546 - accuracy: 0.8729 - val_loss: 0.3868 - val_accuracy: 0.8599\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3477 - accuracy: 0.8748 - val_loss: 0.3715 - val_accuracy: 0.8649\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3432 - accuracy: 0.8771 - val_loss: 0.3747 - val_accuracy: 0.8666\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3371 - accuracy: 0.8781 - val_loss: 0.3684 - val_accuracy: 0.8712\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3335 - accuracy: 0.8805 - val_loss: 0.3650 - val_accuracy: 0.8708\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3287 - accuracy: 0.8809 - val_loss: 0.3738 - val_accuracy: 0.8662\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3262 - accuracy: 0.8815 - val_loss: 0.3561 - val_accuracy: 0.8712\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3219 - accuracy: 0.8836 - val_loss: 0.3548 - val_accuracy: 0.8731\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3166 - accuracy: 0.8848 - val_loss: 0.3437 - val_accuracy: 0.8753\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3146 - accuracy: 0.8868 - val_loss: 0.3537 - val_accuracy: 0.8719\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3102 - accuracy: 0.8864 - val_loss: 0.3436 - val_accuracy: 0.8760\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3082 - accuracy: 0.8887 - val_loss: 0.3493 - val_accuracy: 0.8740\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3022 - accuracy: 0.8899 - val_loss: 0.3373 - val_accuracy: 0.8788\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2984 - accuracy: 0.8919 - val_loss: 0.3407 - val_accuracy: 0.8802\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2975 - accuracy: 0.8914 - val_loss: 0.3414 - val_accuracy: 0.8774\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2937 - accuracy: 0.8929 - val_loss: 0.3345 - val_accuracy: 0.8809\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2896 - accuracy: 0.8948 - val_loss: 0.3302 - val_accuracy: 0.8847\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.2896 - accuracy: 0.8946 - val_loss: 0.3420 - val_accuracy: 0.8779\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2878 - accuracy: 0.8958 - val_loss: 0.3296 - val_accuracy: 0.8809\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2835 - accuracy: 0.8970 - val_loss: 0.3333 - val_accuracy: 0.8794\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2802 - accuracy: 0.8974 - val_loss: 0.3258 - val_accuracy: 0.8841\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.2803 - accuracy: 0.8987 - val_loss: 0.3502 - val_accuracy: 0.8716\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2769 - accuracy: 0.8992 - val_loss: 0.3239 - val_accuracy: 0.8856\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.2722 - accuracy: 0.9010 - val_loss: 0.3258 - val_accuracy: 0.8840\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2733 - accuracy: 0.9004 - val_loss: 0.3247 - val_accuracy: 0.8850\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2700 - accuracy: 0.9010 - val_loss: 0.3319 - val_accuracy: 0.8793\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3239 - accuracy: 0.8856\n",
      "Epoch 1/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.9297 - accuracy: 0.6570 - val_loss: 0.5692 - val_accuracy: 0.7810\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 11s 19ms/step - loss: 0.5995 - accuracy: 0.7737 - val_loss: 0.4946 - val_accuracy: 0.8134\n",
      "Epoch 3/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.5313 - accuracy: 0.8032 - val_loss: 0.4459 - val_accuracy: 0.8384\n",
      "Epoch 4/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4892 - accuracy: 0.8198 - val_loss: 0.4135 - val_accuracy: 0.8484\n",
      "Epoch 5/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4555 - accuracy: 0.8334 - val_loss: 0.3899 - val_accuracy: 0.8581\n",
      "Epoch 6/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4333 - accuracy: 0.8408 - val_loss: 0.3693 - val_accuracy: 0.8615\n",
      "Epoch 7/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4162 - accuracy: 0.8487 - val_loss: 0.3652 - val_accuracy: 0.8646\n",
      "Epoch 8/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.4008 - accuracy: 0.8534 - val_loss: 0.3552 - val_accuracy: 0.8724\n",
      "Epoch 9/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3894 - accuracy: 0.8562 - val_loss: 0.3433 - val_accuracy: 0.8741\n",
      "Epoch 10/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3781 - accuracy: 0.8595 - val_loss: 0.3386 - val_accuracy: 0.8742\n",
      "Epoch 11/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3667 - accuracy: 0.8649 - val_loss: 0.3294 - val_accuracy: 0.8780\n",
      "Epoch 12/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3608 - accuracy: 0.8671 - val_loss: 0.3278 - val_accuracy: 0.8821\n",
      "Epoch 13/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3528 - accuracy: 0.8700 - val_loss: 0.3251 - val_accuracy: 0.8814\n",
      "Epoch 14/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3451 - accuracy: 0.8727 - val_loss: 0.3220 - val_accuracy: 0.8814\n",
      "Epoch 15/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.3401 - accuracy: 0.8742 - val_loss: 0.3093 - val_accuracy: 0.8882\n",
      "Epoch 16/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.3352 - accuracy: 0.8759 - val_loss: 0.3083 - val_accuracy: 0.8874\n",
      "Epoch 17/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3306 - accuracy: 0.8774 - val_loss: 0.3077 - val_accuracy: 0.8888\n",
      "Epoch 18/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3271 - accuracy: 0.8793 - val_loss: 0.3046 - val_accuracy: 0.8884\n",
      "Epoch 19/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3245 - accuracy: 0.8806 - val_loss: 0.3016 - val_accuracy: 0.8910\n",
      "Epoch 20/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3212 - accuracy: 0.8808 - val_loss: 0.2987 - val_accuracy: 0.8918\n",
      "Epoch 21/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3163 - accuracy: 0.8830 - val_loss: 0.2960 - val_accuracy: 0.8937\n",
      "Epoch 22/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3150 - accuracy: 0.8840 - val_loss: 0.3000 - val_accuracy: 0.8920\n",
      "Epoch 23/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3105 - accuracy: 0.8859 - val_loss: 0.2898 - val_accuracy: 0.8958\n",
      "Epoch 24/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3089 - accuracy: 0.8855 - val_loss: 0.2918 - val_accuracy: 0.8941\n",
      "Epoch 25/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3063 - accuracy: 0.8866 - val_loss: 0.2886 - val_accuracy: 0.8960\n",
      "Epoch 26/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.3029 - accuracy: 0.8888 - val_loss: 0.2846 - val_accuracy: 0.8984\n",
      "Epoch 27/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2999 - accuracy: 0.8896 - val_loss: 0.2908 - val_accuracy: 0.8953\n",
      "Epoch 28/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2960 - accuracy: 0.8912 - val_loss: 0.2828 - val_accuracy: 0.8969\n",
      "Epoch 29/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.2930 - accuracy: 0.8921 - val_loss: 0.2829 - val_accuracy: 0.8978\n",
      "Epoch 30/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2976 - accuracy: 0.8905 - val_loss: 0.2840 - val_accuracy: 0.8965\n",
      "Epoch 31/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2906 - accuracy: 0.8924 - val_loss: 0.2821 - val_accuracy: 0.8977\n",
      "Epoch 32/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2889 - accuracy: 0.8931 - val_loss: 0.2845 - val_accuracy: 0.8957\n",
      "Epoch 33/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2863 - accuracy: 0.8941 - val_loss: 0.2813 - val_accuracy: 0.8981\n",
      "Epoch 34/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2860 - accuracy: 0.8945 - val_loss: 0.2761 - val_accuracy: 0.9003\n",
      "Epoch 35/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.2848 - accuracy: 0.8956 - val_loss: 0.2836 - val_accuracy: 0.8962\n",
      "Epoch 36/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2813 - accuracy: 0.8953 - val_loss: 0.2799 - val_accuracy: 0.8992\n",
      "Epoch 37/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2820 - accuracy: 0.8961 - val_loss: 0.2743 - val_accuracy: 0.9002\n",
      "Epoch 38/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2777 - accuracy: 0.8967 - val_loss: 0.2752 - val_accuracy: 0.9010\n",
      "Epoch 39/100\n",
      "600/600 [==============================] - 11s 19ms/step - loss: 0.2778 - accuracy: 0.8971 - val_loss: 0.2907 - val_accuracy: 0.8921\n",
      "Epoch 40/100\n",
      "600/600 [==============================] - 12s 19ms/step - loss: 0.2761 - accuracy: 0.8983 - val_loss: 0.2744 - val_accuracy: 0.9008\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2743 - accuracy: 0.9002\n",
      "Epoch 1/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 1.1811 - accuracy: 0.5705 - val_loss: 0.6315 - val_accuracy: 0.7596\n",
      "Epoch 2/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.6625 - accuracy: 0.7523 - val_loss: 0.5397 - val_accuracy: 0.7936\n",
      "Epoch 3/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.5828 - accuracy: 0.7825 - val_loss: 0.4890 - val_accuracy: 0.8178\n",
      "Epoch 4/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.5342 - accuracy: 0.8033 - val_loss: 0.4606 - val_accuracy: 0.8260\n",
      "Epoch 5/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4999 - accuracy: 0.8175 - val_loss: 0.4317 - val_accuracy: 0.8382\n",
      "Epoch 6/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4757 - accuracy: 0.8271 - val_loss: 0.4104 - val_accuracy: 0.8454\n",
      "Epoch 7/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4532 - accuracy: 0.8349 - val_loss: 0.3988 - val_accuracy: 0.8515\n",
      "Epoch 8/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.4371 - accuracy: 0.8416 - val_loss: 0.3812 - val_accuracy: 0.8605\n",
      "Epoch 9/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4240 - accuracy: 0.8466 - val_loss: 0.3723 - val_accuracy: 0.8625\n",
      "Epoch 10/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.4106 - accuracy: 0.8512 - val_loss: 0.3648 - val_accuracy: 0.8641\n",
      "Epoch 11/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3998 - accuracy: 0.8565 - val_loss: 0.3595 - val_accuracy: 0.8669\n",
      "Epoch 12/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3929 - accuracy: 0.8563 - val_loss: 0.3481 - val_accuracy: 0.8710\n",
      "Epoch 13/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3819 - accuracy: 0.8605 - val_loss: 0.3435 - val_accuracy: 0.8733\n",
      "Epoch 14/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3788 - accuracy: 0.8611 - val_loss: 0.3397 - val_accuracy: 0.8754\n",
      "Epoch 15/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3719 - accuracy: 0.8647 - val_loss: 0.3313 - val_accuracy: 0.8764\n",
      "Epoch 16/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3625 - accuracy: 0.8672 - val_loss: 0.3280 - val_accuracy: 0.8768\n",
      "Epoch 17/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3595 - accuracy: 0.8689 - val_loss: 0.3253 - val_accuracy: 0.8807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3512 - accuracy: 0.8707 - val_loss: 0.3227 - val_accuracy: 0.8823\n",
      "Epoch 19/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3480 - accuracy: 0.8718 - val_loss: 0.3183 - val_accuracy: 0.8816\n",
      "Epoch 20/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3429 - accuracy: 0.8736 - val_loss: 0.3154 - val_accuracy: 0.8829\n",
      "Epoch 21/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3383 - accuracy: 0.8767 - val_loss: 0.3141 - val_accuracy: 0.8842\n",
      "Epoch 22/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3347 - accuracy: 0.8749 - val_loss: 0.3116 - val_accuracy: 0.8834\n",
      "Epoch 23/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3328 - accuracy: 0.8787 - val_loss: 0.3089 - val_accuracy: 0.8857\n",
      "Epoch 24/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3308 - accuracy: 0.8777 - val_loss: 0.3056 - val_accuracy: 0.8899\n",
      "Epoch 25/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3272 - accuracy: 0.8805 - val_loss: 0.3012 - val_accuracy: 0.8877\n",
      "Epoch 26/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3236 - accuracy: 0.8813 - val_loss: 0.3009 - val_accuracy: 0.8883\n",
      "Epoch 27/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3171 - accuracy: 0.8830 - val_loss: 0.2991 - val_accuracy: 0.8891\n",
      "Epoch 28/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3140 - accuracy: 0.8855 - val_loss: 0.2996 - val_accuracy: 0.8907\n",
      "Epoch 29/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3157 - accuracy: 0.8843 - val_loss: 0.2973 - val_accuracy: 0.8905\n",
      "Epoch 30/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3128 - accuracy: 0.8839 - val_loss: 0.2947 - val_accuracy: 0.8920\n",
      "Epoch 31/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3118 - accuracy: 0.8863 - val_loss: 0.2932 - val_accuracy: 0.8908\n",
      "Epoch 32/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3073 - accuracy: 0.8864 - val_loss: 0.2951 - val_accuracy: 0.8927\n",
      "Epoch 33/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3088 - accuracy: 0.8860 - val_loss: 0.2913 - val_accuracy: 0.8926\n",
      "Epoch 34/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.3042 - accuracy: 0.8878 - val_loss: 0.2883 - val_accuracy: 0.8943\n",
      "Epoch 35/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3038 - accuracy: 0.8871 - val_loss: 0.2859 - val_accuracy: 0.8938\n",
      "Epoch 36/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.3027 - accuracy: 0.8874 - val_loss: 0.2884 - val_accuracy: 0.8932\n",
      "Epoch 37/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2956 - accuracy: 0.8914 - val_loss: 0.2852 - val_accuracy: 0.8955\n",
      "Epoch 38/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2955 - accuracy: 0.8902 - val_loss: 0.2838 - val_accuracy: 0.8934\n",
      "Epoch 39/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2918 - accuracy: 0.8925 - val_loss: 0.2837 - val_accuracy: 0.8970\n",
      "Epoch 40/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2911 - accuracy: 0.8918 - val_loss: 0.2870 - val_accuracy: 0.8926\n",
      "Epoch 41/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2922 - accuracy: 0.8929 - val_loss: 0.2813 - val_accuracy: 0.8965\n",
      "Epoch 42/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2867 - accuracy: 0.8946 - val_loss: 0.2816 - val_accuracy: 0.8957\n",
      "Epoch 43/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2904 - accuracy: 0.8919 - val_loss: 0.2799 - val_accuracy: 0.8975\n",
      "Epoch 44/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2893 - accuracy: 0.8927 - val_loss: 0.2800 - val_accuracy: 0.8962\n",
      "Epoch 45/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2870 - accuracy: 0.8942 - val_loss: 0.2822 - val_accuracy: 0.8975\n",
      "Epoch 46/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2854 - accuracy: 0.8950 - val_loss: 0.2776 - val_accuracy: 0.8973\n",
      "Epoch 47/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2820 - accuracy: 0.8953 - val_loss: 0.2759 - val_accuracy: 0.8987\n",
      "Epoch 48/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2827 - accuracy: 0.8954 - val_loss: 0.2771 - val_accuracy: 0.8967\n",
      "Epoch 49/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2775 - accuracy: 0.8974 - val_loss: 0.2771 - val_accuracy: 0.8998\n",
      "Epoch 50/100\n",
      "240/240 [==============================] - 10s 41ms/step - loss: 0.2793 - accuracy: 0.8966 - val_loss: 0.2748 - val_accuracy: 0.8995\n",
      "Epoch 51/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2768 - accuracy: 0.8968 - val_loss: 0.2780 - val_accuracy: 0.8977\n",
      "Epoch 52/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2776 - accuracy: 0.8954 - val_loss: 0.2758 - val_accuracy: 0.8986\n",
      "Epoch 53/100\n",
      "240/240 [==============================] - 10s 40ms/step - loss: 0.2745 - accuracy: 0.8981 - val_loss: 0.2770 - val_accuracy: 0.8979\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2748 - accuracy: 0.8995\n",
      "Epoch 1/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 1.4685 - accuracy: 0.4763 - val_loss: 0.7708 - val_accuracy: 0.7299\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.8078 - accuracy: 0.7019 - val_loss: 0.6283 - val_accuracy: 0.7646\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.6902 - accuracy: 0.7419 - val_loss: 0.5682 - val_accuracy: 0.7833\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.6308 - accuracy: 0.7657 - val_loss: 0.5334 - val_accuracy: 0.7993\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.5899 - accuracy: 0.7808 - val_loss: 0.5008 - val_accuracy: 0.8171\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.5577 - accuracy: 0.7961 - val_loss: 0.4792 - val_accuracy: 0.8257\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.5295 - accuracy: 0.8062 - val_loss: 0.4593 - val_accuracy: 0.8327\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.5072 - accuracy: 0.8159 - val_loss: 0.4438 - val_accuracy: 0.8391\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 9s 78ms/step - loss: 0.4905 - accuracy: 0.8223 - val_loss: 0.4333 - val_accuracy: 0.8439\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4772 - accuracy: 0.8269 - val_loss: 0.4171 - val_accuracy: 0.8513\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4631 - accuracy: 0.8309 - val_loss: 0.4055 - val_accuracy: 0.8535\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4546 - accuracy: 0.8340 - val_loss: 0.3982 - val_accuracy: 0.8557\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4423 - accuracy: 0.8400 - val_loss: 0.3894 - val_accuracy: 0.8575\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.4350 - accuracy: 0.8407 - val_loss: 0.3844 - val_accuracy: 0.8607\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4264 - accuracy: 0.8446 - val_loss: 0.3769 - val_accuracy: 0.8632\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4170 - accuracy: 0.8490 - val_loss: 0.3690 - val_accuracy: 0.8641\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4091 - accuracy: 0.8527 - val_loss: 0.3651 - val_accuracy: 0.8677\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.4040 - accuracy: 0.8526 - val_loss: 0.3635 - val_accuracy: 0.8676\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.4000 - accuracy: 0.8553 - val_loss: 0.3573 - val_accuracy: 0.8696\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3947 - accuracy: 0.8573 - val_loss: 0.3528 - val_accuracy: 0.8702\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3888 - accuracy: 0.8595 - val_loss: 0.3496 - val_accuracy: 0.8718\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3865 - accuracy: 0.8592 - val_loss: 0.3469 - val_accuracy: 0.8725\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3780 - accuracy: 0.8625 - val_loss: 0.3444 - val_accuracy: 0.8755\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3738 - accuracy: 0.8643 - val_loss: 0.3410 - val_accuracy: 0.8741\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3745 - accuracy: 0.8642 - val_loss: 0.3383 - val_accuracy: 0.8720\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3662 - accuracy: 0.8678 - val_loss: 0.3353 - val_accuracy: 0.8761\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3627 - accuracy: 0.8676 - val_loss: 0.3343 - val_accuracy: 0.8767\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3593 - accuracy: 0.8696 - val_loss: 0.3347 - val_accuracy: 0.8747\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3566 - accuracy: 0.8696 - val_loss: 0.3288 - val_accuracy: 0.8779\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3515 - accuracy: 0.8719 - val_loss: 0.3254 - val_accuracy: 0.8788\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3523 - accuracy: 0.8720 - val_loss: 0.3254 - val_accuracy: 0.8785\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3456 - accuracy: 0.8733 - val_loss: 0.3209 - val_accuracy: 0.8802\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3443 - accuracy: 0.8759 - val_loss: 0.3210 - val_accuracy: 0.8791\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3446 - accuracy: 0.8740 - val_loss: 0.3204 - val_accuracy: 0.8803\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3422 - accuracy: 0.8754 - val_loss: 0.3147 - val_accuracy: 0.8814\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 10s 81ms/step - loss: 0.3395 - accuracy: 0.8760 - val_loss: 0.3157 - val_accuracy: 0.8816\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3334 - accuracy: 0.8776 - val_loss: 0.3194 - val_accuracy: 0.8808\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3333 - accuracy: 0.8778 - val_loss: 0.3095 - val_accuracy: 0.8835\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3321 - accuracy: 0.8780 - val_loss: 0.3086 - val_accuracy: 0.8863\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3298 - accuracy: 0.8784 - val_loss: 0.3114 - val_accuracy: 0.8823\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3287 - accuracy: 0.8804 - val_loss: 0.3070 - val_accuracy: 0.8856\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3238 - accuracy: 0.8816 - val_loss: 0.3071 - val_accuracy: 0.8865\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3229 - accuracy: 0.8825 - val_loss: 0.3046 - val_accuracy: 0.8851\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3200 - accuracy: 0.8832 - val_loss: 0.3050 - val_accuracy: 0.8839\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3184 - accuracy: 0.8817 - val_loss: 0.3016 - val_accuracy: 0.8883\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3206 - accuracy: 0.8820 - val_loss: 0.3002 - val_accuracy: 0.8891\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3143 - accuracy: 0.8843 - val_loss: 0.2983 - val_accuracy: 0.8897\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3144 - accuracy: 0.8836 - val_loss: 0.2996 - val_accuracy: 0.8897\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3127 - accuracy: 0.8855 - val_loss: 0.3011 - val_accuracy: 0.8866\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3132 - accuracy: 0.8858 - val_loss: 0.2977 - val_accuracy: 0.8880\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 10s 82ms/step - loss: 0.3099 - accuracy: 0.8874 - val_loss: 0.2972 - val_accuracy: 0.8879\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3099 - accuracy: 0.8859 - val_loss: 0.2975 - val_accuracy: 0.8894\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3070 - accuracy: 0.8883 - val_loss: 0.2930 - val_accuracy: 0.8919\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3065 - accuracy: 0.8887 - val_loss: 0.2929 - val_accuracy: 0.8914\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 9s 79ms/step - loss: 0.3034 - accuracy: 0.8885 - val_loss: 0.2927 - val_accuracy: 0.8915\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3044 - accuracy: 0.8896 - val_loss: 0.2944 - val_accuracy: 0.8928\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.3006 - accuracy: 0.8904 - val_loss: 0.2902 - val_accuracy: 0.8926\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.3037 - accuracy: 0.8890 - val_loss: 0.2898 - val_accuracy: 0.8930\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2996 - accuracy: 0.8893 - val_loss: 0.2907 - val_accuracy: 0.8925\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 10s 79ms/step - loss: 0.2973 - accuracy: 0.8911 - val_loss: 0.2943 - val_accuracy: 0.8881\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 10s 80ms/step - loss: 0.2966 - accuracy: 0.8903 - val_loss: 0.2922 - val_accuracy: 0.8904\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2898 - accuracy: 0.8930\n"
     ]
    }
   ],
   "source": [
    "all_models = []\n",
    "for params in full_params:\n",
    "    model = build_and_fit_model(**params)\n",
    "    all_models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.sequential.Sequential at 0x217878c2e10>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x21787e7dfd0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217ac899d30>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b3f49668>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217ac8ceb00>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b020fc50>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b016e7b8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b022a5f8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b24e1a90>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b1017f98>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b0f3a828>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b1bb4c18>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b10d8828>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b6be1b38>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b73e5978>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b24e1630>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b6c3aeb8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b553ad68>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b586bbe0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b5823748>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b5711908>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b5bc5f98>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b5d4e5c0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b5e2d8d0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b551c828>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b526d1d0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217be1c6160>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217c31c3160>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217c394b710>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217c52e1208>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217c538c9b0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217c9e1c8d0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217ca070b00>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217b0061a58>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217ba3f58d0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217ac82c400>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217c9f81c18>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cbd37ba8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cbd5bbe0>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cc37fcf8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cc3f8748>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cc59bb70>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217d0962828>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217d0bcf048>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217d0cffeb8>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217d0f4eb00>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cc3f1898>,\n",
       " <tensorflow.python.keras.engine.sequential.Sequential at 0x217cc3cdf60>]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1687 - accuracy: 0.9378\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2667 - accuracy: 0.9019\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1594 - accuracy: 0.9431\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2670 - accuracy: 0.9047\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1418 - accuracy: 0.9484\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2570 - accuracy: 0.9109\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1506 - accuracy: 0.9444\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2529 - accuracy: 0.9095\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1322 - accuracy: 0.9526\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2479 - accuracy: 0.9127\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1858 - accuracy: 0.9309\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2569 - accuracy: 0.9057\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1466 - accuracy: 0.9472\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2484 - accuracy: 0.9115\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1709 - accuracy: 0.9378\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2614 - accuracy: 0.9070\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2221 - accuracy: 0.9217\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2867 - accuracy: 0.8971\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1266 - accuracy: 0.9560\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2458 - accuracy: 0.9115\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1421 - accuracy: 0.9477\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2430 - accuracy: 0.9117\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.1500 - accuracy: 0.9464\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2451 - accuracy: 0.9093\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2359 - accuracy: 0.9117\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2962 - accuracy: 0.8929\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2207 - accuracy: 0.9183\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2996 - accuracy: 0.8897\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2724 - accuracy: 0.9009\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3194 - accuracy: 0.8871\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2119 - accuracy: 0.9208\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2746 - accuracy: 0.8983\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1931 - accuracy: 0.9286\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2738 - accuracy: 0.9015\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2157 - accuracy: 0.9204\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2724 - accuracy: 0.8993\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2364 - accuracy: 0.9131\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3003 - accuracy: 0.8944\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2945 - accuracy: 0.8953\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3365 - accuracy: 0.8777\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2676 - accuracy: 0.9042\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.3268 - accuracy: 0.8819\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1967 - accuracy: 0.9282\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2595 - accuracy: 0.9055\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2064 - accuracy: 0.9245\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.2693 - accuracy: 0.9034\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2327 - accuracy: 0.9153\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2840 - accuracy: 0.8946\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1859 - accuracy: 0.9333\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2704 - accuracy: 0.9022\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1780 - accuracy: 0.9342\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2831 - accuracy: 0.8967\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.2111 - accuracy: 0.9234\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2837 - accuracy: 0.8959\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1944 - accuracy: 0.9270\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2757 - accuracy: 0.9016\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1609 - accuracy: 0.9413\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2657 - accuracy: 0.9080\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1566 - accuracy: 0.9422\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2569 - accuracy: 0.9093\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1547 - accuracy: 0.9444\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2722 - accuracy: 0.9042\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1818 - accuracy: 0.9347\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2796 - accuracy: 0.9016\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.2049 - accuracy: 0.9260\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2858 - accuracy: 0.8978\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1445 - accuracy: 0.9480\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2514 - accuracy: 0.9112\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1800 - accuracy: 0.9335\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2555 - accuracy: 0.9072\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.1524 - accuracy: 0.9449\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 0.2475 - accuracy: 0.9113\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2261 - accuracy: 0.9175\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3020 - accuracy: 0.8912\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2133 - accuracy: 0.9208\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3014 - accuracy: 0.8938\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2589 - accuracy: 0.9059\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3162 - accuracy: 0.8885\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2478 - accuracy: 0.9069\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2995 - accuracy: 0.8889\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2169 - accuracy: 0.9200\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2744 - accuracy: 0.9024\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2335 - accuracy: 0.9122\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2877 - accuracy: 0.8937\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2201 - accuracy: 0.9203\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2999 - accuracy: 0.8917\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2546 - accuracy: 0.9083\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3105 - accuracy: 0.8917\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2693 - accuracy: 0.9015\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.3239 - accuracy: 0.8856\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2132 - accuracy: 0.9208\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2743 - accuracy: 0.9002\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2129 - accuracy: 0.9204\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2748 - accuracy: 0.8995\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.2368 - accuracy: 0.9128\n",
      "313/313 [==============================] - 1s 4ms/step - loss: 0.2898 - accuracy: 0.8930\n"
     ]
    }
   ],
   "source": [
    "all_model_results = []\n",
    "for cur_model in all_models:\n",
    "    cur_model_results = {}\n",
    "    cur_model_results['model'] = cur_model\n",
    "    cur_model_results['train_eval'] = cur_model.evaluate(train_images, train_labels)\n",
    "    cur_model_results['test_eval'] = cur_model.evaluate(test_images, test_labels)\n",
    "    all_model_results.append(cur_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "all_df = pd.DataFrame(all_model_results)\n",
    "all_df[['train_loss','train_acc']] = pd.DataFrame(all_df.train_eval.tolist(), index= all_df.index)\n",
    "all_df[['test_loss','test_acc']] = pd.DataFrame(all_df.test_eval.tolist(), index= all_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_eval</th>\n",
       "      <th>test_eval</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.16868141293525696, 0.9378499984741211]</td>\n",
       "      <td>[0.26669758558273315, 0.9018999934196472]</td>\n",
       "      <td>0.168681</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.266698</td>\n",
       "      <td>0.9019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15938624739646912, 0.9430500268936157]</td>\n",
       "      <td>[0.2670109272003174, 0.904699981212616]</td>\n",
       "      <td>0.159386</td>\n",
       "      <td>0.943050</td>\n",
       "      <td>0.267011</td>\n",
       "      <td>0.9047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1417781561613083, 0.948366641998291]</td>\n",
       "      <td>[0.25698399543762207, 0.9108999967575073]</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>0.948367</td>\n",
       "      <td>0.256984</td>\n",
       "      <td>0.9109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15062175691127777, 0.9443666934967041]</td>\n",
       "      <td>[0.252878874540329, 0.909500002861023]</td>\n",
       "      <td>0.150622</td>\n",
       "      <td>0.944367</td>\n",
       "      <td>0.252879</td>\n",
       "      <td>0.9095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.13222971558570862, 0.9526333212852478]</td>\n",
       "      <td>[0.24787791073322296, 0.9126999974250793]</td>\n",
       "      <td>0.132230</td>\n",
       "      <td>0.952633</td>\n",
       "      <td>0.247878</td>\n",
       "      <td>0.9127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1857813447713852, 0.930899977684021]</td>\n",
       "      <td>[0.25688549876213074, 0.9057000279426575]</td>\n",
       "      <td>0.185781</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>0.256885</td>\n",
       "      <td>0.9057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1465650051832199, 0.9471666812896729]</td>\n",
       "      <td>[0.24840134382247925, 0.9114999771118164]</td>\n",
       "      <td>0.146565</td>\n",
       "      <td>0.947167</td>\n",
       "      <td>0.248401</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1708768606185913, 0.9378499984741211]</td>\n",
       "      <td>[0.2613731920719147, 0.9070000052452087]</td>\n",
       "      <td>0.170877</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.261373</td>\n",
       "      <td>0.9070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22205980122089386, 0.92166668176651]</td>\n",
       "      <td>[0.28667232394218445, 0.8970999717712402]</td>\n",
       "      <td>0.222060</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>0.286672</td>\n",
       "      <td>0.8971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.12658622860908508, 0.9560166597366333]</td>\n",
       "      <td>[0.24576333165168762, 0.9114999771118164]</td>\n",
       "      <td>0.126586</td>\n",
       "      <td>0.956017</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1421215534210205, 0.9477333426475525]</td>\n",
       "      <td>[0.24299895763397217, 0.9117000102996826]</td>\n",
       "      <td>0.142122</td>\n",
       "      <td>0.947733</td>\n",
       "      <td>0.242999</td>\n",
       "      <td>0.9117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14996056258678436, 0.9464333057403564]</td>\n",
       "      <td>[0.24511033296585083, 0.9093000292778015]</td>\n",
       "      <td>0.149961</td>\n",
       "      <td>0.946433</td>\n",
       "      <td>0.245110</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23593302071094513, 0.9116500020027161]</td>\n",
       "      <td>[0.29616788029670715, 0.8928999900817871]</td>\n",
       "      <td>0.235933</td>\n",
       "      <td>0.911650</td>\n",
       "      <td>0.296168</td>\n",
       "      <td>0.8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22073142230510712, 0.9183333516120911]</td>\n",
       "      <td>[0.2996402978897095, 0.8896999955177307]</td>\n",
       "      <td>0.220731</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>0.299640</td>\n",
       "      <td>0.8897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.27241677045822144, 0.9009333252906799]</td>\n",
       "      <td>[0.31941139698028564, 0.8870999813079834]</td>\n",
       "      <td>0.272417</td>\n",
       "      <td>0.900933</td>\n",
       "      <td>0.319411</td>\n",
       "      <td>0.8871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21186839044094086, 0.9207500219345093]</td>\n",
       "      <td>[0.2745698392391205, 0.8982999920845032]</td>\n",
       "      <td>0.211868</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274570</td>\n",
       "      <td>0.8983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19306862354278564, 0.9285666942596436]</td>\n",
       "      <td>[0.2737598717212677, 0.9014999866485596]</td>\n",
       "      <td>0.193069</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.273760</td>\n",
       "      <td>0.9015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21572570502758026, 0.9204333424568176]</td>\n",
       "      <td>[0.2723506689071655, 0.8992999792098999]</td>\n",
       "      <td>0.215726</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.272351</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23643141984939575, 0.9130666851997375]</td>\n",
       "      <td>[0.30026862025260925, 0.8944000005722046]</td>\n",
       "      <td>0.236431</td>\n",
       "      <td>0.913067</td>\n",
       "      <td>0.300269</td>\n",
       "      <td>0.8944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.29450979828834534, 0.8953333497047424]</td>\n",
       "      <td>[0.3364871144294739, 0.8776999711990356]</td>\n",
       "      <td>0.294510</td>\n",
       "      <td>0.895333</td>\n",
       "      <td>0.336487</td>\n",
       "      <td>0.8777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2675584852695465, 0.9041666388511658]</td>\n",
       "      <td>[0.3267644941806793, 0.8819000124931335]</td>\n",
       "      <td>0.267558</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>0.326764</td>\n",
       "      <td>0.8819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19672514498233795, 0.9282000064849854]</td>\n",
       "      <td>[0.25953343510627747, 0.9054999947547913]</td>\n",
       "      <td>0.196725</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.259533</td>\n",
       "      <td>0.9055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20643432438373566, 0.9244666695594788]</td>\n",
       "      <td>[0.2693329453468323, 0.9034000039100647]</td>\n",
       "      <td>0.206434</td>\n",
       "      <td>0.924467</td>\n",
       "      <td>0.269333</td>\n",
       "      <td>0.9034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23273038864135742, 0.9152833223342896]</td>\n",
       "      <td>[0.2840319871902466, 0.894599974155426]</td>\n",
       "      <td>0.232730</td>\n",
       "      <td>0.915283</td>\n",
       "      <td>0.284032</td>\n",
       "      <td>0.8946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1859435737133026, 0.9332666397094727]</td>\n",
       "      <td>[0.2703552842140198, 0.9021999835968018]</td>\n",
       "      <td>0.185944</td>\n",
       "      <td>0.933267</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.9022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.17796066403388977, 0.9342333078384399]</td>\n",
       "      <td>[0.28307250142097473, 0.8967000246047974]</td>\n",
       "      <td>0.177961</td>\n",
       "      <td>0.934233</td>\n",
       "      <td>0.283073</td>\n",
       "      <td>0.8967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21107719838619232, 0.9233666658401489]</td>\n",
       "      <td>[0.28365108370780945, 0.8959000110626221]</td>\n",
       "      <td>0.211077</td>\n",
       "      <td>0.923367</td>\n",
       "      <td>0.283651</td>\n",
       "      <td>0.8959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19442103803157806, 0.9269833564758301]</td>\n",
       "      <td>[0.27573519945144653, 0.9016000032424927]</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.926983</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1608797013759613, 0.9412999749183655]</td>\n",
       "      <td>[0.26573246717453003, 0.9079999923706055]</td>\n",
       "      <td>0.160880</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.265732</td>\n",
       "      <td>0.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15656131505966187, 0.9422333240509033]</td>\n",
       "      <td>[0.25688880681991577, 0.9093000292778015]</td>\n",
       "      <td>0.156561</td>\n",
       "      <td>0.942233</td>\n",
       "      <td>0.256889</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1546628773212433, 0.9443833231925964]</td>\n",
       "      <td>[0.27218130230903625, 0.90420001745224]</td>\n",
       "      <td>0.154663</td>\n",
       "      <td>0.944383</td>\n",
       "      <td>0.272181</td>\n",
       "      <td>0.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18178533017635345, 0.9347333312034607]</td>\n",
       "      <td>[0.2796153426170349, 0.9016000032424927]</td>\n",
       "      <td>0.181785</td>\n",
       "      <td>0.934733</td>\n",
       "      <td>0.279615</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20485873520374298, 0.9259999990463257]</td>\n",
       "      <td>[0.285814106464386, 0.8978000283241272]</td>\n",
       "      <td>0.204859</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.285814</td>\n",
       "      <td>0.8978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14446735382080078, 0.948033332824707]</td>\n",
       "      <td>[0.25142455101013184, 0.9111999869346619]</td>\n",
       "      <td>0.144467</td>\n",
       "      <td>0.948033</td>\n",
       "      <td>0.251425</td>\n",
       "      <td>0.9112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18002688884735107, 0.9334666728973389]</td>\n",
       "      <td>[0.2554672062397003, 0.9071999788284302]</td>\n",
       "      <td>0.180027</td>\n",
       "      <td>0.933467</td>\n",
       "      <td>0.255467</td>\n",
       "      <td>0.9072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15237271785736084, 0.9448999762535095]</td>\n",
       "      <td>[0.24747571349143982, 0.911300003528595]</td>\n",
       "      <td>0.152373</td>\n",
       "      <td>0.944900</td>\n",
       "      <td>0.247476</td>\n",
       "      <td>0.9113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22614991664886475, 0.9175000190734863]</td>\n",
       "      <td>[0.3019619286060333, 0.8912000060081482]</td>\n",
       "      <td>0.226150</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.301962</td>\n",
       "      <td>0.8912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21326030790805817, 0.9208333492279053]</td>\n",
       "      <td>[0.30138877034187317, 0.8938000202178955]</td>\n",
       "      <td>0.213260</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.301389</td>\n",
       "      <td>0.8938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2588706910610199, 0.9059000015258789]</td>\n",
       "      <td>[0.3161528706550598, 0.8884999752044678]</td>\n",
       "      <td>0.258871</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.316153</td>\n",
       "      <td>0.8885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.24779362976551056, 0.9068999886512756]</td>\n",
       "      <td>[0.2994548976421356, 0.8888999819755554]</td>\n",
       "      <td>0.247794</td>\n",
       "      <td>0.906900</td>\n",
       "      <td>0.299455</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21686747670173645, 0.9199833273887634]</td>\n",
       "      <td>[0.2744320034980774, 0.902400016784668]</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.919983</td>\n",
       "      <td>0.274432</td>\n",
       "      <td>0.9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23349125683307648, 0.9122499823570251]</td>\n",
       "      <td>[0.287698894739151, 0.8937000036239624]</td>\n",
       "      <td>0.233491</td>\n",
       "      <td>0.912250</td>\n",
       "      <td>0.287699</td>\n",
       "      <td>0.8937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22014310956001282, 0.9203000068664551]</td>\n",
       "      <td>[0.2999141812324524, 0.891700029373169]</td>\n",
       "      <td>0.220143</td>\n",
       "      <td>0.920300</td>\n",
       "      <td>0.299914</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.25457531213760376, 0.90829998254776]</td>\n",
       "      <td>[0.3104911148548126, 0.891700029373169]</td>\n",
       "      <td>0.254575</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>0.310491</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2692805528640747, 0.9014833569526672]</td>\n",
       "      <td>[0.32393553853034973, 0.8855999708175659]</td>\n",
       "      <td>0.269281</td>\n",
       "      <td>0.901483</td>\n",
       "      <td>0.323936</td>\n",
       "      <td>0.8856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2132299542427063, 0.9207500219345093]</td>\n",
       "      <td>[0.27428877353668213, 0.9002000093460083]</td>\n",
       "      <td>0.213230</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274289</td>\n",
       "      <td>0.9002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21291086077690125, 0.9204333424568176]</td>\n",
       "      <td>[0.27476274967193604, 0.8995000123977661]</td>\n",
       "      <td>0.212911</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.274763</td>\n",
       "      <td>0.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23679600656032562, 0.9127500057220459]</td>\n",
       "      <td>[0.28981828689575195, 0.8930000066757202]</td>\n",
       "      <td>0.236796</td>\n",
       "      <td>0.912750</td>\n",
       "      <td>0.289818</td>\n",
       "      <td>0.8930</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  \\\n",
       "0   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "1   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "2   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "3   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "4   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "5   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "6   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "7   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "8   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "9   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "10  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "11  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "12  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "13  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "14  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "15  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "16  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "17  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "18  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "19  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "20  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "21  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "22  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "23  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "24  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "25  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "26  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "27  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "28  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "29  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "30  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "31  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "32  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "33  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "34  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "35  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "36  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "37  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "38  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "39  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "40  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "41  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "42  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "43  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "44  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "45  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "46  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "47  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "\n",
       "                                   train_eval  \\\n",
       "0   [0.16868141293525696, 0.9378499984741211]   \n",
       "1   [0.15938624739646912, 0.9430500268936157]   \n",
       "2     [0.1417781561613083, 0.948366641998291]   \n",
       "3   [0.15062175691127777, 0.9443666934967041]   \n",
       "4   [0.13222971558570862, 0.9526333212852478]   \n",
       "5     [0.1857813447713852, 0.930899977684021]   \n",
       "6    [0.1465650051832199, 0.9471666812896729]   \n",
       "7    [0.1708768606185913, 0.9378499984741211]   \n",
       "8     [0.22205980122089386, 0.92166668176651]   \n",
       "9   [0.12658622860908508, 0.9560166597366333]   \n",
       "10   [0.1421215534210205, 0.9477333426475525]   \n",
       "11  [0.14996056258678436, 0.9464333057403564]   \n",
       "12  [0.23593302071094513, 0.9116500020027161]   \n",
       "13  [0.22073142230510712, 0.9183333516120911]   \n",
       "14  [0.27241677045822144, 0.9009333252906799]   \n",
       "15  [0.21186839044094086, 0.9207500219345093]   \n",
       "16  [0.19306862354278564, 0.9285666942596436]   \n",
       "17  [0.21572570502758026, 0.9204333424568176]   \n",
       "18  [0.23643141984939575, 0.9130666851997375]   \n",
       "19  [0.29450979828834534, 0.8953333497047424]   \n",
       "20   [0.2675584852695465, 0.9041666388511658]   \n",
       "21  [0.19672514498233795, 0.9282000064849854]   \n",
       "22  [0.20643432438373566, 0.9244666695594788]   \n",
       "23  [0.23273038864135742, 0.9152833223342896]   \n",
       "24   [0.1859435737133026, 0.9332666397094727]   \n",
       "25  [0.17796066403388977, 0.9342333078384399]   \n",
       "26  [0.21107719838619232, 0.9233666658401489]   \n",
       "27  [0.19442103803157806, 0.9269833564758301]   \n",
       "28   [0.1608797013759613, 0.9412999749183655]   \n",
       "29  [0.15656131505966187, 0.9422333240509033]   \n",
       "30   [0.1546628773212433, 0.9443833231925964]   \n",
       "31  [0.18178533017635345, 0.9347333312034607]   \n",
       "32  [0.20485873520374298, 0.9259999990463257]   \n",
       "33   [0.14446735382080078, 0.948033332824707]   \n",
       "34  [0.18002688884735107, 0.9334666728973389]   \n",
       "35  [0.15237271785736084, 0.9448999762535095]   \n",
       "36  [0.22614991664886475, 0.9175000190734863]   \n",
       "37  [0.21326030790805817, 0.9208333492279053]   \n",
       "38   [0.2588706910610199, 0.9059000015258789]   \n",
       "39  [0.24779362976551056, 0.9068999886512756]   \n",
       "40  [0.21686747670173645, 0.9199833273887634]   \n",
       "41  [0.23349125683307648, 0.9122499823570251]   \n",
       "42  [0.22014310956001282, 0.9203000068664551]   \n",
       "43    [0.25457531213760376, 0.90829998254776]   \n",
       "44   [0.2692805528640747, 0.9014833569526672]   \n",
       "45   [0.2132299542427063, 0.9207500219345093]   \n",
       "46  [0.21291086077690125, 0.9204333424568176]   \n",
       "47  [0.23679600656032562, 0.9127500057220459]   \n",
       "\n",
       "                                    test_eval  train_loss  train_acc  \\\n",
       "0   [0.26669758558273315, 0.9018999934196472]    0.168681   0.937850   \n",
       "1     [0.2670109272003174, 0.904699981212616]    0.159386   0.943050   \n",
       "2   [0.25698399543762207, 0.9108999967575073]    0.141778   0.948367   \n",
       "3      [0.252878874540329, 0.909500002861023]    0.150622   0.944367   \n",
       "4   [0.24787791073322296, 0.9126999974250793]    0.132230   0.952633   \n",
       "5   [0.25688549876213074, 0.9057000279426575]    0.185781   0.930900   \n",
       "6   [0.24840134382247925, 0.9114999771118164]    0.146565   0.947167   \n",
       "7    [0.2613731920719147, 0.9070000052452087]    0.170877   0.937850   \n",
       "8   [0.28667232394218445, 0.8970999717712402]    0.222060   0.921667   \n",
       "9   [0.24576333165168762, 0.9114999771118164]    0.126586   0.956017   \n",
       "10  [0.24299895763397217, 0.9117000102996826]    0.142122   0.947733   \n",
       "11  [0.24511033296585083, 0.9093000292778015]    0.149961   0.946433   \n",
       "12  [0.29616788029670715, 0.8928999900817871]    0.235933   0.911650   \n",
       "13   [0.2996402978897095, 0.8896999955177307]    0.220731   0.918333   \n",
       "14  [0.31941139698028564, 0.8870999813079834]    0.272417   0.900933   \n",
       "15   [0.2745698392391205, 0.8982999920845032]    0.211868   0.920750   \n",
       "16   [0.2737598717212677, 0.9014999866485596]    0.193069   0.928567   \n",
       "17   [0.2723506689071655, 0.8992999792098999]    0.215726   0.920433   \n",
       "18  [0.30026862025260925, 0.8944000005722046]    0.236431   0.913067   \n",
       "19   [0.3364871144294739, 0.8776999711990356]    0.294510   0.895333   \n",
       "20   [0.3267644941806793, 0.8819000124931335]    0.267558   0.904167   \n",
       "21  [0.25953343510627747, 0.9054999947547913]    0.196725   0.928200   \n",
       "22   [0.2693329453468323, 0.9034000039100647]    0.206434   0.924467   \n",
       "23    [0.2840319871902466, 0.894599974155426]    0.232730   0.915283   \n",
       "24   [0.2703552842140198, 0.9021999835968018]    0.185944   0.933267   \n",
       "25  [0.28307250142097473, 0.8967000246047974]    0.177961   0.934233   \n",
       "26  [0.28365108370780945, 0.8959000110626221]    0.211077   0.923367   \n",
       "27  [0.27573519945144653, 0.9016000032424927]    0.194421   0.926983   \n",
       "28  [0.26573246717453003, 0.9079999923706055]    0.160880   0.941300   \n",
       "29  [0.25688880681991577, 0.9093000292778015]    0.156561   0.942233   \n",
       "30    [0.27218130230903625, 0.90420001745224]    0.154663   0.944383   \n",
       "31   [0.2796153426170349, 0.9016000032424927]    0.181785   0.934733   \n",
       "32    [0.285814106464386, 0.8978000283241272]    0.204859   0.926000   \n",
       "33  [0.25142455101013184, 0.9111999869346619]    0.144467   0.948033   \n",
       "34   [0.2554672062397003, 0.9071999788284302]    0.180027   0.933467   \n",
       "35   [0.24747571349143982, 0.911300003528595]    0.152373   0.944900   \n",
       "36   [0.3019619286060333, 0.8912000060081482]    0.226150   0.917500   \n",
       "37  [0.30138877034187317, 0.8938000202178955]    0.213260   0.920833   \n",
       "38   [0.3161528706550598, 0.8884999752044678]    0.258871   0.905900   \n",
       "39   [0.2994548976421356, 0.8888999819755554]    0.247794   0.906900   \n",
       "40    [0.2744320034980774, 0.902400016784668]    0.216867   0.919983   \n",
       "41    [0.287698894739151, 0.8937000036239624]    0.233491   0.912250   \n",
       "42    [0.2999141812324524, 0.891700029373169]    0.220143   0.920300   \n",
       "43    [0.3104911148548126, 0.891700029373169]    0.254575   0.908300   \n",
       "44  [0.32393553853034973, 0.8855999708175659]    0.269281   0.901483   \n",
       "45  [0.27428877353668213, 0.9002000093460083]    0.213230   0.920750   \n",
       "46  [0.27476274967193604, 0.8995000123977661]    0.212911   0.920433   \n",
       "47  [0.28981828689575195, 0.8930000066757202]    0.236796   0.912750   \n",
       "\n",
       "    test_loss  test_acc  \n",
       "0    0.266698    0.9019  \n",
       "1    0.267011    0.9047  \n",
       "2    0.256984    0.9109  \n",
       "3    0.252879    0.9095  \n",
       "4    0.247878    0.9127  \n",
       "5    0.256885    0.9057  \n",
       "6    0.248401    0.9115  \n",
       "7    0.261373    0.9070  \n",
       "8    0.286672    0.8971  \n",
       "9    0.245763    0.9115  \n",
       "10   0.242999    0.9117  \n",
       "11   0.245110    0.9093  \n",
       "12   0.296168    0.8929  \n",
       "13   0.299640    0.8897  \n",
       "14   0.319411    0.8871  \n",
       "15   0.274570    0.8983  \n",
       "16   0.273760    0.9015  \n",
       "17   0.272351    0.8993  \n",
       "18   0.300269    0.8944  \n",
       "19   0.336487    0.8777  \n",
       "20   0.326764    0.8819  \n",
       "21   0.259533    0.9055  \n",
       "22   0.269333    0.9034  \n",
       "23   0.284032    0.8946  \n",
       "24   0.270355    0.9022  \n",
       "25   0.283073    0.8967  \n",
       "26   0.283651    0.8959  \n",
       "27   0.275735    0.9016  \n",
       "28   0.265732    0.9080  \n",
       "29   0.256889    0.9093  \n",
       "30   0.272181    0.9042  \n",
       "31   0.279615    0.9016  \n",
       "32   0.285814    0.8978  \n",
       "33   0.251425    0.9112  \n",
       "34   0.255467    0.9072  \n",
       "35   0.247476    0.9113  \n",
       "36   0.301962    0.8912  \n",
       "37   0.301389    0.8938  \n",
       "38   0.316153    0.8885  \n",
       "39   0.299455    0.8889  \n",
       "40   0.274432    0.9024  \n",
       "41   0.287699    0.8937  \n",
       "42   0.299914    0.8917  \n",
       "43   0.310491    0.8917  \n",
       "44   0.323936    0.8856  \n",
       "45   0.274289    0.9002  \n",
       "46   0.274763    0.8995  \n",
       "47   0.289818    0.8930  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO setup similar decisions as the Torch notebook - best test_acc, lowest loss, train one, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_eval</th>\n",
       "      <th>test_eval</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.13222971558570862, 0.9526333212852478]</td>\n",
       "      <td>[0.24787791073322296, 0.9126999974250793]</td>\n",
       "      <td>0.132230</td>\n",
       "      <td>0.952633</td>\n",
       "      <td>0.247878</td>\n",
       "      <td>0.9127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1421215534210205, 0.9477333426475525]</td>\n",
       "      <td>[0.24299895763397217, 0.9117000102996826]</td>\n",
       "      <td>0.142122</td>\n",
       "      <td>0.947733</td>\n",
       "      <td>0.242999</td>\n",
       "      <td>0.9117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.12658622860908508, 0.9560166597366333]</td>\n",
       "      <td>[0.24576333165168762, 0.9114999771118164]</td>\n",
       "      <td>0.126586</td>\n",
       "      <td>0.956017</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1465650051832199, 0.9471666812896729]</td>\n",
       "      <td>[0.24840134382247925, 0.9114999771118164]</td>\n",
       "      <td>0.146565</td>\n",
       "      <td>0.947167</td>\n",
       "      <td>0.248401</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15237271785736084, 0.9448999762535095]</td>\n",
       "      <td>[0.24747571349143982, 0.911300003528595]</td>\n",
       "      <td>0.152373</td>\n",
       "      <td>0.944900</td>\n",
       "      <td>0.247476</td>\n",
       "      <td>0.9113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14446735382080078, 0.948033332824707]</td>\n",
       "      <td>[0.25142455101013184, 0.9111999869346619]</td>\n",
       "      <td>0.144467</td>\n",
       "      <td>0.948033</td>\n",
       "      <td>0.251425</td>\n",
       "      <td>0.9112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1417781561613083, 0.948366641998291]</td>\n",
       "      <td>[0.25698399543762207, 0.9108999967575073]</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>0.948367</td>\n",
       "      <td>0.256984</td>\n",
       "      <td>0.9109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15062175691127777, 0.9443666934967041]</td>\n",
       "      <td>[0.252878874540329, 0.909500002861023]</td>\n",
       "      <td>0.150622</td>\n",
       "      <td>0.944367</td>\n",
       "      <td>0.252879</td>\n",
       "      <td>0.9095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14996056258678436, 0.9464333057403564]</td>\n",
       "      <td>[0.24511033296585083, 0.9093000292778015]</td>\n",
       "      <td>0.149961</td>\n",
       "      <td>0.946433</td>\n",
       "      <td>0.245110</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15656131505966187, 0.9422333240509033]</td>\n",
       "      <td>[0.25688880681991577, 0.9093000292778015]</td>\n",
       "      <td>0.156561</td>\n",
       "      <td>0.942233</td>\n",
       "      <td>0.256889</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1608797013759613, 0.9412999749183655]</td>\n",
       "      <td>[0.26573246717453003, 0.9079999923706055]</td>\n",
       "      <td>0.160880</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.265732</td>\n",
       "      <td>0.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18002688884735107, 0.9334666728973389]</td>\n",
       "      <td>[0.2554672062397003, 0.9071999788284302]</td>\n",
       "      <td>0.180027</td>\n",
       "      <td>0.933467</td>\n",
       "      <td>0.255467</td>\n",
       "      <td>0.9072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1708768606185913, 0.9378499984741211]</td>\n",
       "      <td>[0.2613731920719147, 0.9070000052452087]</td>\n",
       "      <td>0.170877</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.261373</td>\n",
       "      <td>0.9070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1857813447713852, 0.930899977684021]</td>\n",
       "      <td>[0.25688549876213074, 0.9057000279426575]</td>\n",
       "      <td>0.185781</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>0.256885</td>\n",
       "      <td>0.9057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19672514498233795, 0.9282000064849854]</td>\n",
       "      <td>[0.25953343510627747, 0.9054999947547913]</td>\n",
       "      <td>0.196725</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.259533</td>\n",
       "      <td>0.9055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15938624739646912, 0.9430500268936157]</td>\n",
       "      <td>[0.2670109272003174, 0.904699981212616]</td>\n",
       "      <td>0.159386</td>\n",
       "      <td>0.943050</td>\n",
       "      <td>0.267011</td>\n",
       "      <td>0.9047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1546628773212433, 0.9443833231925964]</td>\n",
       "      <td>[0.27218130230903625, 0.90420001745224]</td>\n",
       "      <td>0.154663</td>\n",
       "      <td>0.944383</td>\n",
       "      <td>0.272181</td>\n",
       "      <td>0.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20643432438373566, 0.9244666695594788]</td>\n",
       "      <td>[0.2693329453468323, 0.9034000039100647]</td>\n",
       "      <td>0.206434</td>\n",
       "      <td>0.924467</td>\n",
       "      <td>0.269333</td>\n",
       "      <td>0.9034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21686747670173645, 0.9199833273887634]</td>\n",
       "      <td>[0.2744320034980774, 0.902400016784668]</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.919983</td>\n",
       "      <td>0.274432</td>\n",
       "      <td>0.9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1859435737133026, 0.9332666397094727]</td>\n",
       "      <td>[0.2703552842140198, 0.9021999835968018]</td>\n",
       "      <td>0.185944</td>\n",
       "      <td>0.933267</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.9022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.16868141293525696, 0.9378499984741211]</td>\n",
       "      <td>[0.26669758558273315, 0.9018999934196472]</td>\n",
       "      <td>0.168681</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.266698</td>\n",
       "      <td>0.9019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18178533017635345, 0.9347333312034607]</td>\n",
       "      <td>[0.2796153426170349, 0.9016000032424927]</td>\n",
       "      <td>0.181785</td>\n",
       "      <td>0.934733</td>\n",
       "      <td>0.279615</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19442103803157806, 0.9269833564758301]</td>\n",
       "      <td>[0.27573519945144653, 0.9016000032424927]</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.926983</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19306862354278564, 0.9285666942596436]</td>\n",
       "      <td>[0.2737598717212677, 0.9014999866485596]</td>\n",
       "      <td>0.193069</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.273760</td>\n",
       "      <td>0.9015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2132299542427063, 0.9207500219345093]</td>\n",
       "      <td>[0.27428877353668213, 0.9002000093460083]</td>\n",
       "      <td>0.213230</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274289</td>\n",
       "      <td>0.9002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21291086077690125, 0.9204333424568176]</td>\n",
       "      <td>[0.27476274967193604, 0.8995000123977661]</td>\n",
       "      <td>0.212911</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.274763</td>\n",
       "      <td>0.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21572570502758026, 0.9204333424568176]</td>\n",
       "      <td>[0.2723506689071655, 0.8992999792098999]</td>\n",
       "      <td>0.215726</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.272351</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21186839044094086, 0.9207500219345093]</td>\n",
       "      <td>[0.2745698392391205, 0.8982999920845032]</td>\n",
       "      <td>0.211868</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274570</td>\n",
       "      <td>0.8983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20485873520374298, 0.9259999990463257]</td>\n",
       "      <td>[0.285814106464386, 0.8978000283241272]</td>\n",
       "      <td>0.204859</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.285814</td>\n",
       "      <td>0.8978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22205980122089386, 0.92166668176651]</td>\n",
       "      <td>[0.28667232394218445, 0.8970999717712402]</td>\n",
       "      <td>0.222060</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>0.286672</td>\n",
       "      <td>0.8971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.17796066403388977, 0.9342333078384399]</td>\n",
       "      <td>[0.28307250142097473, 0.8967000246047974]</td>\n",
       "      <td>0.177961</td>\n",
       "      <td>0.934233</td>\n",
       "      <td>0.283073</td>\n",
       "      <td>0.8967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21107719838619232, 0.9233666658401489]</td>\n",
       "      <td>[0.28365108370780945, 0.8959000110626221]</td>\n",
       "      <td>0.211077</td>\n",
       "      <td>0.923367</td>\n",
       "      <td>0.283651</td>\n",
       "      <td>0.8959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23273038864135742, 0.9152833223342896]</td>\n",
       "      <td>[0.2840319871902466, 0.894599974155426]</td>\n",
       "      <td>0.232730</td>\n",
       "      <td>0.915283</td>\n",
       "      <td>0.284032</td>\n",
       "      <td>0.8946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23643141984939575, 0.9130666851997375]</td>\n",
       "      <td>[0.30026862025260925, 0.8944000005722046]</td>\n",
       "      <td>0.236431</td>\n",
       "      <td>0.913067</td>\n",
       "      <td>0.300269</td>\n",
       "      <td>0.8944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21326030790805817, 0.9208333492279053]</td>\n",
       "      <td>[0.30138877034187317, 0.8938000202178955]</td>\n",
       "      <td>0.213260</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.301389</td>\n",
       "      <td>0.8938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23349125683307648, 0.9122499823570251]</td>\n",
       "      <td>[0.287698894739151, 0.8937000036239624]</td>\n",
       "      <td>0.233491</td>\n",
       "      <td>0.912250</td>\n",
       "      <td>0.287699</td>\n",
       "      <td>0.8937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23679600656032562, 0.9127500057220459]</td>\n",
       "      <td>[0.28981828689575195, 0.8930000066757202]</td>\n",
       "      <td>0.236796</td>\n",
       "      <td>0.912750</td>\n",
       "      <td>0.289818</td>\n",
       "      <td>0.8930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23593302071094513, 0.9116500020027161]</td>\n",
       "      <td>[0.29616788029670715, 0.8928999900817871]</td>\n",
       "      <td>0.235933</td>\n",
       "      <td>0.911650</td>\n",
       "      <td>0.296168</td>\n",
       "      <td>0.8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.25457531213760376, 0.90829998254776]</td>\n",
       "      <td>[0.3104911148548126, 0.891700029373169]</td>\n",
       "      <td>0.254575</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>0.310491</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22014310956001282, 0.9203000068664551]</td>\n",
       "      <td>[0.2999141812324524, 0.891700029373169]</td>\n",
       "      <td>0.220143</td>\n",
       "      <td>0.920300</td>\n",
       "      <td>0.299914</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22614991664886475, 0.9175000190734863]</td>\n",
       "      <td>[0.3019619286060333, 0.8912000060081482]</td>\n",
       "      <td>0.226150</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.301962</td>\n",
       "      <td>0.8912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22073142230510712, 0.9183333516120911]</td>\n",
       "      <td>[0.2996402978897095, 0.8896999955177307]</td>\n",
       "      <td>0.220731</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>0.299640</td>\n",
       "      <td>0.8897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.24779362976551056, 0.9068999886512756]</td>\n",
       "      <td>[0.2994548976421356, 0.8888999819755554]</td>\n",
       "      <td>0.247794</td>\n",
       "      <td>0.906900</td>\n",
       "      <td>0.299455</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2588706910610199, 0.9059000015258789]</td>\n",
       "      <td>[0.3161528706550598, 0.8884999752044678]</td>\n",
       "      <td>0.258871</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.316153</td>\n",
       "      <td>0.8885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.27241677045822144, 0.9009333252906799]</td>\n",
       "      <td>[0.31941139698028564, 0.8870999813079834]</td>\n",
       "      <td>0.272417</td>\n",
       "      <td>0.900933</td>\n",
       "      <td>0.319411</td>\n",
       "      <td>0.8871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2692805528640747, 0.9014833569526672]</td>\n",
       "      <td>[0.32393553853034973, 0.8855999708175659]</td>\n",
       "      <td>0.269281</td>\n",
       "      <td>0.901483</td>\n",
       "      <td>0.323936</td>\n",
       "      <td>0.8856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2675584852695465, 0.9041666388511658]</td>\n",
       "      <td>[0.3267644941806793, 0.8819000124931335]</td>\n",
       "      <td>0.267558</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>0.326764</td>\n",
       "      <td>0.8819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.29450979828834534, 0.8953333497047424]</td>\n",
       "      <td>[0.3364871144294739, 0.8776999711990356]</td>\n",
       "      <td>0.294510</td>\n",
       "      <td>0.895333</td>\n",
       "      <td>0.336487</td>\n",
       "      <td>0.8777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  \\\n",
       "4   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "10  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "9   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "6   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "35  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "33  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "2   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "3   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "11  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "29  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "28  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "34  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "7   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "5   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "21  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "1   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "30  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "22  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "40  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "24  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "0   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "31  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "27  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "16  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "45  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "46  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "17  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "15  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "32  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "8   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "25  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "26  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "23  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "18  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "37  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "41  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "47  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "12  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "43  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "42  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "36  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "13  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "39  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "38  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "14  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "44  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "20  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "19  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "\n",
       "                                   train_eval  \\\n",
       "4   [0.13222971558570862, 0.9526333212852478]   \n",
       "10   [0.1421215534210205, 0.9477333426475525]   \n",
       "9   [0.12658622860908508, 0.9560166597366333]   \n",
       "6    [0.1465650051832199, 0.9471666812896729]   \n",
       "35  [0.15237271785736084, 0.9448999762535095]   \n",
       "33   [0.14446735382080078, 0.948033332824707]   \n",
       "2     [0.1417781561613083, 0.948366641998291]   \n",
       "3   [0.15062175691127777, 0.9443666934967041]   \n",
       "11  [0.14996056258678436, 0.9464333057403564]   \n",
       "29  [0.15656131505966187, 0.9422333240509033]   \n",
       "28   [0.1608797013759613, 0.9412999749183655]   \n",
       "34  [0.18002688884735107, 0.9334666728973389]   \n",
       "7    [0.1708768606185913, 0.9378499984741211]   \n",
       "5     [0.1857813447713852, 0.930899977684021]   \n",
       "21  [0.19672514498233795, 0.9282000064849854]   \n",
       "1   [0.15938624739646912, 0.9430500268936157]   \n",
       "30   [0.1546628773212433, 0.9443833231925964]   \n",
       "22  [0.20643432438373566, 0.9244666695594788]   \n",
       "40  [0.21686747670173645, 0.9199833273887634]   \n",
       "24   [0.1859435737133026, 0.9332666397094727]   \n",
       "0   [0.16868141293525696, 0.9378499984741211]   \n",
       "31  [0.18178533017635345, 0.9347333312034607]   \n",
       "27  [0.19442103803157806, 0.9269833564758301]   \n",
       "16  [0.19306862354278564, 0.9285666942596436]   \n",
       "45   [0.2132299542427063, 0.9207500219345093]   \n",
       "46  [0.21291086077690125, 0.9204333424568176]   \n",
       "17  [0.21572570502758026, 0.9204333424568176]   \n",
       "15  [0.21186839044094086, 0.9207500219345093]   \n",
       "32  [0.20485873520374298, 0.9259999990463257]   \n",
       "8     [0.22205980122089386, 0.92166668176651]   \n",
       "25  [0.17796066403388977, 0.9342333078384399]   \n",
       "26  [0.21107719838619232, 0.9233666658401489]   \n",
       "23  [0.23273038864135742, 0.9152833223342896]   \n",
       "18  [0.23643141984939575, 0.9130666851997375]   \n",
       "37  [0.21326030790805817, 0.9208333492279053]   \n",
       "41  [0.23349125683307648, 0.9122499823570251]   \n",
       "47  [0.23679600656032562, 0.9127500057220459]   \n",
       "12  [0.23593302071094513, 0.9116500020027161]   \n",
       "43    [0.25457531213760376, 0.90829998254776]   \n",
       "42  [0.22014310956001282, 0.9203000068664551]   \n",
       "36  [0.22614991664886475, 0.9175000190734863]   \n",
       "13  [0.22073142230510712, 0.9183333516120911]   \n",
       "39  [0.24779362976551056, 0.9068999886512756]   \n",
       "38   [0.2588706910610199, 0.9059000015258789]   \n",
       "14  [0.27241677045822144, 0.9009333252906799]   \n",
       "44   [0.2692805528640747, 0.9014833569526672]   \n",
       "20   [0.2675584852695465, 0.9041666388511658]   \n",
       "19  [0.29450979828834534, 0.8953333497047424]   \n",
       "\n",
       "                                    test_eval  train_loss  train_acc  \\\n",
       "4   [0.24787791073322296, 0.9126999974250793]    0.132230   0.952633   \n",
       "10  [0.24299895763397217, 0.9117000102996826]    0.142122   0.947733   \n",
       "9   [0.24576333165168762, 0.9114999771118164]    0.126586   0.956017   \n",
       "6   [0.24840134382247925, 0.9114999771118164]    0.146565   0.947167   \n",
       "35   [0.24747571349143982, 0.911300003528595]    0.152373   0.944900   \n",
       "33  [0.25142455101013184, 0.9111999869346619]    0.144467   0.948033   \n",
       "2   [0.25698399543762207, 0.9108999967575073]    0.141778   0.948367   \n",
       "3      [0.252878874540329, 0.909500002861023]    0.150622   0.944367   \n",
       "11  [0.24511033296585083, 0.9093000292778015]    0.149961   0.946433   \n",
       "29  [0.25688880681991577, 0.9093000292778015]    0.156561   0.942233   \n",
       "28  [0.26573246717453003, 0.9079999923706055]    0.160880   0.941300   \n",
       "34   [0.2554672062397003, 0.9071999788284302]    0.180027   0.933467   \n",
       "7    [0.2613731920719147, 0.9070000052452087]    0.170877   0.937850   \n",
       "5   [0.25688549876213074, 0.9057000279426575]    0.185781   0.930900   \n",
       "21  [0.25953343510627747, 0.9054999947547913]    0.196725   0.928200   \n",
       "1     [0.2670109272003174, 0.904699981212616]    0.159386   0.943050   \n",
       "30    [0.27218130230903625, 0.90420001745224]    0.154663   0.944383   \n",
       "22   [0.2693329453468323, 0.9034000039100647]    0.206434   0.924467   \n",
       "40    [0.2744320034980774, 0.902400016784668]    0.216867   0.919983   \n",
       "24   [0.2703552842140198, 0.9021999835968018]    0.185944   0.933267   \n",
       "0   [0.26669758558273315, 0.9018999934196472]    0.168681   0.937850   \n",
       "31   [0.2796153426170349, 0.9016000032424927]    0.181785   0.934733   \n",
       "27  [0.27573519945144653, 0.9016000032424927]    0.194421   0.926983   \n",
       "16   [0.2737598717212677, 0.9014999866485596]    0.193069   0.928567   \n",
       "45  [0.27428877353668213, 0.9002000093460083]    0.213230   0.920750   \n",
       "46  [0.27476274967193604, 0.8995000123977661]    0.212911   0.920433   \n",
       "17   [0.2723506689071655, 0.8992999792098999]    0.215726   0.920433   \n",
       "15   [0.2745698392391205, 0.8982999920845032]    0.211868   0.920750   \n",
       "32    [0.285814106464386, 0.8978000283241272]    0.204859   0.926000   \n",
       "8   [0.28667232394218445, 0.8970999717712402]    0.222060   0.921667   \n",
       "25  [0.28307250142097473, 0.8967000246047974]    0.177961   0.934233   \n",
       "26  [0.28365108370780945, 0.8959000110626221]    0.211077   0.923367   \n",
       "23    [0.2840319871902466, 0.894599974155426]    0.232730   0.915283   \n",
       "18  [0.30026862025260925, 0.8944000005722046]    0.236431   0.913067   \n",
       "37  [0.30138877034187317, 0.8938000202178955]    0.213260   0.920833   \n",
       "41    [0.287698894739151, 0.8937000036239624]    0.233491   0.912250   \n",
       "47  [0.28981828689575195, 0.8930000066757202]    0.236796   0.912750   \n",
       "12  [0.29616788029670715, 0.8928999900817871]    0.235933   0.911650   \n",
       "43    [0.3104911148548126, 0.891700029373169]    0.254575   0.908300   \n",
       "42    [0.2999141812324524, 0.891700029373169]    0.220143   0.920300   \n",
       "36   [0.3019619286060333, 0.8912000060081482]    0.226150   0.917500   \n",
       "13   [0.2996402978897095, 0.8896999955177307]    0.220731   0.918333   \n",
       "39   [0.2994548976421356, 0.8888999819755554]    0.247794   0.906900   \n",
       "38   [0.3161528706550598, 0.8884999752044678]    0.258871   0.905900   \n",
       "14  [0.31941139698028564, 0.8870999813079834]    0.272417   0.900933   \n",
       "44  [0.32393553853034973, 0.8855999708175659]    0.269281   0.901483   \n",
       "20   [0.3267644941806793, 0.8819000124931335]    0.267558   0.904167   \n",
       "19   [0.3364871144294739, 0.8776999711990356]    0.294510   0.895333   \n",
       "\n",
       "    test_loss  test_acc  \n",
       "4    0.247878    0.9127  \n",
       "10   0.242999    0.9117  \n",
       "9    0.245763    0.9115  \n",
       "6    0.248401    0.9115  \n",
       "35   0.247476    0.9113  \n",
       "33   0.251425    0.9112  \n",
       "2    0.256984    0.9109  \n",
       "3    0.252879    0.9095  \n",
       "11   0.245110    0.9093  \n",
       "29   0.256889    0.9093  \n",
       "28   0.265732    0.9080  \n",
       "34   0.255467    0.9072  \n",
       "7    0.261373    0.9070  \n",
       "5    0.256885    0.9057  \n",
       "21   0.259533    0.9055  \n",
       "1    0.267011    0.9047  \n",
       "30   0.272181    0.9042  \n",
       "22   0.269333    0.9034  \n",
       "40   0.274432    0.9024  \n",
       "24   0.270355    0.9022  \n",
       "0    0.266698    0.9019  \n",
       "31   0.279615    0.9016  \n",
       "27   0.275735    0.9016  \n",
       "16   0.273760    0.9015  \n",
       "45   0.274289    0.9002  \n",
       "46   0.274763    0.8995  \n",
       "17   0.272351    0.8993  \n",
       "15   0.274570    0.8983  \n",
       "32   0.285814    0.8978  \n",
       "8    0.286672    0.8971  \n",
       "25   0.283073    0.8967  \n",
       "26   0.283651    0.8959  \n",
       "23   0.284032    0.8946  \n",
       "18   0.300269    0.8944  \n",
       "37   0.301389    0.8938  \n",
       "41   0.287699    0.8937  \n",
       "47   0.289818    0.8930  \n",
       "12   0.296168    0.8929  \n",
       "43   0.310491    0.8917  \n",
       "42   0.299914    0.8917  \n",
       "36   0.301962    0.8912  \n",
       "13   0.299640    0.8897  \n",
       "39   0.299455    0.8889  \n",
       "38   0.316153    0.8885  \n",
       "14   0.319411    0.8871  \n",
       "44   0.323936    0.8856  \n",
       "20   0.326764    0.8819  \n",
       "19   0.336487    0.8777  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.sort_values(by=['test_acc'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_eval</th>\n",
       "      <th>test_eval</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1421215534210205, 0.9477333426475525]</td>\n",
       "      <td>[0.24299895763397217, 0.9117000102996826]</td>\n",
       "      <td>0.142122</td>\n",
       "      <td>0.947733</td>\n",
       "      <td>0.242999</td>\n",
       "      <td>0.9117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14996056258678436, 0.9464333057403564]</td>\n",
       "      <td>[0.24511033296585083, 0.9093000292778015]</td>\n",
       "      <td>0.149961</td>\n",
       "      <td>0.946433</td>\n",
       "      <td>0.245110</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.12658622860908508, 0.9560166597366333]</td>\n",
       "      <td>[0.24576333165168762, 0.9114999771118164]</td>\n",
       "      <td>0.126586</td>\n",
       "      <td>0.956017</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15237271785736084, 0.9448999762535095]</td>\n",
       "      <td>[0.24747571349143982, 0.911300003528595]</td>\n",
       "      <td>0.152373</td>\n",
       "      <td>0.944900</td>\n",
       "      <td>0.247476</td>\n",
       "      <td>0.9113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.13222971558570862, 0.9526333212852478]</td>\n",
       "      <td>[0.24787791073322296, 0.9126999974250793]</td>\n",
       "      <td>0.132230</td>\n",
       "      <td>0.952633</td>\n",
       "      <td>0.247878</td>\n",
       "      <td>0.9127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1465650051832199, 0.9471666812896729]</td>\n",
       "      <td>[0.24840134382247925, 0.9114999771118164]</td>\n",
       "      <td>0.146565</td>\n",
       "      <td>0.947167</td>\n",
       "      <td>0.248401</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14446735382080078, 0.948033332824707]</td>\n",
       "      <td>[0.25142455101013184, 0.9111999869346619]</td>\n",
       "      <td>0.144467</td>\n",
       "      <td>0.948033</td>\n",
       "      <td>0.251425</td>\n",
       "      <td>0.9112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15062175691127777, 0.9443666934967041]</td>\n",
       "      <td>[0.252878874540329, 0.909500002861023]</td>\n",
       "      <td>0.150622</td>\n",
       "      <td>0.944367</td>\n",
       "      <td>0.252879</td>\n",
       "      <td>0.9095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18002688884735107, 0.9334666728973389]</td>\n",
       "      <td>[0.2554672062397003, 0.9071999788284302]</td>\n",
       "      <td>0.180027</td>\n",
       "      <td>0.933467</td>\n",
       "      <td>0.255467</td>\n",
       "      <td>0.9072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1857813447713852, 0.930899977684021]</td>\n",
       "      <td>[0.25688549876213074, 0.9057000279426575]</td>\n",
       "      <td>0.185781</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>0.256885</td>\n",
       "      <td>0.9057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15656131505966187, 0.9422333240509033]</td>\n",
       "      <td>[0.25688880681991577, 0.9093000292778015]</td>\n",
       "      <td>0.156561</td>\n",
       "      <td>0.942233</td>\n",
       "      <td>0.256889</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1417781561613083, 0.948366641998291]</td>\n",
       "      <td>[0.25698399543762207, 0.9108999967575073]</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>0.948367</td>\n",
       "      <td>0.256984</td>\n",
       "      <td>0.9109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19672514498233795, 0.9282000064849854]</td>\n",
       "      <td>[0.25953343510627747, 0.9054999947547913]</td>\n",
       "      <td>0.196725</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.259533</td>\n",
       "      <td>0.9055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1708768606185913, 0.9378499984741211]</td>\n",
       "      <td>[0.2613731920719147, 0.9070000052452087]</td>\n",
       "      <td>0.170877</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.261373</td>\n",
       "      <td>0.9070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1608797013759613, 0.9412999749183655]</td>\n",
       "      <td>[0.26573246717453003, 0.9079999923706055]</td>\n",
       "      <td>0.160880</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.265732</td>\n",
       "      <td>0.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.16868141293525696, 0.9378499984741211]</td>\n",
       "      <td>[0.26669758558273315, 0.9018999934196472]</td>\n",
       "      <td>0.168681</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.266698</td>\n",
       "      <td>0.9019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15938624739646912, 0.9430500268936157]</td>\n",
       "      <td>[0.2670109272003174, 0.904699981212616]</td>\n",
       "      <td>0.159386</td>\n",
       "      <td>0.943050</td>\n",
       "      <td>0.267011</td>\n",
       "      <td>0.9047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20643432438373566, 0.9244666695594788]</td>\n",
       "      <td>[0.2693329453468323, 0.9034000039100647]</td>\n",
       "      <td>0.206434</td>\n",
       "      <td>0.924467</td>\n",
       "      <td>0.269333</td>\n",
       "      <td>0.9034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1859435737133026, 0.9332666397094727]</td>\n",
       "      <td>[0.2703552842140198, 0.9021999835968018]</td>\n",
       "      <td>0.185944</td>\n",
       "      <td>0.933267</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.9022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1546628773212433, 0.9443833231925964]</td>\n",
       "      <td>[0.27218130230903625, 0.90420001745224]</td>\n",
       "      <td>0.154663</td>\n",
       "      <td>0.944383</td>\n",
       "      <td>0.272181</td>\n",
       "      <td>0.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21572570502758026, 0.9204333424568176]</td>\n",
       "      <td>[0.2723506689071655, 0.8992999792098999]</td>\n",
       "      <td>0.215726</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.272351</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19306862354278564, 0.9285666942596436]</td>\n",
       "      <td>[0.2737598717212677, 0.9014999866485596]</td>\n",
       "      <td>0.193069</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.273760</td>\n",
       "      <td>0.9015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2132299542427063, 0.9207500219345093]</td>\n",
       "      <td>[0.27428877353668213, 0.9002000093460083]</td>\n",
       "      <td>0.213230</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274289</td>\n",
       "      <td>0.9002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21686747670173645, 0.9199833273887634]</td>\n",
       "      <td>[0.2744320034980774, 0.902400016784668]</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.919983</td>\n",
       "      <td>0.274432</td>\n",
       "      <td>0.9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21186839044094086, 0.9207500219345093]</td>\n",
       "      <td>[0.2745698392391205, 0.8982999920845032]</td>\n",
       "      <td>0.211868</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274570</td>\n",
       "      <td>0.8983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21291086077690125, 0.9204333424568176]</td>\n",
       "      <td>[0.27476274967193604, 0.8995000123977661]</td>\n",
       "      <td>0.212911</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.274763</td>\n",
       "      <td>0.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19442103803157806, 0.9269833564758301]</td>\n",
       "      <td>[0.27573519945144653, 0.9016000032424927]</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.926983</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18178533017635345, 0.9347333312034607]</td>\n",
       "      <td>[0.2796153426170349, 0.9016000032424927]</td>\n",
       "      <td>0.181785</td>\n",
       "      <td>0.934733</td>\n",
       "      <td>0.279615</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.17796066403388977, 0.9342333078384399]</td>\n",
       "      <td>[0.28307250142097473, 0.8967000246047974]</td>\n",
       "      <td>0.177961</td>\n",
       "      <td>0.934233</td>\n",
       "      <td>0.283073</td>\n",
       "      <td>0.8967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21107719838619232, 0.9233666658401489]</td>\n",
       "      <td>[0.28365108370780945, 0.8959000110626221]</td>\n",
       "      <td>0.211077</td>\n",
       "      <td>0.923367</td>\n",
       "      <td>0.283651</td>\n",
       "      <td>0.8959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23273038864135742, 0.9152833223342896]</td>\n",
       "      <td>[0.2840319871902466, 0.894599974155426]</td>\n",
       "      <td>0.232730</td>\n",
       "      <td>0.915283</td>\n",
       "      <td>0.284032</td>\n",
       "      <td>0.8946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20485873520374298, 0.9259999990463257]</td>\n",
       "      <td>[0.285814106464386, 0.8978000283241272]</td>\n",
       "      <td>0.204859</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.285814</td>\n",
       "      <td>0.8978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22205980122089386, 0.92166668176651]</td>\n",
       "      <td>[0.28667232394218445, 0.8970999717712402]</td>\n",
       "      <td>0.222060</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>0.286672</td>\n",
       "      <td>0.8971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23349125683307648, 0.9122499823570251]</td>\n",
       "      <td>[0.287698894739151, 0.8937000036239624]</td>\n",
       "      <td>0.233491</td>\n",
       "      <td>0.912250</td>\n",
       "      <td>0.287699</td>\n",
       "      <td>0.8937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23679600656032562, 0.9127500057220459]</td>\n",
       "      <td>[0.28981828689575195, 0.8930000066757202]</td>\n",
       "      <td>0.236796</td>\n",
       "      <td>0.912750</td>\n",
       "      <td>0.289818</td>\n",
       "      <td>0.8930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23593302071094513, 0.9116500020027161]</td>\n",
       "      <td>[0.29616788029670715, 0.8928999900817871]</td>\n",
       "      <td>0.235933</td>\n",
       "      <td>0.911650</td>\n",
       "      <td>0.296168</td>\n",
       "      <td>0.8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.24779362976551056, 0.9068999886512756]</td>\n",
       "      <td>[0.2994548976421356, 0.8888999819755554]</td>\n",
       "      <td>0.247794</td>\n",
       "      <td>0.906900</td>\n",
       "      <td>0.299455</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22073142230510712, 0.9183333516120911]</td>\n",
       "      <td>[0.2996402978897095, 0.8896999955177307]</td>\n",
       "      <td>0.220731</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>0.299640</td>\n",
       "      <td>0.8897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22014310956001282, 0.9203000068664551]</td>\n",
       "      <td>[0.2999141812324524, 0.891700029373169]</td>\n",
       "      <td>0.220143</td>\n",
       "      <td>0.920300</td>\n",
       "      <td>0.299914</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23643141984939575, 0.9130666851997375]</td>\n",
       "      <td>[0.30026862025260925, 0.8944000005722046]</td>\n",
       "      <td>0.236431</td>\n",
       "      <td>0.913067</td>\n",
       "      <td>0.300269</td>\n",
       "      <td>0.8944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21326030790805817, 0.9208333492279053]</td>\n",
       "      <td>[0.30138877034187317, 0.8938000202178955]</td>\n",
       "      <td>0.213260</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.301389</td>\n",
       "      <td>0.8938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22614991664886475, 0.9175000190734863]</td>\n",
       "      <td>[0.3019619286060333, 0.8912000060081482]</td>\n",
       "      <td>0.226150</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.301962</td>\n",
       "      <td>0.8912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.25457531213760376, 0.90829998254776]</td>\n",
       "      <td>[0.3104911148548126, 0.891700029373169]</td>\n",
       "      <td>0.254575</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>0.310491</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2588706910610199, 0.9059000015258789]</td>\n",
       "      <td>[0.3161528706550598, 0.8884999752044678]</td>\n",
       "      <td>0.258871</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.316153</td>\n",
       "      <td>0.8885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.27241677045822144, 0.9009333252906799]</td>\n",
       "      <td>[0.31941139698028564, 0.8870999813079834]</td>\n",
       "      <td>0.272417</td>\n",
       "      <td>0.900933</td>\n",
       "      <td>0.319411</td>\n",
       "      <td>0.8871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2692805528640747, 0.9014833569526672]</td>\n",
       "      <td>[0.32393553853034973, 0.8855999708175659]</td>\n",
       "      <td>0.269281</td>\n",
       "      <td>0.901483</td>\n",
       "      <td>0.323936</td>\n",
       "      <td>0.8856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2675584852695465, 0.9041666388511658]</td>\n",
       "      <td>[0.3267644941806793, 0.8819000124931335]</td>\n",
       "      <td>0.267558</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>0.326764</td>\n",
       "      <td>0.8819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.29450979828834534, 0.8953333497047424]</td>\n",
       "      <td>[0.3364871144294739, 0.8776999711990356]</td>\n",
       "      <td>0.294510</td>\n",
       "      <td>0.895333</td>\n",
       "      <td>0.336487</td>\n",
       "      <td>0.8777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  \\\n",
       "10  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "11  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "9   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "35  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "4   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "6   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "33  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "3   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "34  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "5   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "29  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "2   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "21  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "7   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "28  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "0   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "1   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "22  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "24  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "30  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "17  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "16  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "45  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "40  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "15  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "46  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "27  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "31  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "25  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "26  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "23  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "32  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "8   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "41  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "47  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "12  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "39  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "13  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "42  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "18  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "37  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "36  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "43  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "38  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "14  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "44  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "20  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "19  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "\n",
       "                                   train_eval  \\\n",
       "10   [0.1421215534210205, 0.9477333426475525]   \n",
       "11  [0.14996056258678436, 0.9464333057403564]   \n",
       "9   [0.12658622860908508, 0.9560166597366333]   \n",
       "35  [0.15237271785736084, 0.9448999762535095]   \n",
       "4   [0.13222971558570862, 0.9526333212852478]   \n",
       "6    [0.1465650051832199, 0.9471666812896729]   \n",
       "33   [0.14446735382080078, 0.948033332824707]   \n",
       "3   [0.15062175691127777, 0.9443666934967041]   \n",
       "34  [0.18002688884735107, 0.9334666728973389]   \n",
       "5     [0.1857813447713852, 0.930899977684021]   \n",
       "29  [0.15656131505966187, 0.9422333240509033]   \n",
       "2     [0.1417781561613083, 0.948366641998291]   \n",
       "21  [0.19672514498233795, 0.9282000064849854]   \n",
       "7    [0.1708768606185913, 0.9378499984741211]   \n",
       "28   [0.1608797013759613, 0.9412999749183655]   \n",
       "0   [0.16868141293525696, 0.9378499984741211]   \n",
       "1   [0.15938624739646912, 0.9430500268936157]   \n",
       "22  [0.20643432438373566, 0.9244666695594788]   \n",
       "24   [0.1859435737133026, 0.9332666397094727]   \n",
       "30   [0.1546628773212433, 0.9443833231925964]   \n",
       "17  [0.21572570502758026, 0.9204333424568176]   \n",
       "16  [0.19306862354278564, 0.9285666942596436]   \n",
       "45   [0.2132299542427063, 0.9207500219345093]   \n",
       "40  [0.21686747670173645, 0.9199833273887634]   \n",
       "15  [0.21186839044094086, 0.9207500219345093]   \n",
       "46  [0.21291086077690125, 0.9204333424568176]   \n",
       "27  [0.19442103803157806, 0.9269833564758301]   \n",
       "31  [0.18178533017635345, 0.9347333312034607]   \n",
       "25  [0.17796066403388977, 0.9342333078384399]   \n",
       "26  [0.21107719838619232, 0.9233666658401489]   \n",
       "23  [0.23273038864135742, 0.9152833223342896]   \n",
       "32  [0.20485873520374298, 0.9259999990463257]   \n",
       "8     [0.22205980122089386, 0.92166668176651]   \n",
       "41  [0.23349125683307648, 0.9122499823570251]   \n",
       "47  [0.23679600656032562, 0.9127500057220459]   \n",
       "12  [0.23593302071094513, 0.9116500020027161]   \n",
       "39  [0.24779362976551056, 0.9068999886512756]   \n",
       "13  [0.22073142230510712, 0.9183333516120911]   \n",
       "42  [0.22014310956001282, 0.9203000068664551]   \n",
       "18  [0.23643141984939575, 0.9130666851997375]   \n",
       "37  [0.21326030790805817, 0.9208333492279053]   \n",
       "36  [0.22614991664886475, 0.9175000190734863]   \n",
       "43    [0.25457531213760376, 0.90829998254776]   \n",
       "38   [0.2588706910610199, 0.9059000015258789]   \n",
       "14  [0.27241677045822144, 0.9009333252906799]   \n",
       "44   [0.2692805528640747, 0.9014833569526672]   \n",
       "20   [0.2675584852695465, 0.9041666388511658]   \n",
       "19  [0.29450979828834534, 0.8953333497047424]   \n",
       "\n",
       "                                    test_eval  train_loss  train_acc  \\\n",
       "10  [0.24299895763397217, 0.9117000102996826]    0.142122   0.947733   \n",
       "11  [0.24511033296585083, 0.9093000292778015]    0.149961   0.946433   \n",
       "9   [0.24576333165168762, 0.9114999771118164]    0.126586   0.956017   \n",
       "35   [0.24747571349143982, 0.911300003528595]    0.152373   0.944900   \n",
       "4   [0.24787791073322296, 0.9126999974250793]    0.132230   0.952633   \n",
       "6   [0.24840134382247925, 0.9114999771118164]    0.146565   0.947167   \n",
       "33  [0.25142455101013184, 0.9111999869346619]    0.144467   0.948033   \n",
       "3      [0.252878874540329, 0.909500002861023]    0.150622   0.944367   \n",
       "34   [0.2554672062397003, 0.9071999788284302]    0.180027   0.933467   \n",
       "5   [0.25688549876213074, 0.9057000279426575]    0.185781   0.930900   \n",
       "29  [0.25688880681991577, 0.9093000292778015]    0.156561   0.942233   \n",
       "2   [0.25698399543762207, 0.9108999967575073]    0.141778   0.948367   \n",
       "21  [0.25953343510627747, 0.9054999947547913]    0.196725   0.928200   \n",
       "7    [0.2613731920719147, 0.9070000052452087]    0.170877   0.937850   \n",
       "28  [0.26573246717453003, 0.9079999923706055]    0.160880   0.941300   \n",
       "0   [0.26669758558273315, 0.9018999934196472]    0.168681   0.937850   \n",
       "1     [0.2670109272003174, 0.904699981212616]    0.159386   0.943050   \n",
       "22   [0.2693329453468323, 0.9034000039100647]    0.206434   0.924467   \n",
       "24   [0.2703552842140198, 0.9021999835968018]    0.185944   0.933267   \n",
       "30    [0.27218130230903625, 0.90420001745224]    0.154663   0.944383   \n",
       "17   [0.2723506689071655, 0.8992999792098999]    0.215726   0.920433   \n",
       "16   [0.2737598717212677, 0.9014999866485596]    0.193069   0.928567   \n",
       "45  [0.27428877353668213, 0.9002000093460083]    0.213230   0.920750   \n",
       "40    [0.2744320034980774, 0.902400016784668]    0.216867   0.919983   \n",
       "15   [0.2745698392391205, 0.8982999920845032]    0.211868   0.920750   \n",
       "46  [0.27476274967193604, 0.8995000123977661]    0.212911   0.920433   \n",
       "27  [0.27573519945144653, 0.9016000032424927]    0.194421   0.926983   \n",
       "31   [0.2796153426170349, 0.9016000032424927]    0.181785   0.934733   \n",
       "25  [0.28307250142097473, 0.8967000246047974]    0.177961   0.934233   \n",
       "26  [0.28365108370780945, 0.8959000110626221]    0.211077   0.923367   \n",
       "23    [0.2840319871902466, 0.894599974155426]    0.232730   0.915283   \n",
       "32    [0.285814106464386, 0.8978000283241272]    0.204859   0.926000   \n",
       "8   [0.28667232394218445, 0.8970999717712402]    0.222060   0.921667   \n",
       "41    [0.287698894739151, 0.8937000036239624]    0.233491   0.912250   \n",
       "47  [0.28981828689575195, 0.8930000066757202]    0.236796   0.912750   \n",
       "12  [0.29616788029670715, 0.8928999900817871]    0.235933   0.911650   \n",
       "39   [0.2994548976421356, 0.8888999819755554]    0.247794   0.906900   \n",
       "13   [0.2996402978897095, 0.8896999955177307]    0.220731   0.918333   \n",
       "42    [0.2999141812324524, 0.891700029373169]    0.220143   0.920300   \n",
       "18  [0.30026862025260925, 0.8944000005722046]    0.236431   0.913067   \n",
       "37  [0.30138877034187317, 0.8938000202178955]    0.213260   0.920833   \n",
       "36   [0.3019619286060333, 0.8912000060081482]    0.226150   0.917500   \n",
       "43    [0.3104911148548126, 0.891700029373169]    0.254575   0.908300   \n",
       "38   [0.3161528706550598, 0.8884999752044678]    0.258871   0.905900   \n",
       "14  [0.31941139698028564, 0.8870999813079834]    0.272417   0.900933   \n",
       "44  [0.32393553853034973, 0.8855999708175659]    0.269281   0.901483   \n",
       "20   [0.3267644941806793, 0.8819000124931335]    0.267558   0.904167   \n",
       "19   [0.3364871144294739, 0.8776999711990356]    0.294510   0.895333   \n",
       "\n",
       "    test_loss  test_acc  \n",
       "10   0.242999    0.9117  \n",
       "11   0.245110    0.9093  \n",
       "9    0.245763    0.9115  \n",
       "35   0.247476    0.9113  \n",
       "4    0.247878    0.9127  \n",
       "6    0.248401    0.9115  \n",
       "33   0.251425    0.9112  \n",
       "3    0.252879    0.9095  \n",
       "34   0.255467    0.9072  \n",
       "5    0.256885    0.9057  \n",
       "29   0.256889    0.9093  \n",
       "2    0.256984    0.9109  \n",
       "21   0.259533    0.9055  \n",
       "7    0.261373    0.9070  \n",
       "28   0.265732    0.9080  \n",
       "0    0.266698    0.9019  \n",
       "1    0.267011    0.9047  \n",
       "22   0.269333    0.9034  \n",
       "24   0.270355    0.9022  \n",
       "30   0.272181    0.9042  \n",
       "17   0.272351    0.8993  \n",
       "16   0.273760    0.9015  \n",
       "45   0.274289    0.9002  \n",
       "40   0.274432    0.9024  \n",
       "15   0.274570    0.8983  \n",
       "46   0.274763    0.8995  \n",
       "27   0.275735    0.9016  \n",
       "31   0.279615    0.9016  \n",
       "25   0.283073    0.8967  \n",
       "26   0.283651    0.8959  \n",
       "23   0.284032    0.8946  \n",
       "32   0.285814    0.8978  \n",
       "8    0.286672    0.8971  \n",
       "41   0.287699    0.8937  \n",
       "47   0.289818    0.8930  \n",
       "12   0.296168    0.8929  \n",
       "39   0.299455    0.8889  \n",
       "13   0.299640    0.8897  \n",
       "42   0.299914    0.8917  \n",
       "18   0.300269    0.8944  \n",
       "37   0.301389    0.8938  \n",
       "36   0.301962    0.8912  \n",
       "43   0.310491    0.8917  \n",
       "38   0.316153    0.8885  \n",
       "14   0.319411    0.8871  \n",
       "44   0.323936    0.8856  \n",
       "20   0.326764    0.8819  \n",
       "19   0.336487    0.8777  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.sort_values(by=['test_loss'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_eval</th>\n",
       "      <th>test_eval</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.12658622860908508, 0.9560166597366333]</td>\n",
       "      <td>[0.24576333165168762, 0.9114999771118164]</td>\n",
       "      <td>0.126586</td>\n",
       "      <td>0.956017</td>\n",
       "      <td>0.245763</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.13222971558570862, 0.9526333212852478]</td>\n",
       "      <td>[0.24787791073322296, 0.9126999974250793]</td>\n",
       "      <td>0.132230</td>\n",
       "      <td>0.952633</td>\n",
       "      <td>0.247878</td>\n",
       "      <td>0.9127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1417781561613083, 0.948366641998291]</td>\n",
       "      <td>[0.25698399543762207, 0.9108999967575073]</td>\n",
       "      <td>0.141778</td>\n",
       "      <td>0.948367</td>\n",
       "      <td>0.256984</td>\n",
       "      <td>0.9109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1421215534210205, 0.9477333426475525]</td>\n",
       "      <td>[0.24299895763397217, 0.9117000102996826]</td>\n",
       "      <td>0.142122</td>\n",
       "      <td>0.947733</td>\n",
       "      <td>0.242999</td>\n",
       "      <td>0.9117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14446735382080078, 0.948033332824707]</td>\n",
       "      <td>[0.25142455101013184, 0.9111999869346619]</td>\n",
       "      <td>0.144467</td>\n",
       "      <td>0.948033</td>\n",
       "      <td>0.251425</td>\n",
       "      <td>0.9112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1465650051832199, 0.9471666812896729]</td>\n",
       "      <td>[0.24840134382247925, 0.9114999771118164]</td>\n",
       "      <td>0.146565</td>\n",
       "      <td>0.947167</td>\n",
       "      <td>0.248401</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.14996056258678436, 0.9464333057403564]</td>\n",
       "      <td>[0.24511033296585083, 0.9093000292778015]</td>\n",
       "      <td>0.149961</td>\n",
       "      <td>0.946433</td>\n",
       "      <td>0.245110</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15062175691127777, 0.9443666934967041]</td>\n",
       "      <td>[0.252878874540329, 0.909500002861023]</td>\n",
       "      <td>0.150622</td>\n",
       "      <td>0.944367</td>\n",
       "      <td>0.252879</td>\n",
       "      <td>0.9095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15237271785736084, 0.9448999762535095]</td>\n",
       "      <td>[0.24747571349143982, 0.911300003528595]</td>\n",
       "      <td>0.152373</td>\n",
       "      <td>0.944900</td>\n",
       "      <td>0.247476</td>\n",
       "      <td>0.9113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1546628773212433, 0.9443833231925964]</td>\n",
       "      <td>[0.27218130230903625, 0.90420001745224]</td>\n",
       "      <td>0.154663</td>\n",
       "      <td>0.944383</td>\n",
       "      <td>0.272181</td>\n",
       "      <td>0.9042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15656131505966187, 0.9422333240509033]</td>\n",
       "      <td>[0.25688880681991577, 0.9093000292778015]</td>\n",
       "      <td>0.156561</td>\n",
       "      <td>0.942233</td>\n",
       "      <td>0.256889</td>\n",
       "      <td>0.9093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.15938624739646912, 0.9430500268936157]</td>\n",
       "      <td>[0.2670109272003174, 0.904699981212616]</td>\n",
       "      <td>0.159386</td>\n",
       "      <td>0.943050</td>\n",
       "      <td>0.267011</td>\n",
       "      <td>0.9047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1608797013759613, 0.9412999749183655]</td>\n",
       "      <td>[0.26573246717453003, 0.9079999923706055]</td>\n",
       "      <td>0.160880</td>\n",
       "      <td>0.941300</td>\n",
       "      <td>0.265732</td>\n",
       "      <td>0.9080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.16868141293525696, 0.9378499984741211]</td>\n",
       "      <td>[0.26669758558273315, 0.9018999934196472]</td>\n",
       "      <td>0.168681</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.266698</td>\n",
       "      <td>0.9019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1708768606185913, 0.9378499984741211]</td>\n",
       "      <td>[0.2613731920719147, 0.9070000052452087]</td>\n",
       "      <td>0.170877</td>\n",
       "      <td>0.937850</td>\n",
       "      <td>0.261373</td>\n",
       "      <td>0.9070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.17796066403388977, 0.9342333078384399]</td>\n",
       "      <td>[0.28307250142097473, 0.8967000246047974]</td>\n",
       "      <td>0.177961</td>\n",
       "      <td>0.934233</td>\n",
       "      <td>0.283073</td>\n",
       "      <td>0.8967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18002688884735107, 0.9334666728973389]</td>\n",
       "      <td>[0.2554672062397003, 0.9071999788284302]</td>\n",
       "      <td>0.180027</td>\n",
       "      <td>0.933467</td>\n",
       "      <td>0.255467</td>\n",
       "      <td>0.9072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.18178533017635345, 0.9347333312034607]</td>\n",
       "      <td>[0.2796153426170349, 0.9016000032424927]</td>\n",
       "      <td>0.181785</td>\n",
       "      <td>0.934733</td>\n",
       "      <td>0.279615</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1857813447713852, 0.930899977684021]</td>\n",
       "      <td>[0.25688549876213074, 0.9057000279426575]</td>\n",
       "      <td>0.185781</td>\n",
       "      <td>0.930900</td>\n",
       "      <td>0.256885</td>\n",
       "      <td>0.9057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.1859435737133026, 0.9332666397094727]</td>\n",
       "      <td>[0.2703552842140198, 0.9021999835968018]</td>\n",
       "      <td>0.185944</td>\n",
       "      <td>0.933267</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.9022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19306862354278564, 0.9285666942596436]</td>\n",
       "      <td>[0.2737598717212677, 0.9014999866485596]</td>\n",
       "      <td>0.193069</td>\n",
       "      <td>0.928567</td>\n",
       "      <td>0.273760</td>\n",
       "      <td>0.9015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19442103803157806, 0.9269833564758301]</td>\n",
       "      <td>[0.27573519945144653, 0.9016000032424927]</td>\n",
       "      <td>0.194421</td>\n",
       "      <td>0.926983</td>\n",
       "      <td>0.275735</td>\n",
       "      <td>0.9016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.19672514498233795, 0.9282000064849854]</td>\n",
       "      <td>[0.25953343510627747, 0.9054999947547913]</td>\n",
       "      <td>0.196725</td>\n",
       "      <td>0.928200</td>\n",
       "      <td>0.259533</td>\n",
       "      <td>0.9055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20485873520374298, 0.9259999990463257]</td>\n",
       "      <td>[0.285814106464386, 0.8978000283241272]</td>\n",
       "      <td>0.204859</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.285814</td>\n",
       "      <td>0.8978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.20643432438373566, 0.9244666695594788]</td>\n",
       "      <td>[0.2693329453468323, 0.9034000039100647]</td>\n",
       "      <td>0.206434</td>\n",
       "      <td>0.924467</td>\n",
       "      <td>0.269333</td>\n",
       "      <td>0.9034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21107719838619232, 0.9233666658401489]</td>\n",
       "      <td>[0.28365108370780945, 0.8959000110626221]</td>\n",
       "      <td>0.211077</td>\n",
       "      <td>0.923367</td>\n",
       "      <td>0.283651</td>\n",
       "      <td>0.8959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21186839044094086, 0.9207500219345093]</td>\n",
       "      <td>[0.2745698392391205, 0.8982999920845032]</td>\n",
       "      <td>0.211868</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274570</td>\n",
       "      <td>0.8983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21291086077690125, 0.9204333424568176]</td>\n",
       "      <td>[0.27476274967193604, 0.8995000123977661]</td>\n",
       "      <td>0.212911</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.274763</td>\n",
       "      <td>0.8995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2132299542427063, 0.9207500219345093]</td>\n",
       "      <td>[0.27428877353668213, 0.9002000093460083]</td>\n",
       "      <td>0.213230</td>\n",
       "      <td>0.920750</td>\n",
       "      <td>0.274289</td>\n",
       "      <td>0.9002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21326030790805817, 0.9208333492279053]</td>\n",
       "      <td>[0.30138877034187317, 0.8938000202178955]</td>\n",
       "      <td>0.213260</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.301389</td>\n",
       "      <td>0.8938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21572570502758026, 0.9204333424568176]</td>\n",
       "      <td>[0.2723506689071655, 0.8992999792098999]</td>\n",
       "      <td>0.215726</td>\n",
       "      <td>0.920433</td>\n",
       "      <td>0.272351</td>\n",
       "      <td>0.8993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.21686747670173645, 0.9199833273887634]</td>\n",
       "      <td>[0.2744320034980774, 0.902400016784668]</td>\n",
       "      <td>0.216867</td>\n",
       "      <td>0.919983</td>\n",
       "      <td>0.274432</td>\n",
       "      <td>0.9024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22014310956001282, 0.9203000068664551]</td>\n",
       "      <td>[0.2999141812324524, 0.891700029373169]</td>\n",
       "      <td>0.220143</td>\n",
       "      <td>0.920300</td>\n",
       "      <td>0.299914</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22073142230510712, 0.9183333516120911]</td>\n",
       "      <td>[0.2996402978897095, 0.8896999955177307]</td>\n",
       "      <td>0.220731</td>\n",
       "      <td>0.918333</td>\n",
       "      <td>0.299640</td>\n",
       "      <td>0.8897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22205980122089386, 0.92166668176651]</td>\n",
       "      <td>[0.28667232394218445, 0.8970999717712402]</td>\n",
       "      <td>0.222060</td>\n",
       "      <td>0.921667</td>\n",
       "      <td>0.286672</td>\n",
       "      <td>0.8971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.22614991664886475, 0.9175000190734863]</td>\n",
       "      <td>[0.3019619286060333, 0.8912000060081482]</td>\n",
       "      <td>0.226150</td>\n",
       "      <td>0.917500</td>\n",
       "      <td>0.301962</td>\n",
       "      <td>0.8912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23273038864135742, 0.9152833223342896]</td>\n",
       "      <td>[0.2840319871902466, 0.894599974155426]</td>\n",
       "      <td>0.232730</td>\n",
       "      <td>0.915283</td>\n",
       "      <td>0.284032</td>\n",
       "      <td>0.8946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23349125683307648, 0.9122499823570251]</td>\n",
       "      <td>[0.287698894739151, 0.8937000036239624]</td>\n",
       "      <td>0.233491</td>\n",
       "      <td>0.912250</td>\n",
       "      <td>0.287699</td>\n",
       "      <td>0.8937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23593302071094513, 0.9116500020027161]</td>\n",
       "      <td>[0.29616788029670715, 0.8928999900817871]</td>\n",
       "      <td>0.235933</td>\n",
       "      <td>0.911650</td>\n",
       "      <td>0.296168</td>\n",
       "      <td>0.8929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23643141984939575, 0.9130666851997375]</td>\n",
       "      <td>[0.30026862025260925, 0.8944000005722046]</td>\n",
       "      <td>0.236431</td>\n",
       "      <td>0.913067</td>\n",
       "      <td>0.300269</td>\n",
       "      <td>0.8944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.23679600656032562, 0.9127500057220459]</td>\n",
       "      <td>[0.28981828689575195, 0.8930000066757202]</td>\n",
       "      <td>0.236796</td>\n",
       "      <td>0.912750</td>\n",
       "      <td>0.289818</td>\n",
       "      <td>0.8930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.24779362976551056, 0.9068999886512756]</td>\n",
       "      <td>[0.2994548976421356, 0.8888999819755554]</td>\n",
       "      <td>0.247794</td>\n",
       "      <td>0.906900</td>\n",
       "      <td>0.299455</td>\n",
       "      <td>0.8889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.25457531213760376, 0.90829998254776]</td>\n",
       "      <td>[0.3104911148548126, 0.891700029373169]</td>\n",
       "      <td>0.254575</td>\n",
       "      <td>0.908300</td>\n",
       "      <td>0.310491</td>\n",
       "      <td>0.8917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2588706910610199, 0.9059000015258789]</td>\n",
       "      <td>[0.3161528706550598, 0.8884999752044678]</td>\n",
       "      <td>0.258871</td>\n",
       "      <td>0.905900</td>\n",
       "      <td>0.316153</td>\n",
       "      <td>0.8885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2675584852695465, 0.9041666388511658]</td>\n",
       "      <td>[0.3267644941806793, 0.8819000124931335]</td>\n",
       "      <td>0.267558</td>\n",
       "      <td>0.904167</td>\n",
       "      <td>0.326764</td>\n",
       "      <td>0.8819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.2692805528640747, 0.9014833569526672]</td>\n",
       "      <td>[0.32393553853034973, 0.8855999708175659]</td>\n",
       "      <td>0.269281</td>\n",
       "      <td>0.901483</td>\n",
       "      <td>0.323936</td>\n",
       "      <td>0.8856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.27241677045822144, 0.9009333252906799]</td>\n",
       "      <td>[0.31941139698028564, 0.8870999813079834]</td>\n",
       "      <td>0.272417</td>\n",
       "      <td>0.900933</td>\n",
       "      <td>0.319411</td>\n",
       "      <td>0.8871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;tensorflow.python.keras.engine.sequential.Seq...</td>\n",
       "      <td>[0.29450979828834534, 0.8953333497047424]</td>\n",
       "      <td>[0.3364871144294739, 0.8776999711990356]</td>\n",
       "      <td>0.294510</td>\n",
       "      <td>0.895333</td>\n",
       "      <td>0.336487</td>\n",
       "      <td>0.8777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                model  \\\n",
       "9   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "4   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "2   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "10  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "33  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "6   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "11  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "3   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "35  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "30  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "29  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "1   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "28  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "0   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "7   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "25  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "34  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "31  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "5   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "24  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "16  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "27  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "21  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "32  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "22  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "26  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "15  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "46  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "45  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "37  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "17  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "40  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "42  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "13  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "8   <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "36  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "23  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "41  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "12  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "18  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "47  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "39  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "43  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "38  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "20  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "44  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "14  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "19  <tensorflow.python.keras.engine.sequential.Seq...   \n",
       "\n",
       "                                   train_eval  \\\n",
       "9   [0.12658622860908508, 0.9560166597366333]   \n",
       "4   [0.13222971558570862, 0.9526333212852478]   \n",
       "2     [0.1417781561613083, 0.948366641998291]   \n",
       "10   [0.1421215534210205, 0.9477333426475525]   \n",
       "33   [0.14446735382080078, 0.948033332824707]   \n",
       "6    [0.1465650051832199, 0.9471666812896729]   \n",
       "11  [0.14996056258678436, 0.9464333057403564]   \n",
       "3   [0.15062175691127777, 0.9443666934967041]   \n",
       "35  [0.15237271785736084, 0.9448999762535095]   \n",
       "30   [0.1546628773212433, 0.9443833231925964]   \n",
       "29  [0.15656131505966187, 0.9422333240509033]   \n",
       "1   [0.15938624739646912, 0.9430500268936157]   \n",
       "28   [0.1608797013759613, 0.9412999749183655]   \n",
       "0   [0.16868141293525696, 0.9378499984741211]   \n",
       "7    [0.1708768606185913, 0.9378499984741211]   \n",
       "25  [0.17796066403388977, 0.9342333078384399]   \n",
       "34  [0.18002688884735107, 0.9334666728973389]   \n",
       "31  [0.18178533017635345, 0.9347333312034607]   \n",
       "5     [0.1857813447713852, 0.930899977684021]   \n",
       "24   [0.1859435737133026, 0.9332666397094727]   \n",
       "16  [0.19306862354278564, 0.9285666942596436]   \n",
       "27  [0.19442103803157806, 0.9269833564758301]   \n",
       "21  [0.19672514498233795, 0.9282000064849854]   \n",
       "32  [0.20485873520374298, 0.9259999990463257]   \n",
       "22  [0.20643432438373566, 0.9244666695594788]   \n",
       "26  [0.21107719838619232, 0.9233666658401489]   \n",
       "15  [0.21186839044094086, 0.9207500219345093]   \n",
       "46  [0.21291086077690125, 0.9204333424568176]   \n",
       "45   [0.2132299542427063, 0.9207500219345093]   \n",
       "37  [0.21326030790805817, 0.9208333492279053]   \n",
       "17  [0.21572570502758026, 0.9204333424568176]   \n",
       "40  [0.21686747670173645, 0.9199833273887634]   \n",
       "42  [0.22014310956001282, 0.9203000068664551]   \n",
       "13  [0.22073142230510712, 0.9183333516120911]   \n",
       "8     [0.22205980122089386, 0.92166668176651]   \n",
       "36  [0.22614991664886475, 0.9175000190734863]   \n",
       "23  [0.23273038864135742, 0.9152833223342896]   \n",
       "41  [0.23349125683307648, 0.9122499823570251]   \n",
       "12  [0.23593302071094513, 0.9116500020027161]   \n",
       "18  [0.23643141984939575, 0.9130666851997375]   \n",
       "47  [0.23679600656032562, 0.9127500057220459]   \n",
       "39  [0.24779362976551056, 0.9068999886512756]   \n",
       "43    [0.25457531213760376, 0.90829998254776]   \n",
       "38   [0.2588706910610199, 0.9059000015258789]   \n",
       "20   [0.2675584852695465, 0.9041666388511658]   \n",
       "44   [0.2692805528640747, 0.9014833569526672]   \n",
       "14  [0.27241677045822144, 0.9009333252906799]   \n",
       "19  [0.29450979828834534, 0.8953333497047424]   \n",
       "\n",
       "                                    test_eval  train_loss  train_acc  \\\n",
       "9   [0.24576333165168762, 0.9114999771118164]    0.126586   0.956017   \n",
       "4   [0.24787791073322296, 0.9126999974250793]    0.132230   0.952633   \n",
       "2   [0.25698399543762207, 0.9108999967575073]    0.141778   0.948367   \n",
       "10  [0.24299895763397217, 0.9117000102996826]    0.142122   0.947733   \n",
       "33  [0.25142455101013184, 0.9111999869346619]    0.144467   0.948033   \n",
       "6   [0.24840134382247925, 0.9114999771118164]    0.146565   0.947167   \n",
       "11  [0.24511033296585083, 0.9093000292778015]    0.149961   0.946433   \n",
       "3      [0.252878874540329, 0.909500002861023]    0.150622   0.944367   \n",
       "35   [0.24747571349143982, 0.911300003528595]    0.152373   0.944900   \n",
       "30    [0.27218130230903625, 0.90420001745224]    0.154663   0.944383   \n",
       "29  [0.25688880681991577, 0.9093000292778015]    0.156561   0.942233   \n",
       "1     [0.2670109272003174, 0.904699981212616]    0.159386   0.943050   \n",
       "28  [0.26573246717453003, 0.9079999923706055]    0.160880   0.941300   \n",
       "0   [0.26669758558273315, 0.9018999934196472]    0.168681   0.937850   \n",
       "7    [0.2613731920719147, 0.9070000052452087]    0.170877   0.937850   \n",
       "25  [0.28307250142097473, 0.8967000246047974]    0.177961   0.934233   \n",
       "34   [0.2554672062397003, 0.9071999788284302]    0.180027   0.933467   \n",
       "31   [0.2796153426170349, 0.9016000032424927]    0.181785   0.934733   \n",
       "5   [0.25688549876213074, 0.9057000279426575]    0.185781   0.930900   \n",
       "24   [0.2703552842140198, 0.9021999835968018]    0.185944   0.933267   \n",
       "16   [0.2737598717212677, 0.9014999866485596]    0.193069   0.928567   \n",
       "27  [0.27573519945144653, 0.9016000032424927]    0.194421   0.926983   \n",
       "21  [0.25953343510627747, 0.9054999947547913]    0.196725   0.928200   \n",
       "32    [0.285814106464386, 0.8978000283241272]    0.204859   0.926000   \n",
       "22   [0.2693329453468323, 0.9034000039100647]    0.206434   0.924467   \n",
       "26  [0.28365108370780945, 0.8959000110626221]    0.211077   0.923367   \n",
       "15   [0.2745698392391205, 0.8982999920845032]    0.211868   0.920750   \n",
       "46  [0.27476274967193604, 0.8995000123977661]    0.212911   0.920433   \n",
       "45  [0.27428877353668213, 0.9002000093460083]    0.213230   0.920750   \n",
       "37  [0.30138877034187317, 0.8938000202178955]    0.213260   0.920833   \n",
       "17   [0.2723506689071655, 0.8992999792098999]    0.215726   0.920433   \n",
       "40    [0.2744320034980774, 0.902400016784668]    0.216867   0.919983   \n",
       "42    [0.2999141812324524, 0.891700029373169]    0.220143   0.920300   \n",
       "13   [0.2996402978897095, 0.8896999955177307]    0.220731   0.918333   \n",
       "8   [0.28667232394218445, 0.8970999717712402]    0.222060   0.921667   \n",
       "36   [0.3019619286060333, 0.8912000060081482]    0.226150   0.917500   \n",
       "23    [0.2840319871902466, 0.894599974155426]    0.232730   0.915283   \n",
       "41    [0.287698894739151, 0.8937000036239624]    0.233491   0.912250   \n",
       "12  [0.29616788029670715, 0.8928999900817871]    0.235933   0.911650   \n",
       "18  [0.30026862025260925, 0.8944000005722046]    0.236431   0.913067   \n",
       "47  [0.28981828689575195, 0.8930000066757202]    0.236796   0.912750   \n",
       "39   [0.2994548976421356, 0.8888999819755554]    0.247794   0.906900   \n",
       "43    [0.3104911148548126, 0.891700029373169]    0.254575   0.908300   \n",
       "38   [0.3161528706550598, 0.8884999752044678]    0.258871   0.905900   \n",
       "20   [0.3267644941806793, 0.8819000124931335]    0.267558   0.904167   \n",
       "44  [0.32393553853034973, 0.8855999708175659]    0.269281   0.901483   \n",
       "14  [0.31941139698028564, 0.8870999813079834]    0.272417   0.900933   \n",
       "19   [0.3364871144294739, 0.8776999711990356]    0.294510   0.895333   \n",
       "\n",
       "    test_loss  test_acc  \n",
       "9    0.245763    0.9115  \n",
       "4    0.247878    0.9127  \n",
       "2    0.256984    0.9109  \n",
       "10   0.242999    0.9117  \n",
       "33   0.251425    0.9112  \n",
       "6    0.248401    0.9115  \n",
       "11   0.245110    0.9093  \n",
       "3    0.252879    0.9095  \n",
       "35   0.247476    0.9113  \n",
       "30   0.272181    0.9042  \n",
       "29   0.256889    0.9093  \n",
       "1    0.267011    0.9047  \n",
       "28   0.265732    0.9080  \n",
       "0    0.266698    0.9019  \n",
       "7    0.261373    0.9070  \n",
       "25   0.283073    0.8967  \n",
       "34   0.255467    0.9072  \n",
       "31   0.279615    0.9016  \n",
       "5    0.256885    0.9057  \n",
       "24   0.270355    0.9022  \n",
       "16   0.273760    0.9015  \n",
       "27   0.275735    0.9016  \n",
       "21   0.259533    0.9055  \n",
       "32   0.285814    0.8978  \n",
       "22   0.269333    0.9034  \n",
       "26   0.283651    0.8959  \n",
       "15   0.274570    0.8983  \n",
       "46   0.274763    0.8995  \n",
       "45   0.274289    0.9002  \n",
       "37   0.301389    0.8938  \n",
       "17   0.272351    0.8993  \n",
       "40   0.274432    0.9024  \n",
       "42   0.299914    0.8917  \n",
       "13   0.299640    0.8897  \n",
       "8    0.286672    0.8971  \n",
       "36   0.301962    0.8912  \n",
       "23   0.284032    0.8946  \n",
       "41   0.287699    0.8937  \n",
       "12   0.296168    0.8929  \n",
       "18   0.300269    0.8944  \n",
       "47   0.289818    0.8930  \n",
       "39   0.299455    0.8889  \n",
       "43   0.310491    0.8917  \n",
       "38   0.316153    0.8885  \n",
       "20   0.326764    0.8819  \n",
       "44   0.323936    0.8856  \n",
       "14   0.319411    0.8871  \n",
       "19   0.336487    0.8777  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df.sort_values(by=['train_loss'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, model         <tensorflow.python.keras.engine.sequential.Seq...\n",
      "train_eval             [0.1421215534210205, 0.9477333426475525]\n",
      "test_eval             [0.24299895763397217, 0.9117000102996826]\n",
      "train_loss                                             0.142122\n",
      "train_acc                                              0.947733\n",
      "test_loss                                              0.242999\n",
      "test_acc                                                 0.9117\n",
      "Name: 10, dtype: object)\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_30 (Conv2D)           (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 25, 25, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 23, 23, 12)        660       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_31 (MaxPooling (None, 22, 22, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 120)               697080    \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 10)                610       \n",
      "=================================================================\n",
      "Total params: 705,670\n",
      "Trainable params: 705,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'kernel_size': 3, 'max_pool_size': 2, 'pool_strides': 1, 'filters': (6, 12), 'dense_size': (120, 60), 'learning_rate': 0.0005, 'dropout_d': 0.2, 'batch_size': 250, 'epochs': 100, 'shuffle': True, 'model_name': 'model-10', 'callbacks': [[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x00000217884E49B0>]], 'test_images': array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]]), 'test_labels': array([9, 2, 1, ..., 8, 1, 5], dtype=uint8), 'train_images': array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]]), 'train_labels': array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)}\n",
      "(9, model         <tensorflow.python.keras.engine.sequential.Seq...\n",
      "train_eval            [0.12658622860908508, 0.9560166597366333]\n",
      "test_eval             [0.24576333165168762, 0.9114999771118164]\n",
      "train_loss                                             0.126586\n",
      "train_acc                                              0.956017\n",
      "test_loss                                              0.245763\n",
      "test_acc                                                 0.9115\n",
      "Name: 9, dtype: object)\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_28 (Conv2D)           (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 25, 25, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 23, 23, 12)        660       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 22, 22, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 120)               697080    \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 60)                0         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 10)                610       \n",
      "=================================================================\n",
      "Total params: 705,670\n",
      "Trainable params: 705,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'kernel_size': 3, 'max_pool_size': 2, 'pool_strides': 1, 'filters': (6, 12), 'dense_size': (120, 60), 'learning_rate': 0.0005, 'dropout_d': 0.2, 'batch_size': 100, 'epochs': 100, 'shuffle': True, 'model_name': 'model-9', 'callbacks': [[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x00000217884E49B0>]], 'test_images': array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]]), 'test_labels': array([9, 2, 1, ..., 8, 1, 5], dtype=uint8), 'train_images': array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]]), 'train_labels': array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)}\n",
      "(4, model         <tensorflow.python.keras.engine.sequential.Seq...\n",
      "train_eval            [0.13222971558570862, 0.9526333212852478]\n",
      "test_eval             [0.24787791073322296, 0.9126999974250793]\n",
      "train_loss                                              0.13223\n",
      "train_acc                                              0.952633\n",
      "test_loss                                              0.247878\n",
      "test_acc                                                 0.9127\n",
      "Name: 4, dtype: object)\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_18 (Conv2D)           (None, 26, 26, 6)         60        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 25, 25, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 23, 23, 12)        660       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 22, 22, 12)        0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 5808)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 120)               697080    \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 60)                7260      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 10)                610       \n",
      "=================================================================\n",
      "Total params: 705,670\n",
      "Trainable params: 705,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'kernel_size': 3, 'max_pool_size': 2, 'pool_strides': 1, 'filters': (6, 12), 'dense_size': (120, 60), 'learning_rate': 0.001, 'dropout_d': 0.2, 'batch_size': 250, 'epochs': 100, 'shuffle': True, 'model_name': 'model-4', 'callbacks': [[<tensorflow.python.keras.callbacks.EarlyStopping object at 0x00000217884E49B0>]], 'test_images': array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]]), 'test_labels': array([9, 2, 1, ..., 8, 1, 5], dtype=uint8), 'train_images': array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]]), 'train_labels': array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)}\n"
     ]
    }
   ],
   "source": [
    "# Chosen based on evaluation of the above losses and accuracy\n",
    "# However, all models are saved elsewhere (for now) so they can be grabbed\n",
    "best_models = [10, 9, 4]\n",
    "best_df = all_df.iloc[best_models]\n",
    "best_df\n",
    "for row in best_df.iterrows():\n",
    "    print(row)\n",
    "    print(row[1]['model'].summary())\n",
    "    print(full_params[row[0]])\n",
    "    row[1]['model'].save(f'./saved_models/tf-model-{row[0]}.h5')\n",
    "\n",
    "# print(full_params[*best_models])\n",
    "# for m in best_models:\n",
    "#     print(f\"model {m}\")\n",
    "#     print(f\"metrics: {all_df[[\"train_loss\",\"train_acc\",\"test_loss\",\"test_acc\"]][m]}\")\n",
    "#     print(all_df['model'][m].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 's'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\gibso\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\gibso\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\pool.py\", line 47, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"C:\\Users\\gibso\\Documents\\fashion-classification\\model\\build_and_fit_model.py\", line 34, in build_and_fit_model\n    model.add(keras.layers.Conv2D(filters=filters[0], kernel_size=kernel_size, activation='relu', input_shape=(28, 28,1), data_format='channels_last'))\n  File \"C:\\Users\\gibso\\Documents\\fashion-classification\\env\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 599, in __init__\n    **kwargs)\n  File \"C:\\Users\\gibso\\Documents\\fashion-classification\\env\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\", line 128, in __init__\n    filters = int(filters)\nValueError: invalid literal for int() with base 10: 's'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-870fa4ecbc44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Reserve 1 processor for the os so it doesn't die\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdata_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuild_and_fit_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# To make sure processes are closed in the end, even if errors happen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mstarmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mbecomes\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         '''\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstarmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m     def starmap_async(self, func, iterable, chunksize=None, callback=None,\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    642\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 's'"
     ]
    }
   ],
   "source": [
    "# I want to go to bed so I'm just gonna brute force it and not worry about multiprocessing for now\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "# try:\n",
    "#     pool = Pool(os.cpu_count()-1) # Reserve 1 processor for the os so it doesn't die\n",
    "#     data_outputs = pool.starmap(build_and_fit_model, full_params)\n",
    "# finally: # To make sure processes are closed in the end, even if errors happen\n",
    "#     pool.close()\n",
    "#     pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f-c",
   "language": "python",
   "name": "f-c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
